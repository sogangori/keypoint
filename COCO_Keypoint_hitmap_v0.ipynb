{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keypoint 는 [17, 3] 의 2차원 배열\n",
    "- 17는 키포인트의 개수이고 3은 [x, y, visibility] 이다\n",
    "- visibility {0:안보여서 알수없음, 1:가려짐, 2:보임)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_KEYPOINT = 17 # 키포인트 개수\n",
    "NUM_KEYPOINT_CH = 3 # x, y, visibility\n",
    "NUM_KEYPOINT_CLASS = 3\n",
    "thresh_keypoint_count = 10 # 데이터 선별시 최소 개수\n",
    "VISIBILITY_NOT_EXIST = 0\n",
    "VISIBILITY_OCCLUDED = 1\n",
    "VISIBILITY_VISIBLE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_data_m = 20\n",
    "channel_label = 4 + 1 + NUM_KEYPOINT * NUM_KEYPOINT_CH #(box:4, cls:1, keypoint:17*3)\n",
    "channel_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_COCO = '/media/mvlab/469B5B3C650FBA77/data/COCO/'\n",
    "folder_COCO_annotation = folder_COCO + 'annotations_trainval2017/annotations/'\n",
    "folder_img = 'val2017'\n",
    "#folder_img = 'train2017'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_annotation_keypoints_val2017 = folder_COCO_annotation + 'person_keypoints_'+folder_img + '.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.isdir(folder_COCO), os.path.isdir(folder_COCO_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.isfile(path_annotation_keypoints_val2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_annotation_keypoints_val2017) as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "    \n",
    "len(json_data), json_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_keypoints = json_data['categories'][0]['keypoints']\n",
    "images = json_data['images']\n",
    "annotations = json_data['annotations']\n",
    "\n",
    "len(class_keypoints), len(images), len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AIHub 사람 동작 영상 AI 데이터는 키포인트 16개\n",
    "- 추가 4 head, neck, chest, hip\n",
    "- 제거 5 nose, left_eye, right_eye, left_ear, right_ear\n",
    "\n",
    "##### COCO VS AIHub\n",
    "- COCO 얼굴 중요(눈, 코, 귀)\n",
    "- AIHub 동작 중요(목, 가슴, 골반 중심)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations[0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations[0]['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_id_image_info = dict()\n",
    "image_w = []\n",
    "image_h = []\n",
    "for image_info in images:\n",
    "    #print('image_info', image_info)    \n",
    "    \n",
    "    image_w.append(image_info['width'])\n",
    "    image_h.append(image_info['height'])\n",
    "    \n",
    "    image_id = image_info['id']    \n",
    "    dict_id_image_info[image_id] = image_info\n",
    "        \n",
    "len(dict_id_image_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(image_w), np.min(image_h), np.max(image_w), np.max(image_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(image_w) < 200).mean(), (np.array(image_h) < 200).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('w, h')\n",
    "plt.scatter(image_w, image_h, s=2)\n",
    "plt.xlabel('width')\n",
    "plt.ylabel('height')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get image_filename from image_id\n",
    "list_id = []\n",
    "list_category_id = []\n",
    "for annotation in annotations:\n",
    "    \n",
    "    image_id = annotation['image_id']\n",
    "    category_id = annotation['category_id']    \n",
    "    \n",
    "    image_info = dict_id_image_info[image_id]\n",
    "    if len(list_id) == 0:\n",
    "        print('category_id', category_id)\n",
    "        print('annotation', annotation)\n",
    "        print('image_info', image_info)\n",
    "        \n",
    "    file_name = image_info['file_name']\n",
    "    height = image_info['height']\n",
    "    width = image_info['width']\n",
    "    \n",
    "    file_full_path = folder_COCO + folder_img + os.sep + file_name\n",
    "    \n",
    "    if not os.path.isfile(file_full_path):\n",
    "        print('not exist', file_full_path)\n",
    "        continue\n",
    "        \n",
    "    annotation['file_name'] = file_full_path\n",
    "    annotation['height'] = height\n",
    "    annotation['width'] = width\n",
    "    \n",
    "    #print(annotation)\n",
    "    list_id.append(image_id)\n",
    "    list_category_id.append(category_id)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전부 사람(1) 카테고리\n",
    "np.unique(list_category_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#generate dictionary(key:filename, value:annotation list)\n",
    "#박스와 클래스, 키포인트를 통합\n",
    "dict_annotation_coco = dict()\n",
    "for annotation in annotations:\n",
    "    #print(annotation)\n",
    "    segmentation = annotation['segmentation']\n",
    "    bbox = annotation['bbox']\n",
    "    category_id = annotation['category_id']\n",
    "    keypoints = annotation['keypoints']    \n",
    "    file_name = annotation['file_name']\n",
    "    \n",
    "    #x0, y0, x0 + w, y0 + h\n",
    "    x0, y0, w, h = bbox\n",
    "    b = np.array([x0, y0, x0 + w, y0 + h]).flatten()\n",
    "    c = np.array(category_id).flatten()\n",
    "    k = np.array(keypoints).flatten()\n",
    "    #print(len(b), 'box', bbox, len(c), len(k))\n",
    "    cbox = np.concatenate((b, c, k)).reshape((1, -1))\n",
    "    \n",
    "    if len(dict_annotation_coco) == 0:        \n",
    "        print('file_name', file_name)\n",
    "        print('bbox', bbox)\n",
    "        print('segmentation', len(segmentation), len(segmentation[0]))\n",
    "        print('keypoints', np.array(keypoints).reshape((-1, 3)))    \n",
    "        print('cbox', len(cbox), cbox)\n",
    "    \n",
    "    if file_name in dict_annotation_coco.keys():\n",
    "        pre_annotation = dict_annotation_coco[file_name]\n",
    "        new_bbox = np.concatenate((pre_annotation, cbox), axis=0)        \n",
    "        dict_annotation_coco[file_name] = new_bbox\n",
    "    else:\n",
    "        dict_annotation_coco[file_name] = cbox\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_annotation_coco), list(dict_annotation_coco.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_id), len(set(list_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_keypoint_label = np.concatenate(list(dict_annotation_coco.values()), 0)\n",
    "box_keypoint_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x0 = box_keypoint_label[:, 0]\n",
    "y0 = box_keypoint_label[:, 1]\n",
    "x1 = box_keypoint_label[:, 2]\n",
    "y1 = box_keypoint_label[:, 3]\n",
    "box_w = x1-x0\n",
    "box_h = y1-y0\n",
    "plt.title('h / w = ' +  str(np.mean(box_h/box_w)))\n",
    "ax = plt.hist(box_w )\n",
    "ax = plt.hist(box_h, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 선별\n",
    "- 한사람의 키포인트가 일정 개수 이상 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_annotation_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_annotation_coco_select = dict()\n",
    "\n",
    "for key in dict_annotation_coco:\n",
    "    value = dict_annotation_coco[key]\n",
    "    #print('key', key)\n",
    "    #print('value', type(value), value.shape, value)\n",
    "    \n",
    "    box = value[:, :4]\n",
    "    cls = value[:, 4]\n",
    "    keypoint = value[:, 5:]\n",
    "    keypoint = keypoint.reshape((-1, NUM_KEYPOINT, NUM_KEYPOINT_CH))\n",
    "    visibility = keypoint[:, :, 2]\n",
    "    visibility_count_per_person = np.sum(visibility > VISIBILITY_NOT_EXIST, -1)\n",
    "    visibility_max_count_per_person = np.max(visibility_count_per_person)\n",
    "        \n",
    "    if visibility_max_count_per_person >= thresh_keypoint_count:                \n",
    "        dict_annotation_coco_select[key] = value\n",
    "\n",
    "len(dict_annotation_coco), len(dict_annotation_coco_select)\n",
    "#train2017:(64115, 45900), val2017:(2693, 2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_annotation_coco = dict_annotation_coco_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_cbox(image, cbox):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    x0, y0, x1, y1, cls = cbox\n",
    "    draw.text((x0+2, y0), str(int(cls)))\n",
    "    draw.rectangle((x0, y0, x1, y1), fill=None, outline=(0,255,0), width=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoint(image, keypoints, is_show_class=True):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    #print('draw_keypoint', keypoints.shape, keypoints[:,2])\n",
    "    #[0. 0. 0. 0. 0. 1. 2. 2. 2. 2. 2. 2. 2. 0. 2. 0. 2.]\n",
    "    for i in range(len(keypoints)):\n",
    "        point = keypoints[i]\n",
    "        cls_name = str(i)+':'+ class_keypoints[i] if is_show_class else ''                \n",
    "        cx = point[0]\n",
    "        cy = point[1]\n",
    "        state = point[2]\n",
    "        circle_radius = 2\n",
    "        if state > 1:\n",
    "            draw.ellipse((cx-circle_radius, cy-circle_radius, cx+circle_radius, cy+circle_radius),fill=None, width=1)\n",
    "            draw.text((cx, cy-10), cls_name)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_valid_line(draw, p0, p1, color):\n",
    "    #point : [x, y, state]\n",
    "    state0 = p0[2]\n",
    "    state1 = p1[2]\n",
    "    if state0 * state1 > 0:\n",
    "        draw.line((tuple(p0[:2]), tuple(p1[:2])), fill=color)\n",
    "    \n",
    "\n",
    "def draw_keypoint_line(image, keypoints):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "        \n",
    "    nose = keypoints[0]\n",
    "    left_eye = keypoints[1]\n",
    "    right_eye = keypoints[2]\n",
    "    left_ear = keypoints[3]\n",
    "    right_ear = keypoints[4]            \n",
    "    left_shoulder = keypoints[5]\n",
    "    right_shoulder = keypoints[6]\n",
    "    left_elbow = keypoints[7]\n",
    "    right_elbow = keypoints[8]\n",
    "    left_wrist = keypoints[9]\n",
    "    right_wrist = keypoints[10]\n",
    "    left_hip = keypoints[11]\n",
    "    right_hip = keypoints[12]\n",
    "    left_knee = keypoints[13]\n",
    "    right_knee = keypoints[14]\n",
    "    left_ankle = keypoints[15]\n",
    "    right_ankle = keypoints[16]        \n",
    "    \n",
    "    color_center = (255, 255, 255)\n",
    "    color_left = (255,255,0)\n",
    "    color_right = (0,255,255)\n",
    "    \n",
    "    draw_valid_line(draw, left_shoulder, right_shoulder, color_center)\n",
    "    draw_valid_line(draw, left_hip, right_hip, color_center)\n",
    "    \n",
    "    draw_valid_line(draw, nose, left_eye, color_left)\n",
    "    draw_valid_line(draw, left_eye, left_ear, color_left)\n",
    "    \n",
    "    draw_valid_line(draw, nose, right_eye, color_right)\n",
    "    draw_valid_line(draw, right_eye, right_ear, color_right)\n",
    "        \n",
    "    draw_valid_line(draw, left_shoulder, left_ear, color_left)    \n",
    "    draw_valid_line(draw, left_shoulder, left_elbow, color_left)\n",
    "    draw_valid_line(draw, left_shoulder, left_hip, color_left)\n",
    "    draw_valid_line(draw, left_elbow, left_wrist, color_left)\n",
    "    draw_valid_line(draw, left_hip, left_knee, color_left)\n",
    "    draw_valid_line(draw, left_knee, left_ankle, color_left)\n",
    "    \n",
    "    draw_valid_line(draw, right_shoulder, right_ear, color_right)\n",
    "    draw_valid_line(draw, right_shoulder, right_elbow, color_right)\n",
    "    draw_valid_line(draw, right_shoulder, right_hip, color_right)\n",
    "    draw_valid_line(draw, right_elbow, right_wrist, color_right)    \n",
    "    draw_valid_line(draw, right_hip, right_knee, color_right)\n",
    "    draw_valid_line(draw, right_knee, right_ankle, color_right)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_box_keypoint(image, keypoint_box_label, is_show_class=False):\n",
    "    #keypoint_box_label : (m, 17*3+5)\n",
    "    box_label = keypoint_box_label[:, :5]\n",
    "    keypoint_label = keypoint_box_label[:,5:].reshape((-1, NUM_KEYPOINT, NUM_KEYPOINT_CH))\n",
    "    #box_label : (m, 5)\n",
    "    #keypoint_label : (m, 17, 3)\n",
    "    \n",
    "    if np.mean(box_label[:, :4]) < 2:\n",
    "        print('unnormalize')\n",
    "        img_w, img_h = image.width, image.height        \n",
    "        box_label[:, :4] *= np.array((img_w, img_h, img_w, img_h)).reshape((1, 4))\n",
    "        keypoint_label[:, :, :2] *= np.array((img_w, img_h)).reshape((1, 1, 2))\n",
    "    \n",
    "    for keypoints in keypoint_label:\n",
    "        draw_keypoint(image, keypoints, is_show_class=is_show_class)\n",
    "        draw_keypoint_line(image, keypoints)\n",
    "\n",
    "    for cbox in box_label:\n",
    "        draw_cbox(image, cbox)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_i = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_image_path = list(dict_annotation_coco.keys())[sample_i]\n",
    "keypoint_box_label_sample = dict_annotation_coco[sample_image_path]\n",
    "\n",
    "box_label_sample = keypoint_box_label_sample[:, :5]\n",
    "keypoint_label_sample = keypoint_box_label_sample[:,5:].reshape((-1, 17, 3))#todo \n",
    "sample_image_path, len(keypoint_label_sample), keypoint_label_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(sample_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(keypoint_label_sample[0].astype(np.int), class_keypoints, ['x','y','visibility'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_label_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = Image.open(sample_image_path)\n",
    "print('sample_image.size', sample_image.size)\n",
    "draw_box_keypoint(sample_image, keypoint_box_label_sample, is_show_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 데이터 구성\n",
    "- X : 이미지 (h,w,3)\n",
    "- Y : 박스 + 클래스 + 키포인트 (box:4, class:1, keypoint:17*3(x,y,visibility)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keypoint 는 영상에서의 좌표가 아닌 박스에서의 상대좌표? NO, 박스가 부정확하면 keypoint 까지 망가진다\n",
    "- 박스 검출 후 그 내부에서 keypoint 의 상대좌표를 regression 하면 안됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keypoint 를 검출해서 그것으로부터 박스를 그리는 것이 안전\n",
    "- keypoint 의 visibility 까지 학습시켜서 keypoint를 그리는데 사용하자\n",
    "- 그려진 keypoint를 이용해서 box 를 그리자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- box & keypoint = positive\n",
    "- no box * no keypoint = background\n",
    "- box & no keypoint = ignore (New!)\n",
    "- no box & keypoint = not exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. keypoint는 groundtruth 로 클래스별 hitmap으로 변환해서 traget 이 되어 학습된다.\n",
    "    - hitmap gt 는 좌표맵과 키포인트 좌표의 차이를 이용해 생성한다\n",
    "      - 차이가 가장 작은 위치는 positive + (VISIBLE, OCCLUDED)\n",
    "      - 차이가 일정 범위 이하는 ignore\n",
    "      - 차이가 일정 범위 이상은 negative + (NOT_EXIST)\n",
    "    - hitmap_visibility_gt 는 hitmap_coord_gt 와 별도로 생성한다\n",
    "      - hitmap의 할당값은 VISIBLE(2), OCCLUDED(1), NOT_EXIST(0) 로 맵핑\n",
    "      - hitmap_gt는 class별로 존재하나 visibility 는 class와 독립적인 변수이다.\n",
    "      - hitmap_gt는 keypoint를 찾는 용도이며 visibility는 hitmap_gt로 찾은 keypoint의 상태이다.\n",
    "    - hitmap gt 는 multi-scale 일 필요가 있다\n",
    "    - hitmap gt 는 anchor별로 생성될 필요가 있다. 그것이 구현이 더 편리하다\n",
    "    - 모델은 클래스별 hitmap을 출력한다\n",
    "    - hitmap loss 적용\n",
    "1. 모델은 검출기와 같은 anchor를 사용한다\n",
    "   - 모델은 objectness와 박스 regression을 출력한다\n",
    "1. positive objectness를 갖는 anchor 는 모델이 예측한 box 변환 값을 이용해 오브젝트 후보의 경계 박스(ROI)를 생성한다\n",
    "   1. ROI 내부에 존재하는 최대 확률의 keypoint (ROI에 대한 상대)좌표를 구한다\n",
    "      - keypoint 간의 연결성에 대해서는 무시하는 꼴이다.\n",
    "      - 따라서 주위에서 침입한 keypoint 가 있는 경우 문제가 발생한다. 이 문제는 hierarchical keypoint 나 affinity vector를 이용하면 해결 가능하다.\n",
    "      - 일단은 연결성은 무시해서 구현해보자\n",
    "   1. ROI 정보를 이용해 상대좌표를 이미지에 대한 상대 좌표로 변환한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xy(annotation, stride=1):\n",
    "    input_list = []\n",
    "    bbox_list = []\n",
    "    path_list = []\n",
    "    i = 0\n",
    "    \n",
    "    for path_image in annotation:\n",
    "        i+=1\n",
    "        if stride!=1 and np.random.randint(1, 1+stride)%stride!=0:\n",
    "            continue\n",
    "            \n",
    "        cls_bbox = annotation[path_image]                \n",
    "        bbox = np.array(cls_bbox[:, :4])\n",
    "        cls = cls_bbox[:, 4:5]\n",
    "        keypoint = cls_bbox[:, 5:].astype(np.float)\n",
    "        keypoint_3d = np.reshape(keypoint, (-1, NUM_KEYPOINT, NUM_KEYPOINT_CH))\n",
    "        \n",
    "        img = Image.open(path_image)    \n",
    "        scale = np.array((img.width, img.height, img.width, img.height))\n",
    "        scale = np.reshape(scale, (1, 4)).astype(np.float)\n",
    "        scale_keypoint = np.array((img.width, img.height, 1)).reshape((1,1,3))\n",
    "\n",
    "        img_arr = np.array(img)\n",
    "        if img_arr.ndim < 3: \n",
    "            print('gray image skip')\n",
    "            continue #gray image skip\n",
    "        \n",
    "        try:\n",
    "            std_v = np.std(img_arr)\n",
    "            if std_v < 3:\n",
    "                print('std_v', std_v)\n",
    "                continue\n",
    "        except:\n",
    "            print('error', path_image)\n",
    "            continue\n",
    "        \n",
    "        box_width = bbox[:, 2] - bbox[:, 0]\n",
    "        box_height = bbox[:, 3] - bbox[:, 1]\n",
    "        \n",
    "        if np.min(box_width) < 1 or np.min(box_height) < 1:\n",
    "            print('box_size < 1', box_width, box_height)#check\n",
    "            continue\n",
    "            \n",
    "        bbox_norm = bbox.astype(np.float) / scale        \n",
    "        keypoint_norm = keypoint_3d / scale_keypoint        \n",
    "        keypoint_norm_2d = np.reshape(keypoint_norm, (-1, NUM_KEYPOINT * NUM_KEYPOINT_CH))\n",
    "        \n",
    "        cls_bbox_norm = np.concatenate((bbox_norm, cls, keypoint_norm_2d), axis=1)\n",
    "\n",
    "        input_list.append(img_arr)\n",
    "        bbox_list.append(cls_bbox_norm)\n",
    "        path_list.append(path_image)\n",
    "        if len(input_list)%100==0:        \n",
    "            print(len(annotation), i, len(input_list))   \n",
    "        if len(input_list) >= max_data_m:\n",
    "            break       \n",
    "\n",
    "    print(len(input_list), len(bbox_list))\n",
    "    return input_list, bbox_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_annotation_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_x, list_y = load_xy(dict_annotation_coco, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_x[0].shape, list_y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(list_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_y[0][:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_y[0][:, 5:].reshape((-1, NUM_KEYPOINT, NUM_KEYPOINT_CH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_y[0][:, 5:].reshape((-1, NUM_KEYPOINT, NUM_KEYPOINT_CH))[:,:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_y[0][:, 5:].reshape((-1, NUM_KEYPOINT, NUM_KEYPOINT_CH))[:,:,-1] = 1+np.arange(NUM_KEYPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_y[0][:, 5:].reshape((-1, NUM_KEYPOINT, NUM_KEYPOINT_CH))[:,:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_data(list_x, list_y, stride=1):\n",
    "    \n",
    "    for i in range(0, len(list_x), stride):\n",
    "        x = list_x[i]\n",
    "        y = list_y[i]\n",
    "                \n",
    "        img = Image.fromarray(x)\n",
    "        kp = y[:, -NUM_KEYPOINT * NUM_KEYPOINT_CH:]        \n",
    "        kp_cls = kp[:, 2::3]\n",
    "        print(i, 'y', 'box', y[:, :4])\n",
    "        #print('kp', kp)\n",
    "        draw_box_keypoint(img, y, is_show_class=False)\n",
    "        display(img)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_data(list_x, list_y, stride=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/building-a-resnet-in-keras-e8f1322a49ba\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import Tensor\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization,\\\n",
    "                                    Add, AveragePooling2D, Flatten, Dense, MaxPool2D\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/tensorflow/addons/blob/v0.11.2/tensorflow_addons/image/__init__.py\n",
    "from tensorflow_addons.image.color_ops import sharpness\n",
    "from tensorflow_addons.image.filters import gaussian_filter2d\n",
    "from tensorflow_addons.image.dense_image_warp import dense_image_warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__, tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_image_shape = (512, 512)#320, 512, 640\n",
    "anchor_k = 9\n",
    "num_classes = 3\n",
    "level_start = 4\n",
    "level_end = 8\n",
    "l1 = 1e-9\n",
    "activation = 'swish'#'selu' is not converted to tflite\n",
    "kernel_init = tf.initializers.he_normal()\n",
    "edgecolors = np.random.rand(num_classes, 3) \n",
    "edgecolors = np.minimum(edgecolors+0.1, 1.0)\n",
    "path_weight = \"weight/keypoint_efficientDet-D2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_x_train = list_x\n",
    "list_x_test = list_x\n",
    "list_y_train = list_y\n",
    "list_y_test = list_y\n",
    "len(list_x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_xy(boxes):\n",
    "    \"\"\"Swaps order the of x and y coordinates of the boxes.\n",
    "    Arguments:\n",
    "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes.\n",
    "    Returns:\n",
    "      swapped boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.stack([boxes[:, 1], boxes[:, 0], boxes[:, 3], boxes[:, 2]], axis=-1)\n",
    "\n",
    "\n",
    "def convert_to_xywh(boxes):\n",
    "    \"\"\"Changes the box format to center, width and height.\n",
    "    Arguments:\n",
    "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
    "        representing bounding boxes where each box is of the format\n",
    "        `[xmin, ymin, xmax, ymax]`.\n",
    "    Returns:\n",
    "      converted boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.concat(\n",
    "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_to_corners(boxes):\n",
    "    \"\"\"Changes the box format to corner coordinates\n",
    "    Arguments:\n",
    "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
    "        representing bounding boxes where each box is of the format\n",
    "        `[x, y, width, height]`.\n",
    "    Returns:\n",
    "      converted boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.concat(\n",
    "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Computing pairwise Intersection Over Union (IOU)\n",
    "As we will see later in the example, we would be assigning ground truth boxes\n",
    "to anchor boxes based on the extent of overlapping. This will require us to\n",
    "calculate the Intersection Over Union (IOU) between all the anchor\n",
    "boxes and ground truth boxes pairs.\n",
    "\"\"\"\n",
    "\n",
    "def compute_iou(boxes1, boxes2):#compute_iou(anchor_boxes, gt_boxes)\n",
    "    \"\"\"Computes pairwise IOU matrix for given two sets of boxes\n",
    "    Arguments:\n",
    "      boxes1: A tensor with shape `(N, 4)` representing bounding boxes\n",
    "        where each box is of the format `[x, y, width, height]`.\n",
    "        boxes2: A tensor with shape `(M, 4)` representing bounding boxes\n",
    "        where each box is of the format `[x, y, width, height]`.\n",
    "    Returns:\n",
    "      pairwise IOU matrix with shape `(N, M)`, where the value at ith row\n",
    "        jth column holds the IOU between ith box and jth box from\n",
    "        boxes1 and boxes2 respectively.\n",
    "    \"\"\"\n",
    "    boxes1_corners = convert_to_corners(boxes1)\n",
    "    boxes2_corners = convert_to_corners(boxes2)\n",
    "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
    "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n",
    "    intersection = tf.maximum(0.0, rd - lu)\n",
    "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
    "    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
    "    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
    "    union_area = tf.maximum(\n",
    "        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n",
    "    )\n",
    "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "## Implementing Anchor generator\n",
    "Anchor boxes are fixed sized boxes that the model uses to predict the bounding\n",
    "box for an object. It does this by regressing the offset between the location\n",
    "of the object's center and the center of an anchor box, and then uses the width\n",
    "and height of the anchor box to predict a relative scale of the object. In the\n",
    "case of RetinaNet, each location on a given feature map has nine anchor boxes\n",
    "(at three scales and three ratios).\n",
    "\"\"\"\n",
    "class AnchorBox:\n",
    "    \"\"\"Generates anchor boxes.\n",
    "    This class has operations to generate anchor boxes for feature maps at\n",
    "    strides `[8, 16, 32, 64, 128]`. Where each anchor each box is of the\n",
    "    format `[x, y, width, height]`.\n",
    "    Attributes:\n",
    "      aspect_ratios: A list of float values representing the aspect ratios of\n",
    "        the anchor boxes at each location on the feature map\n",
    "      scales: A list of float values representing the scale of the anchor boxes\n",
    "        at each location on the feature map.\n",
    "      num_anchors: The number of anchor boxes at each location on feature map\n",
    "      areas: A list of float values representing the areas of the anchor\n",
    "        boxes for each feature map in the feature pyramid.\n",
    "      strides: A list of float value representing the strides for each feature\n",
    "        map in the feature pyramid.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.level_start = level_start\n",
    "        self.level_end = level_end\n",
    "        \n",
    "        if anchor_k==9:\n",
    "            self.aspect_ratios = [0.5, 1.0, 2.0]        \n",
    "            self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\n",
    "        else:\n",
    "            self.aspect_ratios = [1.0]        \n",
    "            self.scales = [2 ** x for x in [0]]\n",
    "                \n",
    "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
    "        self._strides = [2 ** i for i in range(self.level_start, self.level_end)]\n",
    "        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 196.0, 256.0]]                        \n",
    "        self._areas = self._areas[:level_end - level_start]\n",
    "        \n",
    "        self._anchor_dims = self._compute_dims()\n",
    "\n",
    "    def _compute_dims(self):\n",
    "        \"\"\"Computes anchor box dimensions for all ratios and scales at all levels\n",
    "        of the feature pyramid.\n",
    "        \"\"\"\n",
    "        anchor_dims_all = []\n",
    "        for area in self._areas:\n",
    "            anchor_dims = []\n",
    "            for ratio in self.aspect_ratios:\n",
    "                anchor_height = tf.math.sqrt(area / ratio)\n",
    "                anchor_width = area / anchor_height\n",
    "                dims = tf.reshape(\n",
    "                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n",
    "                )\n",
    "                for scale in self.scales:\n",
    "                    anchor_dims.append(scale * dims)\n",
    "            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n",
    "        return anchor_dims_all\n",
    "\n",
    "    def _get_anchors(self, feature_height, feature_width, level):\n",
    "        \"\"\"Generates anchor boxes for a given feature map size and level\n",
    "        Arguments:\n",
    "          feature_height: An integer representing the height of the feature map.\n",
    "          feature_width: An integer representing the width of the feature map.\n",
    "          level: An integer representing the level of the feature map in the\n",
    "            feature pyramid.\n",
    "        Returns:\n",
    "          anchor boxes with the shape\n",
    "          `(feature_height * feature_width * num_anchors, 4)`\n",
    "        \"\"\"\n",
    "        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n",
    "        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n",
    "        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - self.level_start]\n",
    "        centers = tf.expand_dims(centers, axis=-2)\n",
    "        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
    "        dims = tf.tile(\n",
    "            self._anchor_dims[level - self.level_start], [feature_height, feature_width, 1, 1]\n",
    "        )\n",
    "        anchors = tf.concat([centers, dims], axis=-1)\n",
    "        return tf.reshape(\n",
    "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
    "        )\n",
    "\n",
    "    def get_anchors(self, image_height, image_width):\n",
    "        \"\"\"Generates anchor boxes for all the feature maps of the feature pyramid.\n",
    "        Arguments:\n",
    "          image_height: Height of the input image.\n",
    "          image_width: Width of the input image.\n",
    "        Returns:\n",
    "          anchor boxes for all the feature maps, stacked as a single tensor\n",
    "            with shape `(total_anchors, 4)`\n",
    "        \"\"\"\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2 ** i),\n",
    "                tf.math.ceil(image_width / 2 ** i),\n",
    "                i,\n",
    "            )\n",
    "            for i in range(self.level_start, self.level_end)\n",
    "        ]\n",
    "        return tf.concat(anchors, axis=0)\n",
    "    \n",
    "    def get_anchors_check(self, image_height, image_width):\n",
    "        \"\"\"Generates anchor boxes for all the feature maps of the feature pyramid.\n",
    "        Arguments:\n",
    "          image_height: Height of the input image.\n",
    "          image_width: Width of the input image.\n",
    "        Returns:\n",
    "          anchor boxes for all the feature maps, stacked as a single tensor\n",
    "            with shape `(total_anchors, 4)`\n",
    "        \"\"\"\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2 ** i),\n",
    "                tf.math.ceil(image_width / 2 ** i),\n",
    "                i,\n",
    "            )\n",
    "            for i in range(self.level_start, self.level_end)\n",
    "        ]\n",
    "        return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_image_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_pixel(net):\n",
    "    # net # (m, h, w, c)    \n",
    "    n0, n1, n2, n3 = tf.split(net, 4, -1)\n",
    "    n01 = tf.stack((n0, n1), 3) # (m, h, w, 2, c)\n",
    "    n23 = tf.stack((n2, n3), 3) # (m, h, w, 2, c)\n",
    "\n",
    "    n01234 = tf.stack((n01, n23), 2)# (m, h, 2, w, 2, c)\n",
    "    out = tf.reshape(n01234, [-1])#seg\n",
    "    return out\n",
    "\n",
    "\n",
    "def shuffle_pixel_2x(net, h, w, c):\n",
    "    # net # (m, h, w, c)\n",
    "    #n0, n1 = tf.split(net, 2, -1)\n",
    "    n0, n1, n2, n3 = tf.split(net, 4, -1)\n",
    "    n01 = tf.stack((n0, n1), 3) # (m, h, w, 2, c)\n",
    "    n23 = tf.stack((n2, n3), 3) # (m, h, w, 2, c)\n",
    "\n",
    "    n01234 = tf.stack((n01, n23), 2)# (m, h, 2, w, 2, c)\n",
    "    out = tf.reshape(n01234, [-1, h*2, w*2, c//4])\n",
    "    return out\n",
    "\n",
    "\n",
    "def shuffle_pixel_3x(net, h, w):\n",
    "    # net # (m, h, w, c)\n",
    "    #n0, n1 = tf.split(net, 2, -1)\n",
    "    n0, n1, n2, n3, n4, n5, n6, n7, n8 = tf.split(net, 9, -1)\n",
    "    r0 = tf.stack((n0, n4, n1), 3) # (m, h, w, 2, c)\n",
    "    r1 = tf.stack((n5, n6, n7), 3) # (m, h, w, 2, c)\n",
    "    r2 = tf.stack((n2, n8, n3), 3) # (m, h, w, 2, c)\n",
    "\n",
    "    r = tf.stack((r0, r1, r2), 2)# (m, h, 2, w, 2, c)\n",
    "    out = tf.reshape(r, [-1, h*3, w*3, 1])\n",
    "    return out\n",
    "\n",
    "\n",
    "def shuffle_pixel_4x(net):\n",
    "    # net # (m, h, w, c)\n",
    "    #n0, n1 = tf.split(net, 2, -1)\n",
    "    net_split = tf.split(net, 16, -1)\n",
    "    r0 = tf.stack(net_split[0:4], 3) # (m, h, w, 2, c)\n",
    "    r1 = tf.stack(net_split[4:8], 3) # (m, h, w, 2, c)\n",
    "    r2 = tf.stack(net_split[8:12], 3)\n",
    "    r3 = tf.stack(net_split[12:16], 3)\n",
    "\n",
    "    r = tf.stack((r0, r1, r2, r3), 2)# (m, h, 4, w, 2, c)\n",
    "    #out = tf.reshape(r, [-1, h*4, w*4, 1])\n",
    "    out = tf.reshape(r, [-1])\n",
    "    return out\n",
    "\n",
    "\n",
    "def shuffle_pixel_5x(net, h, w, c=1):\n",
    "    # net # (m, h, w, c)\n",
    "    k = 5\n",
    "    net_split = tf.split(net, k*k, -1)\n",
    "    r0 = tf.stack(net_split[k * 0:k * 1], 3)\n",
    "    r1 = tf.stack(net_split[k * 1:k * 2], 3)\n",
    "    r2 = tf.stack(net_split[k * 2:k * 3], 3)\n",
    "    r3 = tf.stack(net_split[k * 3:k * 4], 3)\n",
    "    r4 = tf.stack(net_split[k * 4:k * 5], 3)\n",
    "\n",
    "    r = tf.stack((r0, r1, r2, r3, r4), 2)\n",
    "    out = tf.reshape(r, [-1, h*k, w*k, c])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_color_augment(x):\n",
    "    if tf.random.uniform(()) < -0.5:\n",
    "        x_max = tf.reduce_max(x, [1, 2], True)\n",
    "        x = x_max - x\n",
    "    if tf.random.uniform(()) < -0.2:\n",
    "        r, g, b = tf.split(x, 3, axis=-1)\n",
    "        x = tf.concat((r, b, g), -1)\n",
    "    elif tf.random.uniform(()) < -0.4:\n",
    "        r, g, b = tf.split(x, 3, axis=-1)\n",
    "        x = tf.concat((b, r, g), -1)\n",
    "    if tf.random.uniform(()) < 0.2:\n",
    "        x = tf.image.random_hue(x, 0.08)\n",
    "        x = tf.image.random_saturation(x, 0.6, 1.6)\n",
    "    if tf.random.uniform(()) < 0.2:\n",
    "        x = tf.image.random_brightness(x, 0.05)\n",
    "        x = tf.image.random_contrast(x, 0.7, 1.3)\n",
    "    if tf.random.uniform(()) < -0.2:\n",
    "        gray = tf.image.rgb_to_grayscale(x)\n",
    "        x = tf.concat((gray, gray, gray), -1)        \n",
    "   \n",
    "    if tf.random.uniform(()) < -0.2:\n",
    "        x = gaussian_filter2d(x, filter_shape=tuple(np.random.randint(1, 10, (2))), sigma=10)\n",
    "        #x = gaussian_filter2d(x, filter_shape=np.random.randint(3, 10, (2)), sigma=10)\n",
    "    if tf.random.uniform(()) < 0.2:        \n",
    "        x = sharpness(x, factor=10)    \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_flip_horizontal(image, boxes):\n",
    "    \"\"\"Flips image and boxes horizontally with 50% chance\n",
    "    Arguments:\n",
    "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
    "        image.\n",
    "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes,\n",
    "        having normalized coordinates.\n",
    "    Returns:\n",
    "      Randomly flipped image and boxes\n",
    "    \"\"\"\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        boxes = tf.stack([1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]], axis=-1)\n",
    "   \n",
    "    return image, boxes\n",
    "\n",
    "def resize_and_pad_image(\n",
    "    image, min_side=512.0, jitter=[128*4, 128*4+1], stride=128.0):\n",
    "   \n",
    "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "    \n",
    "    ratio = min_side / tf.reduce_min(image_shape)\n",
    "    \n",
    "    image_shape = ratio * image_shape\n",
    "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
    "    \n",
    "    image = tf.image.pad_to_bounding_box(image, 0, 0, padded_image_shape[0], padded_image_shape[1]) \n",
    "    \n",
    "    return image, image_shape, ratio\n",
    "\n",
    "def resize_and_pad_image_bbox(\n",
    "    image, bbox, mask_obj=None, min_side=1024.0, max_side=1024.0*4, jitter=[128*7+32, 128*8-32], stride=128.0\n",
    "):\n",
    "    #image, min_side=800.0, max_side=1333.0, jitter=[640, 1024], stride=128.0\n",
    "    \"\"\"Resizes and pads image while preserving aspect ratio.\n",
    "    1. Resizes images so that the shorter side is equal to `min_side`\n",
    "    2. If the longer side is greater than `max_side`, then resize the image\n",
    "      with longer side equal to `max_side`\n",
    "    3. Pad with zeros on right and bottom to make the image shape divisible by\n",
    "    `stride`\n",
    "    Arguments:\n",
    "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
    "        image.\n",
    "      min_side: The shorter side of the image is resized to this value, if\n",
    "        `jitter` is set to None.\n",
    "      max_side: If the longer side of the image exceeds this value after\n",
    "        resizing, the image is resized such that the longer side now equals to\n",
    "        this value.\n",
    "      jitter: A list of floats containing minimum and maximum size for scale\n",
    "        jittering. If available, the shorter side of the image will be\n",
    "        resized to a random value in this range.\n",
    "      stride: The stride of the smallest feature map in the feature pyramid.\n",
    "        Can be calculated using `image_size / feature_map_size`.\n",
    "    Returns:\n",
    "      image: Resized and padded image.\n",
    "      image_shape: Shape of the image before padding.\n",
    "      ratio: The scaling factor used to resize the image\n",
    "    \"\"\"\n",
    "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "    if jitter is not None:\n",
    "        min_side = tf.random.uniform((), jitter[0], jitter[1], dtype=tf.float32)\n",
    "    ratio = min_side / tf.reduce_min(image_shape)\n",
    "    if ratio * tf.reduce_max(image_shape) > max_side:\n",
    "        ratio = max_side / tf.reduce_max(image_shape)\n",
    "    image_shape = ratio * image_shape\n",
    "    ratio_jitter = tf.random.uniform(tf.shape(image_shape), -32, 32, dtype=tf.float32)\n",
    "    image_shape += ratio_jitter      \n",
    "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
    "    if mask_obj!=None:\n",
    "        mask_obj = tf.image.resize(mask_obj, tf.cast(image_shape, dtype=tf.int32))\n",
    "    padded_image_shape = tf.cast(\n",
    "        tf.math.ceil(image_shape / stride) * stride, dtype=tf.int32\n",
    "    )\n",
    "    image = tf.image.pad_to_bounding_box(image, 0, 0, padded_image_shape[0], padded_image_shape[1])\n",
    "    if mask_obj!=None:\n",
    "        mask_obj = tf.image.pad_to_bounding_box(mask_obj, 0, 0, padded_image_shape[0], padded_image_shape[1])        \n",
    "    padded_image_shape = tf.cast(padded_image_shape, tf.float32)              \n",
    "    pad_ratio = tf.cast(image_shape, tf.float32) / padded_image_shape\n",
    "    bbox_padded = tf.stack(\n",
    "        [\n",
    "            bbox[:, 0] * pad_ratio[1],\n",
    "            bbox[:, 1] * pad_ratio[0],\n",
    "            bbox[:, 2] * pad_ratio[1],\n",
    "            bbox[:, 3] * pad_ratio[0],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    if mask_obj!=None:\n",
    "        return image, padded_image_shape, ratio, bbox_padded, mask_obj    \n",
    "    return image, padded_image_shape, ratio, bbox_padded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Preprocessing data\n",
    "Preprocessing the images involves two steps:\n",
    "- Resizing the image: Images are resized such that the shortest size is equal\n",
    "to 800 px, after resizing if the longest side of the image exceeds 1333 px,\n",
    "the image is resized such that the longest size is now capped at 1333 px.\n",
    "- Applying augmentation: Random scale jittering  and random horizontal flipping\n",
    "are the only augmentations applied to the images.\n",
    "Along with the images, bounding boxes are rescaled and flipped if required.\n",
    "\"\"\"\n",
    "\n",
    "def unnormalize_box(bbox, image_shape):\n",
    "    img_h = image_shape[0]\n",
    "    img_w = image_shape[1]\n",
    "    bbox = tf.stack(\n",
    "        [\n",
    "            bbox[:, 0] * img_w,\n",
    "            bbox[:, 1] * img_h,\n",
    "            bbox[:, 2] * img_w,\n",
    "            bbox[:, 3] * img_h,\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    bbox = convert_to_xywh(bbox)    \n",
    "    return bbox    \n",
    "\n",
    "\n",
    "def unnormalize_keypoint(keypoint, image_shape):\n",
    "    img_h = image_shape[0]\n",
    "    img_w = image_shape[1]\n",
    "    keypoint_un = tf.stack(\n",
    "        [\n",
    "            keypoint[:, :, 0] * img_w,\n",
    "            keypoint[:, :, 1] * img_h,\n",
    "            keypoint[:, :, 2]\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )    \n",
    "    \n",
    "    keypoint_un = tf.cast(keypoint_un, tf.float32)\n",
    "    return keypoint_un\n",
    "\n",
    "\n",
    "def resize_image(image):\n",
    "    image_shape = tf.cast(tf.shape(image)[0:2], tf.float32)    \n",
    "    resized_ratio = tf.cast(padded_image_shape / image_shape, tf.float32)\n",
    "    \n",
    "    image = tf.image.resize(image, padded_image_shape)    \n",
    "    return image, resized_ratio\n",
    "\n",
    "\n",
    "def preprocess_data(image, label):     \n",
    "     \n",
    "    #label (m,56)\n",
    "    bbox_norm = label[:, :4]    \n",
    "    cls = label[:, 4]\n",
    "    keypoints = label[:, 5:]\n",
    "    keypoint_3d = tf.reshape(keypoints, [-1, NUM_KEYPOINT, NUM_KEYPOINT_CH])    \n",
    "    keypoint_3d = tf.cast(keypoint_3d, tf.float32)\n",
    "\n",
    "    #image, bbox = random_flip_horizontal(image, bbox)        \n",
    "    #image, image_shape, _, bbox = resize_and_pad_image_bbox(image, bbox)    \n",
    "    #image, image_shape, _ = resize_and_pad_image(image)\n",
    "    \n",
    "    image, image_shape = resize_image(image)\n",
    "    bbox_unnorm = unnormalize_box(bbox_norm, image_shape)\n",
    "    keypoints_unnorm = unnormalize_keypoint(keypoint_3d, image_shape)\n",
    "    #print('keypoints_unnorm', keypoints_unnorm)\n",
    "    \n",
    "    return image, bbox_unnorm, cls, keypoints_unnorm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_label, NUM_KEYPOINT, NUM_KEYPOINT_CH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():\n",
    "    #ind = np.arange((len(list_x_train)//2)*2)\n",
    "    #ind = np.arange(len(list_x_train))    \n",
    "    for i in range(len(list_x_train)):\n",
    "        x = list_x_train[i]\n",
    "        y = list_y_train[i]        \n",
    "        yield (x, y)\n",
    "\n",
    "def generator_test():    \n",
    "    for i in range(len(list_x_test)):\n",
    "        x = list_x_test[i]\n",
    "        y = list_y_test[i]        \n",
    "        yield (x, y)\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    generator, \n",
    "    output_types=(tf.uint8, tf.float32), \n",
    "    output_shapes=(tf.TensorShape([None, None, 3]), tf.TensorShape([None, channel_label])))\n",
    "dataset_test = tf.data.Dataset.from_generator(\n",
    "    generator_test, \n",
    "    output_types=(tf.uint8, tf.float32), \n",
    "    output_shapes=(tf.TensorShape([None, None, 3]), tf.TensorShape([None, channel_label])))\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "for example in tfds.as_numpy(dataset):\n",
    "    image = example[0]\n",
    "    label = example[1]\n",
    "    print(image.dtype, image.shape, label.shape)\n",
    "    print(label[0])\n",
    "    #print(label[0][-NUM_KEYPOINT*3::3])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 확인\n",
    "train_dataset = dataset.map(preprocess_data)\n",
    "for x, yb, yc, yk in train_dataset:\n",
    "    print('yb', yb)\n",
    "    print('yc', yc)\n",
    "    print('yk', yk)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinate_map_uv(h, w):\n",
    "    #return (6, 18, 256)\n",
    "    x = tf.range(0.5, w, 1) / tf.cast(w, tf.float32) * 2.0 -1\n",
    "    y = tf.range(0.5, h, 1) / tf.cast(h, tf.float32) * 2.0 -1    \n",
    "    X, Y = tf.meshgrid(x, y)\n",
    "    xy = tf.stack((X, Y), -1)\n",
    "    xy = tf.expand_dims(xy, axis=0)   \n",
    "    return xy \n",
    "\n",
    "def coordinate_map(h, w):\n",
    "    #return (1, h, w, 2)    \n",
    "    x = tf.range(w)\n",
    "    y = tf.range(h)\n",
    "    X, Y = tf.meshgrid(x, y)\n",
    "    xy = tf.stack((X, Y), -1)\n",
    "    xy = tf.cast(xy, tf.float32)\n",
    "    return xy \n",
    "\n",
    "def add_map(net):\n",
    "    shape = tf.shape(net)\n",
    "    map_norm = coordinate_map(shape[1], shape[2])        \n",
    "    map_square = tf.sqrt(map_norm)\n",
    "    net = tf.concat((map_norm + net[:, :, :, :2], net[:, :, :, 2:]), -1)    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Encoding labels\n",
    "The raw labels, consisting of bounding boxes and class ids need to be\n",
    "transformed into targets for training. This transformation consists of\n",
    "the following steps:\n",
    "- Generating anchor boxes for the given image dimensions\n",
    "- Assigning ground truth boxes to the anchor boxes\n",
    "- The anchor boxes that are not assigned any objects, are either assigned the\n",
    "background class or ignored depending on the IOU\n",
    "- Generating the classification and regression targets using anchor boxes\n",
    "\"\"\"\n",
    "\n",
    "class LabelEncoder:\n",
    "    \"\"\"Transforms the raw labels into targets for training.\n",
    "    This class has operations to generate targets for a batch of samples which\n",
    "    is made up of the input images, bounding boxes for the objects present and\n",
    "    their class ids.\n",
    "    Attributes:\n",
    "      anchor_box: Anchor box generator to encode the bounding boxes.\n",
    "      box_variance: The scaling factors used to scale the bounding box targets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._anchor_box = AnchorBox()\n",
    "        self._box_variance = tf.convert_to_tensor(\n",
    "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
    "        )           \n",
    "    \n",
    "    def _match_anchor_boxes(\n",
    "        self, anchor_boxes, gt_boxes, match_iou=0.3, ignore_iou=0.1\n",
    "    ):\n",
    "        # anchor : (anchor_k, 4), gt_boxes : (box_m, 4)\n",
    "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)#(anchor_k, box_m)        \n",
    "        max_iou = tf.reduce_max(iou_matrix, axis=1)#from anchor to object-box        \n",
    "        matched_gt_idx = tf.argmax(iou_matrix, axis=1)    \n",
    "        positive_mask = tf.greater_equal(max_iou, match_iou)# not only this, but also need max iou cell\n",
    "        \n",
    "        positive_proposal_mask = tf.greater_equal(iou_matrix, match_iou)\n",
    "        positive_mask = tf.reduce_any(positive_proposal_mask, axis=1)        \n",
    "        negative_mask = tf.less(max_iou, ignore_iou)\n",
    "        \n",
    "        max_iou_anchor = tf.reduce_max(iou_matrix, axis=0, keepdims=True)\n",
    "        max_iou_anchor_mask = tf.greater_equal(iou_matrix, max_iou_anchor)\n",
    "        \n",
    "        positive_max_mask = tf.reduce_any(max_iou_anchor_mask, axis=1)\n",
    "        positive_mask = tf.logical_or(positive_mask, positive_max_mask)#new      \n",
    "        \n",
    "        negative_mask = tf.logical_and(negative_mask, tf.logical_not(positive_mask))\n",
    "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))        \n",
    "        \n",
    "        return (\n",
    "            matched_gt_idx,            \n",
    "            tf.cast(positive_mask, dtype=tf.float32),\n",
    "            tf.cast(positive_max_mask, dtype=tf.float32),            \n",
    "            tf.cast(ignore_mask, dtype=tf.float32),\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
    "        \"\"\"Transforms the ground truth boxes into targets for training\"\"\"\n",
    "        target = tf.concat(\n",
    "            [\n",
    "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
    "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        target = target / self._box_variance\n",
    "        return target\n",
    "    \n",
    "    def _compute_keypoint_target(self, anchor_boxes, matched_gt_points):\n",
    "        \"\"\"Transforms the ground truth boxes into targets for training\"\"\"\n",
    "        target = (matched_gt_points[:, :, :2] - anchor_boxes[:, None, :2]) / anchor_boxes[:, None, 2:]        \n",
    "        target = target / self._box_variance[:2]\n",
    "        return target    \n",
    "      \n",
    "    \n",
    "    def _encode_sample(self, image_shape, gt_boxes, cls_ids, gt_keypoints):\n",
    "        \"\"\"Creates box and classification targets for a single sample\"\"\"\n",
    "        \n",
    "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
    "        \n",
    "        matched_gt_idx, positive_mask, positive_max_mask, ignore_mask = self._match_anchor_boxes(\n",
    "            anchor_boxes, gt_boxes)\n",
    "        \n",
    "        gt_keypoints_xy = gt_keypoints[:, :, :2]\n",
    "        gt_keypoints_cls = gt_keypoints[:, :, 2]       \n",
    "        \n",
    "        print('gt_keypoints_xy', gt_keypoints_xy)\n",
    "        \n",
    "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
    "        matched_gt_keypoint_xy = tf.gather(gt_keypoints_xy, matched_gt_idx)\n",
    "        \n",
    "        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)\n",
    "        keypoint_xy_target = self._compute_keypoint_target(anchor_boxes, matched_gt_keypoint_xy)\n",
    "                \n",
    "        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
    "        matched_gt_keypoint_cls = tf.gather(gt_keypoints_cls, matched_gt_idx)\n",
    "                \n",
    "        cls_target = tf.where(tf.not_equal(positive_mask, 1.0), 0.0, matched_gt_cls_ids)        \n",
    "        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -1.0, cls_target)\n",
    "                \n",
    "        #[?,4], [?,?], [?,17,2], []\n",
    "        keypoint_xy_target = tf.reshape(keypoint_xy_target, [-1, NUM_KEYPOINT * 2])\n",
    "        keypoint_cls_target = tf.reshape(matched_gt_keypoint_cls, [-1, NUM_KEYPOINT])\n",
    "                \n",
    "        cls_target = tf.expand_dims(cls_target, axis=-1)\n",
    "        targets = tf.concat([box_target, cls_target, keypoint_xy_target, keypoint_cls_target], axis=-1)\n",
    "        return targets\n",
    "    \n",
    "    \n",
    "    def _encode_keypoint_heatmap(self, img_h, img_w, keypoints):\n",
    "        #keypoints (n, 17, 3)\n",
    "        #return gt_map (h, w)        \n",
    "        neighbor_k = 4\n",
    "        keypoints_xy = keypoints[:, :, :2]\n",
    "        keypoints_visible = keypoints[:, :, 2]\n",
    "        keypoints_cls = tf.zeros_like(keypoints_visible) + tf.range(NUM_KEYPOINT, dtype=tf.float32)\n",
    "                \n",
    "        xy_map = coordinate_map(img_h, img_w) + 0.5                \n",
    "        \n",
    "        keypoints_xy_5d = tf.reshape(keypoints_xy, [1, 1, -1, NUM_KEYPOINT, 2])#(1, 1, n, 17, 2)        \n",
    "        keypoints_visible_5d = tf.reshape(keypoints_visible, [1, 1, -1, NUM_KEYPOINT, 1])\n",
    "        keypoints_visible_4d_mask = tf.cast(keypoints_visible_5d[:,:,:,:,0] > 1, tf.float32)\n",
    "                \n",
    "        keypoints_xy_5d_to_find_min = keypoints_xy_5d + 5000 * tf.cast(keypoints_visible_5d < 1,tf.float32)\n",
    "        visible_max_xy = tf.reduce_max(keypoints_xy_5d, 3, True)\n",
    "        visible_min_xy = tf.reduce_min(keypoints_xy_5d_to_find_min, 3, True)\n",
    "        visible_xy_range = tf.abs(visible_max_xy - visible_min_xy)#(1, 1, n, 1, 2)\n",
    "        \n",
    "        neighbor_k_relative = neighbor_k + 0.04 * tf.reduce_mean(visible_xy_range, -1)#(1, 1, n, 1)\n",
    "        neighbor_k_relative = neighbor_k_relative + (4 * keypoints_visible_4d_mask)\n",
    "        \n",
    "        xy_map_exp = tf.expand_dims(tf.expand_dims(xy_map, 2), 2)\n",
    "        distance_xy = xy_map_exp - keypoints_xy_5d#(h, w, 1, 1, 2) - (1, 1, n, 17, 2)               \n",
    "        distance = tf.sqrt(tf.reduce_sum(tf.square(distance_xy), -1))#(h, w, n, 17)\n",
    "        \n",
    "        #[?,?,n,17], [1,1,n,1]                \n",
    "        disk_mask = tf.cast(distance < neighbor_k_relative, tf.float32) * keypoints_visible_4d_mask\n",
    "        bg_zero = tf.zeros_like(distance[:,:,:,:1])\n",
    "                \n",
    "        disk_exist = tf.reduce_sum(disk_mask, [2, 3], True) > 0\n",
    "        disk_exist_mask = tf.cast(disk_exist, tf.float32)\n",
    "        bg_map = disk_exist_mask * 10000 + (1 - disk_exist_mask) * (bg_zero - 10000)\n",
    "        \n",
    "        distance_map_with_bg = tf.concat((bg_map, distance), -1)        \n",
    "        distance_map_with_bg = tf.reshape(distance_map_with_bg, [img_h, img_w, -1]) \n",
    "        \n",
    "        cls_map = tf.argmin(distance_map_with_bg, -1, output_type=tf.int32)#(h, w, n)                \n",
    "        cls_k = NUM_KEYPOINT + 1\n",
    "        cls_map = cls_map % cls_k        \n",
    "        return cls_map\n",
    "    \n",
    "    def encode_batch(self, batch_images, gt_boxes, cls_ids, keypoints):\n",
    "        \"\"\"Creates box and classification targets for a batch\"\"\"\n",
    "        #keypoints(batch_m, n, 17, 3)\n",
    "        images_shape = tf.shape(batch_images)\n",
    "        batch_size = images_shape[0]\n",
    "        \n",
    "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
    "        labels_heatmap = tf.TensorArray(dtype=tf.int32, size=batch_size, dynamic_size=True)\n",
    "        for i in range(batch_size):\n",
    "            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i], keypoints[i])\n",
    "            labels = labels.write(i, label)\n",
    "            label_heat = self._encode_keypoint_heatmap(images_shape[1], images_shape[2], keypoints[i])        \n",
    "            labels_heatmap = labels_heatmap.write(i, label_heat)\n",
    "        \n",
    "        batch_images = tf.cast(batch_images, tf.float32)\n",
    "        labels = labels.stack()\n",
    "        labels_heatmap = labels_heatmap.stack()\n",
    "        \n",
    "        #return batch_images, labels, labels_hitmap\n",
    "        return batch_images, {\"detect\": labels, \"heatmap\": labels_heatmap}#dual\n",
    "     \n",
    "    def encode_batch_train(self, batch_images, gt_boxes, cls_ids, keypoints):\n",
    "        \n",
    "        batch_images = image_color_augment(batch_images)\n",
    "        return self.encode_batch(batch_images, gt_boxes, cls_ids, keypoints)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_start, 2**level_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "anchor_k = len(label_encoder._anchor_box.aspect_ratios)*len(label_encoder._anchor_box.scales)\n",
    "anchor_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "#strategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = min(strategy.num_replicas_in_sync, len(list_x_train))\n",
    "autotune = tf.data.experimental.AUTOTUNE\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_dataset = dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
    "print('check_dataset', check_dataset)\n",
    "for image, bbox_unnorm, cls, keypoints_unnorm in check_dataset:\n",
    "    print(image.shape, bbox_unnorm.shape, cls.shape, keypoints_unnorm.shape)\n",
    "    print('bbox_unnorm', bbox_unnorm)\n",
    "    print('cls', cls)\n",
    "    print('keypoints_unnorm', keypoints_unnorm)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
    "#train_dataset = train_dataset.shuffle(8 * batch_size)\n",
    "#train_dataset = train_dataset.padded_batch(batch_size=batch_size, padding_values=(0.0, 1e-8, -1), drop_remainder=False)\n",
    "train_dataset = train_dataset.padded_batch(batch_size=batch_size)\n",
    "train_dataset = train_dataset.map(label_encoder.encode_batch_train, num_parallel_calls=autotune)\n",
    "train_dataset = train_dataset.prefetch(autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, multi_y in train_dataset:\n",
    "    #print('multi_y', type(multi_y), multi_y.keys())\n",
    "    y = multi_y['detect']\n",
    "    y_heatmap = multi_y['heatmap']\n",
    "    #print('x', x.shape)\n",
    "    #print('y', y.shape)\n",
    "    #print('heatmap', heatmap.shape)\n",
    "    print('heatmap.max()', tf.shape(y_heatmap), tf.reduce_max(y_heatmap))\n",
    "    \n",
    "    img_map_xy = tf.cast(y_heatmap[0], tf.uint8)\n",
    "    alpha = 255//(NUM_KEYPOINT+1)\n",
    "    hitmap_h = img_map_xy * alpha\n",
    "    hitmap_img = tf.stack((hitmap_h, hitmap_h, hitmap_h), -1)\n",
    "    fig, ax = plt.subplots(1, 2)    \n",
    "    ax[0].imshow(tf.cast(x[0], tf.uint8))\n",
    "    ax[1].imshow(hitmap_h, cmap='gray')    \n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = dataset_test.map(preprocess_data, num_parallel_calls=autotune)\n",
    "#val_dataset = val_dataset.padded_batch(batch_size=batch_size, padding_values=(0.0, 1e-8, -1), drop_remainder=False)\n",
    "val_dataset = val_dataset.padded_batch(batch_size=1)\n",
    "val_dataset = val_dataset.map(label_encoder.encode_batch, num_parallel_calls=autotune)\n",
    "val_dataset = val_dataset.prefetch(autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.cast([[1,2]], tf.float32)\n",
    "b = tf.constant((1,4))\n",
    "tf.tile(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.cast([[[1,2]]], tf.float32)\n",
    "b = tf.constant((1,1, 4))\n",
    "tf.tile(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_gt(y_true):\n",
    "    #targets = tf.concat([box_target, cls_target, keypoint_xy_target, keypoint_cls_target], axis=-1)\n",
    "    y_box = y_true[:, :, :4]\n",
    "    y_cls = y_true[:, :, 4]\n",
    "    y_keypoint = y_true[:, :, 5:]\n",
    "    return y_box, y_cls, y_keypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filters = 4 + 1 + num_classes + num_keypoints * (2 + num_keypoint_classes)       \n",
    "def split_hyperthesis(y_pred):\n",
    "    h_box = y_pred[:, :, :4]\n",
    "    h_obj = y_pred[:, :, 4]\n",
    "    h_cls = y_pred[:, :, 5:5+num_classes]        \n",
    "    h_keypoint = y_pred[:, :, 5+num_classes:]       \n",
    "    \n",
    "    h_obj = tf.nn.sigmoid(h_obj)\n",
    "    return h_box, h_obj, h_cls, h_keypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_gt_keypoint(y_keypoint):\n",
    "    xy = y_keypoint[:, :, :NUM_KEYPOINT*2]\n",
    "    cls = y_keypoint[:, :, -NUM_KEYPOINT:]\n",
    "        \n",
    "    x = xy[:, :, 0::2]\n",
    "    y = xy[:, :, 1::2]    \n",
    "    return x, y, cls    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_h_keypoint(h_keypoint):    \n",
    "    h_x = h_keypoint[:, :, :NUM_KEYPOINT]\n",
    "    h_y = h_keypoint[:, :, NUM_KEYPOINT:NUM_KEYPOINT * 2]\n",
    "    h_cls_score = h_keypoint[:, :, NUM_KEYPOINT * 2:]\n",
    "    \n",
    "    #h_x = tf.tanh(h_x)\n",
    "    #h_y = tf.tanh(h_y)\n",
    "    \n",
    "    keypoints_cls = tf.round(tf.sigmoid(h_cls_score) * 2)\n",
    "    return h_x, h_y, h_cls_score, keypoints_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decode_box_predictions(anchor_boxes, box_predictions):        \n",
    "    boxes = box_predictions * label_encoder._box_variance\n",
    "    boxes = tf.concat(\n",
    "        [\n",
    "            boxes[:, :, :2] * anchor_boxes[:, :, 2:] + anchor_boxes[:, :, :2],\n",
    "            tf.math.exp(boxes[:, :, 2:]) * anchor_boxes[:, :, 2:],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    boxes_transformed = convert_to_corners(boxes)\n",
    "    return boxes_transformed\n",
    "\n",
    "def _decode_keypoint_predictions(anchor_boxes, keypoint_predictions):        \n",
    "    repeat = tf.constant([NUM_KEYPOINT], tf.int32)\n",
    "    box_variance_rep = tf.tile(label_encoder._box_variance[:2], repeat)\n",
    "    keypoints = keypoint_predictions * box_variance_rep\n",
    "    \n",
    "    batch_m = tf.shape(keypoints)[0]\n",
    "    repeat = tf.constant((1, 1, NUM_KEYPOINT))\n",
    "    \n",
    "    anchor_xy_tile = tf.tile(anchor_boxes[:, :, :2], repeat)\n",
    "    anchor_wh_tile = tf.tile(anchor_boxes[:, :, 2:], repeat)\n",
    "    \n",
    "    keypoint = keypoints * anchor_wh_tile + anchor_xy_tile\n",
    "    return keypoint\n",
    "\n",
    "\n",
    "def decode_debug(images, predictions, \n",
    "                      num_classes=num_classes,\n",
    "                      confidence_threshold=0.5,\n",
    "                      nms_iou_threshold=0.2,\n",
    "                      max_detections_per_class=1000,\n",
    "                      max_detections=1500,\n",
    "                      box_variance=[0.1, 0.1, 0.2, 0.2]):\n",
    "    \n",
    "    _anchor_box = AnchorBox()    \n",
    "        \n",
    "    image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n",
    "    #anchor_boxes = _anchor_box.get_anchors(image_shape[1], image_shape[2])#free size        \n",
    "    image_h = padded_image_shape[0]\n",
    "    image_w = padded_image_shape[1]\n",
    "    anchor_boxes = _anchor_box.get_anchors(image_h, image_w)\n",
    "    \n",
    "    box_predictions, objectness, keypoint_predictions = split_gt(predictions)\n",
    "    x, y, keypoints_cls = split_gt_keypoint(keypoint_predictions)\n",
    "    \n",
    "    m = tf.shape(images)[0]\n",
    "    keypoints_xy = tf.stack((x, y), -1)\n",
    "    keypoints_xy = tf.reshape(keypoints_xy, (m, -1, NUM_KEYPOINT * 2))\n",
    "       \n",
    "    cls_score = predictions[:, :, 5:5+num_classes]    \n",
    "    cls_prob = tf.nn.softmax(cls_score)\n",
    "    cls_prob_max = tf.reduce_max(cls_prob, -1)\n",
    "    cls = tf.argmax(cls_score, -1)\n",
    "    cls = tf.cast(cls, tf.float32)\n",
    "    cls = predictions[:, :, 4]\n",
    "            \n",
    "    boxes = _decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n",
    "    keypoints_xy = _decode_keypoint_predictions(anchor_boxes[None, ...], keypoints_xy)\n",
    "    \n",
    "    boxes_2d = tf.reshape(boxes, [-1, 4])    \n",
    "    scores = tf.reshape(objectness, [-1, 1])\n",
    "    \n",
    "    cls = tf.reshape(cls, [-1, 1])\n",
    "    \n",
    "    keypoints_xy_3d = tf.reshape(keypoints_xy, [-1, NUM_KEYPOINT, 2])\n",
    "    keypoints_cls_3d = tf.reshape(keypoints_cls, [-1, NUM_KEYPOINT, 1])    \n",
    "    keypoints_3d = tf.concat((keypoints_xy_3d, keypoints_cls_3d), -1)\n",
    "    keypoints_2d = tf.reshape(keypoints_3d, [-1, NUM_KEYPOINT * 3])\n",
    "    \n",
    "    ccbox = tf.concat((cls, scores, boxes_2d, keypoints_2d), -1)\n",
    "    ccbox_check = tf.concat((boxes_2d, cls, keypoints_2d), -1)\n",
    "    \n",
    "    selected_indices, selected_scores = tf.image.non_max_suppression_with_scores(    \n",
    "        ccbox[:, 2:2+4],\n",
    "        ccbox[:, 1],        \n",
    "        max_detections,\n",
    "        nms_iou_threshold,\n",
    "        confidence_threshold,        \n",
    "    )\n",
    "    output = tf.gather(ccbox_check, selected_indices)        \n",
    "    return output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodePredictions(images, predictions, \n",
    "                      num_classes=num_classes,\n",
    "                      confidence_threshold=0.5,\n",
    "                      nms_iou_threshold=0.2,\n",
    "                      max_detections_per_class=100,\n",
    "                      max_detections=150,\n",
    "                      box_variance=[0.1, 0.1, 0.2, 0.2]):\n",
    "    \n",
    "    _anchor_box = AnchorBox()    \n",
    "        \n",
    "    image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n",
    "    anchor_boxes = _anchor_box.get_anchors(image_shape[1], image_shape[2])#free size        \n",
    "    #image_h = padded_image_shape[0]\n",
    "    #image_w = padded_image_shape[1]\n",
    "    #anchor_boxes = _anchor_box.get_anchors(image_h, image_w)\n",
    "   \n",
    "    h_box, objectness, h_cls, h_keypoint = split_hyperthesis(predictions)    \n",
    "    h_x, h_y, h_cls_score, keypoints_cls = split_h_keypoint(h_keypoint)\n",
    "    \n",
    "    keypoints_xy = tf.stack((h_x, h_y), -1) #keypoint_predictions[:, :, :NUM_KEYPOINT * 2]\n",
    "    m = tf.shape(images)[0]\n",
    "    \n",
    "    keypoints_xy = tf.reshape(keypoints_xy, [m, -1, NUM_KEYPOINT * 2])\n",
    "    \n",
    "    cls_score = predictions[:, :, 5:5+num_classes]    \n",
    "    cls_prob = tf.nn.softmax(cls_score)\n",
    "    cls_prob_max = tf.reduce_max(cls_prob, -1)\n",
    "    cls = tf.argmax(cls_score, -1)\n",
    "    cls = tf.cast(cls, tf.float32)\n",
    "    cls = predictions[:, :, 4]\n",
    "            \n",
    "    boxes = _decode_box_predictions(anchor_boxes[None, ...], h_box)\n",
    "    keypoints_xy = _decode_keypoint_predictions(anchor_boxes[None, ...], keypoints_xy)\n",
    "    \n",
    "    boxes_2d = tf.reshape(boxes, [-1, 4])    \n",
    "    scores = tf.reshape(objectness, [-1, 1])\n",
    "    \n",
    "    cls = tf.reshape(cls, [-1, 1])\n",
    "    \n",
    "    keypoints_xy_3d = tf.reshape(keypoints_xy, [-1, NUM_KEYPOINT, 2])\n",
    "    keypoints_cls_3d = tf.reshape(keypoints_cls, [-1, NUM_KEYPOINT, 1])    \n",
    "    keypoints_3d = tf.concat((keypoints_xy_3d, keypoints_cls_3d), -1)\n",
    "    keypoints_2d = tf.reshape(keypoints_3d, [-1, NUM_KEYPOINT * 3])\n",
    "    \n",
    "    ccbox = tf.concat((cls, scores, boxes_2d, keypoints_2d), -1)\n",
    "    ccbox_check = tf.concat((boxes_2d, cls, keypoints_2d), -1)\n",
    "    \n",
    "    selected_indices, selected_scores = tf.image.non_max_suppression_with_scores(    \n",
    "        ccbox[:, 2:2+4],\n",
    "        ccbox[:, 1],        \n",
    "        max_detections,\n",
    "        nms_iou_threshold,\n",
    "        confidence_threshold,        \n",
    "    )\n",
    "    output = tf.gather(ccbox_check, selected_indices)        \n",
    "    return output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_check = AnchorBox()\n",
    "anchors = anchor_check.get_anchors_check(512,512)\n",
    "anchor_sum = 0\n",
    "for anchor in anchors:\n",
    "    print(anchor.shape, anchor[-1], 'sqrt', np.sqrt(anchor.shape[0]/anchor_k))\n",
    "    anchor_sum +=anchor.shape[0]\n",
    "print('anchor_sum', anchor_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, multi_y in val_dataset:\n",
    "    target = multi_y['detect']\n",
    "    heatmap = multi_y['heatmap']\n",
    "    print('image', image.shape)    \n",
    "    print('target', target.shape)# (1, 12096, 56)    \n",
    "    detections = decode_debug(image, target, confidence_threshold=0.15, nms_iou_threshold=0.2)\n",
    "    print('detections', detections.shape)\n",
    "    print(detections[0])\n",
    "    target_cp = target[:, :, 4+1:]\n",
    "    target_cp_xy = target_cp[:, :, :NUM_KEYPOINT*2]\n",
    "    print('target_cp', target_cp.shape)\n",
    "    #max tf.Tensor(187.73685, shape=(), dtype=float32) tf.Tensor(-219.20311, shape=(), dtype=float32)\n",
    "    print('max', tf.reduce_max(target_cp_xy), tf.reduce_min(target_cp_xy))#\n",
    "    break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(list_y_train[0][0].shape)\n",
    "print(list_y_train[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인코딩 라벨 어디감?\n",
    "- 65번 가운데 여자(iou thresh 높이면 나옴)\n",
    "- 73번 가운데 남자\n",
    "- 76번 할아버지(label positive 낮추면 나옴)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for image, multi_y in val_dataset:\n",
    "    i += 1\n",
    "    target = multi_y['detect']\n",
    "    heatmap = multi_y['heatmap']        \n",
    "    detections = decode_debug(image, target, confidence_threshold=0.25, nms_iou_threshold=0.9)\n",
    "    print(i, 'detections', detections.shape, 'target', target.shape)    \n",
    "    kp = detections[:, -NUM_KEYPOINT * NUM_KEYPOINT_CH:]\n",
    "    kp_cls = kp[:, 2::3]\n",
    "    print(detections.shape) #양복 2명 할아버지 (7, 56)\n",
    "    print(detections[:, :5])\n",
    "    x = np.array(image[0], np.uint8)\n",
    "    h = np.array(detections)\n",
    "    \n",
    "    img = Image.fromarray(x)        \n",
    "    draw_box_keypoint(img, h, is_show_class=False)\n",
    "    display(img)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netbase = keras.applications.EfficientNetB3(include_top=False, input_shape=[512, 512, 3])\n",
    "netbase.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backbone():\n",
    "    backbone = keras.applications.EfficientNetB3(include_top=False, input_shape=[None, None, 3])\n",
    "    c2_output, c3_output, c4_output, c5_output = [\n",
    "        backbone.get_layer(layer_name).output\n",
    "        for layer_name in [\"block2c_add\", \"block3c_add\", \"block5d_add\", \"top_activation\"]]#block5c_add, block6d_add    \n",
    "    return keras.Model(\n",
    "        inputs=[backbone.inputs], outputs=[c2_output, c3_output, c4_output, c5_output]\n",
    "    )\n",
    "\n",
    "#D0 for layer_name in [\"block2b_add\", \"block3b_add\", \"block5c_add\", \"block6d_add\"]]\n",
    "#B2 for layer_name in [\"block2c_add\", \"block3c_add\", \"block5d_add\", \"top_activation\"]]\n",
    "#B3 for layer_name in [\"block2c_add\", \"block3c_add\", \"block5e_add\", \"top_activation\"]]\n",
    "#D7 for layer_name in [\"block2f_add\", \"block3g_add\", \"block5j_add\", \"block6d_add\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BifeaturePyramidNet(c345, filters):    \n",
    "    a2 = c345[0]\n",
    "    a3 = c345[1]\n",
    "    a4 = c345[2]\n",
    "    a5 = c345[3]\n",
    "    \n",
    "    regulizer  = tf.keras.regularizers.L2(l1)\n",
    "    \n",
    "    a2_0 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a2)    \n",
    "    a33 = Conv2D(filters*2, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a3)\n",
    "    a44 = Conv2D(filters*2, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a4)\n",
    "    a55 = Conv2D(filters*2, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a5)\n",
    "    a66 = Conv2D(filters*2, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a5)\n",
    "    \n",
    "    a3_0, a3_1 = tf.split(a33, 2, -1)\n",
    "    a4_0, a4_1 = tf.split(a44, 2, -1)\n",
    "    a5_0, a5_1 = tf.split(a55, 2, -1)\n",
    "    a6_0, a6_1 = tf.split(a66, 2, -1)\n",
    "    \n",
    "    a7_1 = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a6_0)     \n",
    "    b7 = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a6_0)    \n",
    "    b6 = a6_0\n",
    "    \n",
    "    a6_up = keras.layers.UpSampling2D(2)(a6_1)    \n",
    "    b5 = keras.layers.Add()([a5_0, a6_up])  \n",
    "        \n",
    "    a5_up = keras.layers.UpSampling2D(2)(a5_1)    \n",
    "    b4 = keras.layers.Add()([a4_0, a5_up])  \n",
    "    \n",
    "    b4_up = keras.layers.UpSampling2D(2)(b4)\n",
    "    b3 = keras.layers.Add()([a3_0, b4_up])  \n",
    "    \n",
    "    b3_up = keras.layers.UpSampling2D(2)(b3)\n",
    "    b2 = keras.layers.Add()([a2_0, b3_up])\n",
    "    \n",
    "    b2_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(b2)\n",
    "    b3_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(b3)    \n",
    "    c3 = keras.layers.Add()([a3_1, b3_1, b2_down])\n",
    "    \n",
    "    c3_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(c3)\n",
    "    b4_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(b4)    \n",
    "    c4 = keras.layers.Add()([a4_1, b4_1, c3_down])    \n",
    "    \n",
    "    c4_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(c4)\n",
    "    b5_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(b5)    \n",
    "    c5 = keras.layers.Add()([a5_1, b5_1, c4_down])    \n",
    "    \n",
    "    c5_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(c5)\n",
    "    b6_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(b6)    \n",
    "    c6 = keras.layers.Add()([a6_1, b6_1, c5_down])\n",
    "    \n",
    "    c6_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(c6)\n",
    "    b7_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(b7)\n",
    "    c7 = keras.layers.Add()([a7_1, b7_1, c6_down])\n",
    "    return c3, c4, c5, c6, c7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**level_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heatmap(nets, img_h, img_w):\n",
    "    \n",
    "    ch = NUM_KEYPOINT + 1\n",
    "    heatmap_0 = nets[0][:, :, :, :ch]\n",
    "    heatmap_1 = nets[1][:, :, :, :ch]\n",
    "    heatmap_2 = nets[2][:, :, :, :ch]\n",
    "    stride = 2 ** level_start        \n",
    "    #heatmap_1 = shuffle_pixel_2x(heatmap_1, img_h//stride//2, img_w//stride//2, ch * 4)\n",
    "    \n",
    "    heatmap_fullsize_0 = tf.image.resize(heatmap_0, [img_h, img_w])\n",
    "    heatmap_fullsize_1 = tf.image.resize(heatmap_1, [img_h, img_w])\n",
    "    heatmap_fullsize_2 = tf.image.resize(heatmap_2, [img_h, img_w])\n",
    "    heatmap = heatmap_fullsize_0 + heatmap_fullsize_1 + heatmap_fullsize_2\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNet(num_classes, num_keypoints, num_keypoint_classes, anchor_k, is_train=False, is_backbone_train=True):    \n",
    "    inputs = Input(shape=(None, None, 3))            \n",
    "    prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
    "    kernel_init = tf.initializers.he_normal()\n",
    "    \n",
    "    backbone = get_backbone()\n",
    "    backbone.trainable = is_backbone_train\n",
    "    nets_3 = backbone(inputs, training=is_train) \n",
    "    ch = 256\n",
    "    p34567 = BifeaturePyramidNet(nets_3, ch)    \n",
    "    \n",
    "    cls_outputs = []\n",
    "    box_outputs = []\n",
    "    \n",
    "    kernel_init = tf.initializers.he_normal()\n",
    "    regulizer = tf.keras.regularizers.L2(l1)\n",
    "    \n",
    "    filters = 4 + 1 + num_classes + num_keypoints * (2 + num_keypoint_classes)\n",
    "    print('filters', filters)\n",
    "    \n",
    "    conv_inter = keras.layers.Conv2D(ch, 3, activation=activation, padding=\"same\", kernel_regularizer=regulizer, name='head_inter_0')\n",
    "    conv_h0 = keras.layers.Conv2D(anchor_k * filters, 1, padding=\"same\", name='head_0', kernel_initializer=kernel_init)   \n",
    "    conv_h1 = keras.layers.Conv2D(anchor_k * filters, 3, padding=\"same\", name='head_1', kernel_initializer=kernel_init)   \n",
    "    conv_h2 = keras.layers.Conv2D(anchor_k * filters, 3, padding=\"same\", name='head_2', kernel_initializer=kernel_init)\n",
    "    conv_h3 = keras.layers.Conv2D(anchor_k * filters, 3, padding=\"same\", name='head_3', kernel_initializer=kernel_init)\n",
    "    conv_kernels = [0, conv_h0, conv_h1, conv_h2, conv_h3]\n",
    "    conv_kernels = [0, conv_h0, conv_h0, conv_h0, conv_h0]\n",
    "    \n",
    "    drop = keras.layers.Dropout(0.001)\n",
    "    N = tf.shape(nets_3[0])[0]\n",
    "    \n",
    "    cbox_outputs = []    \n",
    "    \n",
    "    for i in range(1, len(p34567)):            \n",
    "        feature = drop(p34567[i])\n",
    "        #feature = add_map(feature)\n",
    "        conv_kernel = conv_kernels[i]\n",
    "        cls_out = conv_kernel(conv_inter(feature))        \n",
    "        cbox_out = tf.reshape(cls_out, [N, -1, filters])\n",
    "        cbox_outputs.append(cbox_out[:,:,:])\n",
    "    \n",
    "    outputs = tf.concat(cbox_outputs, axis=1)\n",
    "    img_h = tf.shape(inputs)[1]\n",
    "    img_w = tf.shape(inputs)[2]\n",
    "    heatmap = get_heatmap(p34567, img_h, img_w)    \n",
    "    \n",
    "    outputs = {'detect':outputs, 'heatmap':heatmap}        \n",
    "    return keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoxLoss(tf.losses.Loss):\n",
    "\n",
    "    def __init__(self, delta):\n",
    "        super(BoxLoss, self).__init__(reduction=\"none\", name=\"BoxLoss\")\n",
    "        self._delta = delta\n",
    "\n",
    "    def call(self, y_true, y_pred):    \n",
    "        \n",
    "        difference = y_true - y_pred\n",
    "        absolute_difference = tf.abs(difference)        \n",
    "        loss = tf.where(\n",
    "            tf.less(absolute_difference, self._delta),\n",
    "            0.5 * (difference ** 2),\n",
    "            absolute_difference - 0.5,\n",
    "        )\n",
    "        loss = tf.where(loss < 0.05, 0.0, loss)#new marginal loss        \n",
    "        return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "class ClassificationLoss(tf.losses.Loss):\n",
    "\n",
    "    def __init__(self, alpha, gamma, num_classes):\n",
    "        super(ClassificationLoss, self).__init__(reduction=\"none\", name=\"ClassificationLoss\")\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "        self._num_classes = num_classes\n",
    "        \n",
    "        \n",
    "    def call(self, y_cls, y_pred):\n",
    "        y_cls_int = tf.cast(y_cls, dtype=tf.int32)\n",
    "        y_hot = tf.one_hot(y_cls_int, depth=self._num_classes, dtype=tf.float32,)\n",
    "        \n",
    "        y_positive = tf.cast(y_cls > 0, tf.float32)#finetune, 1:unknown\n",
    "        y_positive_identity = tf.cast(y_cls > 1, tf.float32)# 1:unknown\n",
    "        \n",
    "        obj_score = tf.identity(y_pred[:, :], name='obj_score')\n",
    "        #cls_score = y_pred[:, :, 1:1+self._num_classes]\n",
    "        \n",
    "        pt = tf.clip_by_value(obj_score, 1e-7, 1.0 - 1e-7)\n",
    "                \n",
    "        loss_p = - (1.0 - self._alpha) * tf.pow(1.0 - pt, self._gamma) * y_positive * tf.math.log(pt)        \n",
    "        loss_f = - self._alpha * tf.pow(pt, self._gamma) * (1 - y_positive) * tf.math.log(1 - pt)\n",
    "        loss_obj = loss_p + loss_f\n",
    "          \n",
    "        if False:\n",
    "            cls_pt = tf.nn.softmax(cls_score)        \n",
    "            cls_pt = tf.clip_by_value(cls_pt, 1e-7, 1.0 - 1e-7)\n",
    "            loss_cls_p = - tf.pow(1.0 - cls_pt, self._gamma) * y_hot * tf.math.log(cls_pt)\n",
    "            loss_cls_f = - tf.pow(cls_pt, self._gamma) * (1 - y_hot) * tf.math.log(1 - cls_pt)\n",
    "            loss_cls = tf.reduce_sum(loss_cls_p + loss_cls_f, axis=-1)        \n",
    "            is_various_cls_exist = tf.cast(tf.math.reduce_std(y_cls) > 0, tf.float32)                \n",
    "        #loss_cls = is_various_cls_exist * y_positive * loss_cls\n",
    "                        \n",
    "        #normalizer = tf.reduce_sum(y_positive_identity, axis=-1)\n",
    "        #loss_cls = tf.math.divide_no_nan(tf.reduce_sum(loss_cls, axis=-1), normalizer)                        \n",
    "        \n",
    "        loss = loss_obj# + .1*loss_cls#when not stable                \n",
    "        return loss   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointLoss(tf.losses.Loss):\n",
    "    \"\"\"Implements Smooth L1 loss\"\"\"\n",
    "\n",
    "    def __init__(self, delta, gamma, num_classes):\n",
    "        super(KeypointLoss, self).__init__(reduction=\"none\", name=\"KeypointLoss\")\n",
    "        self._delta = delta\n",
    "        self._gamma = gamma\n",
    "        self._num_classes = num_classes\n",
    "        \n",
    "    def get_xy_loss(self, y_true, y_pred):        \n",
    "        difference = y_true - y_pred\n",
    "        absolute_difference = tf.abs(difference)        \n",
    "        loss = tf.where(\n",
    "            tf.less(absolute_difference, self._delta),\n",
    "            0.5 * (difference ** 2),\n",
    "            absolute_difference - 0.5,\n",
    "        )\n",
    "        #loss_xy = tf.where(loss < 0.05, 0.0, loss)#new marginal loss        \n",
    "        loss = tf.reduce_mean(loss, axis=-1)\n",
    "        return loss\n",
    "    \n",
    "    def get_cls_loss(self, y_cls, y_pred):        \n",
    "        loss = tf.square(y_cls - y_pred)\n",
    "        return loss    \n",
    "\n",
    "    def call(self, y_true, y_pred):    \n",
    "        #y: [m, anchors, NUM_KEYPOINT * (2 + classes)]        \n",
    "        gt_x, gt_y, gt_cls = split_gt_keypoint(y_true)        \n",
    "        h_x, h_y, h_cls, h_kp_cls = split_h_keypoint(y_pred)\n",
    "        \n",
    "        gt_xy = tf.stack((gt_x, gt_y), -1)\n",
    "        h_xy = tf.stack((h_x, h_y), -1) \n",
    "        m = tf.shape(y_true)[0]\n",
    "        #h_cls_score = tf.reshape(h_cls, [m, -1, NUM_KEYPOINT, NUM_KEYPOINT_CLASS])\n",
    "        \n",
    "        loss_cls = self.get_cls_loss(gt_cls, h_cls)#(m, anchor)\n",
    "        loss_xy = self.get_xy_loss(gt_xy, h_xy)#(m, anchor, NUM_KEYPOINT)        \n",
    "        \n",
    "        is_visible = tf.cast(gt_cls > 0, tf.float32)        \n",
    "        is_exist_kp_label = tf.cast(tf.reduce_any(gt_cls > 0, -1, True), tf.float32)\n",
    "        \n",
    "        kp_cls_sum_per_anchor = 1 + tf.reduce_sum(gt_cls, -1)\n",
    "        weight_kp = kp_cls_sum_per_anchor / tf.reduce_max(kp_cls_sum_per_anchor, -1, True)\n",
    "        \n",
    "        loss_xy = gt_cls * loss_xy \n",
    "        loss_cls = loss_cls * is_exist_kp_label\n",
    "        loss = 1 * tf.reduce_mean(loss_xy + loss_cls, axis=-1) #(m, anchor)         \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_KEYPOINT, NUM_KEYPOINT_CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetLoss(tf.losses.Loss):\n",
    "    \"\"\"Wrapper to combine both the losses\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=80, num_keypoint_classes=3, alpha=0.3, gamma=3.0, delta=1.0):#alpha=0.25\n",
    "        super(NetLoss, self).__init__(reduction=\"auto\", name=\"NetLoss\")\n",
    "        self._clf_loss = ClassificationLoss(alpha, gamma, num_classes)\n",
    "        self._box_loss = BoxLoss(delta)        \n",
    "        self._keypoint_loss = KeypointLoss(delta, gamma, num_keypoint_classes)                \n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        \n",
    "        y_box, y_cls, y_keypoint = split_gt(y_true)\n",
    "        h_box, h_obj, h_cls, h_keypoint = split_hyperthesis(y_pred)\n",
    "        gt_kp_x, gt_kp_y, gt_kp_cls = split_gt_keypoint(y_true)\n",
    "        \n",
    "        positive_mask = tf.greater(y_cls, 0.0)\n",
    "        ignore_mask = tf.less(y_cls, 0.0)\n",
    "        \n",
    "        clf_loss = self._clf_loss(y_cls, h_obj)\n",
    "        box_loss = self._box_loss(y_box, h_box) \n",
    "        keypoint_loss = self._keypoint_loss(y_keypoint, h_keypoint) \n",
    "        \n",
    "        kp_cls_sum_per_anchor = 1 + tf.reduce_sum(gt_kp_cls, -1)\n",
    "        weight_kp = kp_cls_sum_per_anchor / tf.reduce_max(kp_cls_sum_per_anchor, -1, True)\n",
    "        \n",
    "        clf_loss = tf.where(ignore_mask, 0.0, clf_loss)                \n",
    "        box_loss = tf.where(positive_mask, box_loss, 0.0)\n",
    "        keypoint_loss = tf.where(positive_mask, keypoint_loss, 0.0)\n",
    "        loss = clf_loss + weight_kp * (box_loss + 0.1 * keypoint_loss)\n",
    "        \n",
    "        positive_mask = tf.cast(positive_mask, tf.float32)        \n",
    "        normalizer = tf.reduce_sum(positive_mask, axis=-1)\n",
    "        normalizer = tf.sqrt(normalizer)\n",
    "        loss = tf.math.divide_no_nan(tf.reduce_sum(loss, axis=-1), normalizer)        \n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(y_cls, score, class_k, gamma):\n",
    "    y_cls_int = tf.cast(y_cls, dtype=tf.int32)\n",
    "    y_hot = tf.one_hot(y_cls_int, depth=class_k, dtype=tf.float32)\n",
    "        \n",
    "    cls_pt = tf.nn.softmax(score)        \n",
    "    cls_pt = tf.clip_by_value(cls_pt, 1e-7, 1.0 - 1e-7)\n",
    "    loss_cls_p = - tf.pow(1.0 - cls_pt, gamma) * y_hot * tf.math.log(cls_pt)\n",
    "    loss_cls_f = - tf.pow(cls_pt, gamma) * (1 - y_hot) * tf.math.log(1 - cls_pt)\n",
    "    loss_cls = tf.reduce_sum(loss_cls_p + loss_cls_f, axis=-1) \n",
    "    return loss_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeatmapLoss(tf.losses.Loss):\n",
    "    \"\"\"Wrapper to combine both the losses\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, alpha=0.3, gamma=3.0, delta=1.0):#alpha=0.25\n",
    "        super(HeatmapLoss, self).__init__(reduction=\"auto\", name=\"HeatmapLoss\")\n",
    "        self._delta = delta\n",
    "        self._gamma = gamma\n",
    "        self._num_classes = num_classes\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        #(m, h, w, m_classes)\n",
    "        y_cls_int = tf.cast(y_true, dtype=tf.int32)                        \n",
    "        #[1,32,32,18] vs. [1,32,32,7,18] mul\n",
    "        loss = focal_loss(y_true, y_pred, self._num_classes, self._gamma)        \n",
    "        #loss = loss * tf.cast(y_true > 0, tf.float32)\n",
    "        loss = tf.boolean_mask(loss, y_true > 0)\n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_positive = tf.cast(y_cls > 0, tf.int32)\n",
    "    y_bg = tf.cast(tf.abs(y_cls)==0, tf.int32)\n",
    "    h_score = y_pred[:, :, 4]\n",
    "    h_prob = tf.nn.sigmoid(h_score)    \n",
    "    h_postive = tf.cast(tf.round(h_prob), tf.int32)\n",
    "    \n",
    "    true_positives = tf.cast(tf.logical_and(y_cls > 0, h_postive>0), tf.float32)\n",
    "    false_negative = y_positive * (1 - h_postive)\n",
    "                \n",
    "    tp = tf.reduce_sum(true_positives, axis=1)# + 0.01\n",
    "    fn = tf.reduce_sum(false_negative, axis=1)\n",
    "    tp = tf.cast(tp, tf.float32)\n",
    "    fn = tf.cast(fn, tf.float32)\n",
    "    \n",
    "    rec = tp / (tp + fn + 1e-8)\n",
    "    return rec\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \n",
    "    y_cls_symbol = tf.cast(y_true[:, :, 4], dtype=tf.int32)    \n",
    "    y_cls_symbol = tf.cast(y_cls_symbol != 0, tf.int32)\n",
    "    h_obj_prob = tf.nn.sigmoid(y_pred[:, :, 4])\n",
    "    h_cls_symbol = tf.round(h_obj_prob)    \n",
    "    h_cls_symbol = tf.cast(h_cls_symbol, tf.int32)\n",
    "    \n",
    "    true_positives = y_cls_symbol * h_cls_symbol\n",
    "    false_positive = (1 - y_cls_symbol) * h_cls_symbol\n",
    "    \n",
    "    ones = tf.ones_like(true_positives)\n",
    "    zeeros = tf.zeros_like(true_positives)\n",
    "    true_positives = tf.cast(tf.equal(true_positives, ones), tf.float32)\n",
    "    false_positive = tf.cast(tf.equal(false_positive, ones), tf.float32)\n",
    "    \n",
    "    tp = tf.reduce_sum(true_positives, axis=1)# + 0.01\n",
    "    fp = tf.reduce_sum(false_positive, axis=1)\n",
    "    tp = tf.cast(tp, tf.float32)\n",
    "    fp = tf.cast(fp, tf.float32)\n",
    "    prec = tp / (tp + fp + 1e-8)\n",
    "    return prec\n",
    "\n",
    "def acc(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_positive = y_cls > 0    \n",
    "    h_cls = tf.math.argmax(y_pred[:, :, 5:5+num_classes], -1, output_type=tf.int32)        \n",
    "    acc = tf.boolean_mask(tf.equal(y_cls, h_cls), y_positive)    \n",
    "    #acc = tf.equal(y_cls, h_cls)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kp_L1(y_true, y_pred):#keypoint\n",
    "        \n",
    "    y_box, y_cls, y_keypoint = split_gt(y_true)\n",
    "    h_box, h_obj, h_cls, h_keypoint = split_hyperthesis(y_pred)   \n",
    "    \n",
    "    gt_x, gt_y, gt_cls = split_gt_keypoint(y_keypoint)\n",
    "    h_x, h_y, h_cls, h_kp_cls = split_h_keypoint(h_keypoint)\n",
    "\n",
    "    gt_xy = tf.stack((gt_x, gt_y), -1)\n",
    "    h_xy = tf.stack((h_x, h_y), -1)     \n",
    "    \n",
    "    l1_distance = tf.reduce_mean(tf.abs(gt_xy - h_xy), [-1])#(38, 34)\n",
    "    cond = tf.logical_and(tf.expand_dims(y_cls, -1) > 0, gt_cls > 0)\n",
    "    l1_distance = tf.boolean_mask(l1_distance, cond)\n",
    "    return l1_distance   \n",
    "\n",
    "def kp_cls_acc(y_true, y_pred):#keypoint\n",
    "    y_box, y_cls, y_keypoint = split_gt(y_true)    \n",
    "    h_box, h_obj, h_cls, h_keypoint = split_hyperthesis(y_pred)\n",
    "    \n",
    "    gt_x, gt_y, gt_cls = split_gt_keypoint(y_keypoint)\n",
    "    h_x, h_y, h_kp_cls_score, h_kp_cls = split_h_keypoint(h_keypoint)\n",
    "\n",
    "    gt_xy = tf.concat((gt_x, gt_y), -1)\n",
    "    h_xy = tf.concat((h_x, h_y), -1) \n",
    "    m = tf.shape(y_true)[0]\n",
    "        \n",
    "    #[1,14160,17] vs. [1,12240,17]    \n",
    "    gt_cls = tf.cast(gt_cls, tf.int32)\n",
    "    h_kp_cls = tf.cast(h_kp_cls, tf.int32)\n",
    "    acc = tf.equal(gt_cls, h_kp_cls)\n",
    "    #is_exist_kp_label = tf.cast(tf.reduce_any(gt_cls > 0, -1, True), tf.float32)\n",
    "    cond = tf.logical_and(y_cls > 0, tf.reduce_any(gt_cls > 0, -1))\n",
    "    acc = tf.boolean_mask(acc, cond)\n",
    "    return acc   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_acc(y_true, y_pred):#keypoint\n",
    "    \n",
    "    h_cls = tf.argmax(y_pred, -1)    \n",
    "    h_cls = tf.cast(h_cls, tf.int32)\n",
    "    y_cls = tf.cast(y_true, tf.int32)\n",
    "    \n",
    "    acc = tf.equal(y_cls, h_cls) \n",
    "    acc = tf.boolean_mask(acc, y_true > 0)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, linewidth=200)\n",
    "image_height, image_width = padded_image_shape\n",
    "\n",
    "img_check = 0\n",
    "for image, multi_y in val_dataset:\n",
    "    output_map = multi_y['detect']\n",
    "    heatmap = multi_y['heatmap']\n",
    "    print('output_map', output_map.shape)\n",
    "    cbbox = output_map    \n",
    "    bbox = cbbox[:, :, :4]\n",
    "    cls_gt = cbbox[:,:,4]\n",
    "    img_m, image_height, image_width, image_ch = image.shape\n",
    "    anchor_feature_size = [(np.ceil(image_height / 2 ** i), np.ceil(image_width / 2 ** i)) \n",
    "                           for i in range(level_start, level_end)]\n",
    "    print('anchor_feature_size', anchor_feature_size)    \n",
    "    m = len(cbbox)    \n",
    "    positive_count = np.sum(cls_gt>0)\n",
    "    print('cbbox', cbbox.shape)\n",
    "    print('cls_sum',np.sum(cls_gt < 0.0), np.sum(cls_gt == 0.0), \n",
    "          np.sum(cls_gt == 1.0), np.sum(cls_gt > 1.0))\n",
    "    print('cls_mean',np.mean(cls_gt < 0.0), np.mean(cls_gt == 0.0), \n",
    "          np.mean(cls_gt == 1.0), np.mean(cls_gt > 0.0))\n",
    "    print('shape',image.shape, cbbox.shape,'unique', np.unique(cls_gt))\n",
    "    print('anchor_feature_size', anchor_feature_size)\n",
    "    offset = 0\n",
    "    positive_maps = []\n",
    "    for anchor_feature_size_1 in anchor_feature_size:        \n",
    "        fm_h, fm_w = anchor_feature_size_1\n",
    "        fm_h = int(fm_h)\n",
    "        fm_w = int(fm_w)        \n",
    "        fm_wh = int(fm_h * fm_w * anchor_k)\n",
    "        cbbox_anchor = cbbox[:, offset:offset+fm_wh, 4]\n",
    "        cbbox_anchor = np.reshape(cbbox_anchor, [m, fm_h, fm_w, anchor_k])\n",
    "        coount_m1 = np.count_nonzero(cbbox_anchor==-1)\n",
    "        coount_0 = np.count_nonzero(cbbox_anchor==0)\n",
    "        coount_1 = np.count_nonzero(cbbox_anchor==1)\n",
    "        coount_1_over = np.count_nonzero(cbbox_anchor>1)\n",
    "        positive_ratio = np.mean(cbbox_anchor>0)\n",
    "        positive_maps.append(cbbox_anchor>0)\n",
    "        print('cbbox_anchor', cbbox_anchor.shape, coount_m1, coount_0, coount_1, coount_1_over, 'ratio', positive_ratio)\n",
    "        sample_0_cbbox = cbbox_anchor[0]\n",
    "        sample_0_cbbox_sum = np.max(sample_0_cbbox, -1).astype(np.int)       \n",
    "      \n",
    "        offset += fm_wh\n",
    "        if False:            \n",
    "            file_name = str(fm_h)+ '_' + str(fm_w)+ '.txt'\n",
    "            np.savetxt(file_name,sample_0_cbbox_sum, fmt='%d',delimiter='')\n",
    "    img_check = image\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmap0 = np.array(Image.fromarray(np.max(positive_maps[0][0],-1)).resize((image_width, image_height)))\n",
    "pmap1 = np.array(Image.fromarray(np.max(positive_maps[1][0],-1)).resize((image_width, image_height)))\n",
    "pmap2 = np.array(Image.fromarray(np.max(positive_maps[2][0],-1)).resize((image_width, image_height)))\n",
    "#pmap3 = np.array(Image.fromarray(np.max(positive_maps[3][0],-1)).resize((image_width, image_height)))\n",
    "#pmap4 = np.array(Image.fromarray(np.max(positive_maps[4][0],-1)).resize((image_width, image_height)))\n",
    "pmap0 = pmap0.astype(np.uint8)\n",
    "pmap1 = pmap1.astype(np.uint8)\n",
    "pmap2 = pmap2.astype(np.uint8)\n",
    "pmap3 = 0#pmap3.astype(np.uint8)\n",
    "pmap4 = 0#pmap4.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pmap_with_img = np.array(img_check)[0]#*255\n",
    "pmap_with_img = pmap_with_img.astype(np.uint8)\n",
    "pmap_add = np.expand_dims(pmap0+pmap1+pmap2+pmap3+pmap4, -1)\n",
    "pmap = pmap_add*(255//np.max(pmap_add))\n",
    "mix_rgb = np.concatenate((pmap, pmap_with_img[:,:,1:]),-1)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(mix_rgb)\n",
    "plt.title(str(np.mean(pmap_add)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_image(img0, img1):\n",
    "    a = np.array(img0)\n",
    "    b = np.array(img1)\n",
    "    return Image.fromarray(np.concatenate((a, b), axis=1))\n",
    "\n",
    "def stack_image_3(img0, img1, img2):\n",
    "    a = np.array(img0)\n",
    "    b = np.array(img1)\n",
    "    c = np.array(img2)\n",
    "    return Image.fromarray(np.concatenate((a, b, c), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_gt_h(check_dataset, stride=1):\n",
    "    i = 0\n",
    "    for image, target_dict in check_dataset:        \n",
    "        i += 1\n",
    "        if stride > 1 and i%stride!=0:\n",
    "            continue\n",
    "\n",
    "        x = np.array(image[0], np.uint8)\n",
    "        y_detect = target_dict['detect']\n",
    "        y_heatmap = target_dict['heatmap']\n",
    "        detections_gt = decode_debug(image, y_detect, confidence_threshold=0.15, nms_iou_threshold=0.2)    \n",
    "        model_out = inference_model.predict(image)  \n",
    "        detections = model_out['detect']\n",
    "        heatmap = model_out['heatmap']\n",
    "        print('x', x.shape, 'detections', detections.shape)\n",
    "        #print(detections)\n",
    "        h = np.array(detections)  \n",
    "        gt = np.array(detections_gt)\n",
    "        print(i, 'gt', gt.shape, 'h', h.shape)\n",
    "        \n",
    "        heatmap_scale = (255//(1+NUM_KEYPOINT))\n",
    "        heatmap_arr = np.array(heatmap[0]).astype(np.uint8) * heatmap_scale\n",
    "        heatmap_arr = np.stack((heatmap_arr, 255-heatmap_arr, 255-heatmap_arr), -1)\n",
    "        img_heatmap = Image.fromarray(heatmap_arr)\n",
    "        \n",
    "        img = Image.fromarray(x)\n",
    "        img_copy = x.copy()\n",
    "        img_copy[:, :, 0] = tf.cast(np.array(y_heatmap[0]*heatmap_scale), tf.uint8)\n",
    "        \n",
    "        img_gt = Image.fromarray(img_copy)\n",
    "        draw_box_keypoint(img, h, is_show_class=False)\n",
    "        draw_box_keypoint(img_gt, gt, is_show_class=False)\n",
    "        draw_box_keypoint(img_heatmap, gt, is_show_class=False)        \n",
    "        \n",
    "        print('heatmap_arr', heatmap_arr.shape)\n",
    "        display(stack_image_3(img, img_gt, img_heatmap))    \n",
    "        #display(stack_image(img, img_gt))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weight():           \n",
    "    print('latest_checkpoint', path_weight)\n",
    "    model.load_weights(path_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes, NUM_KEYPOINT_CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filters\n",
    "4+1+num_classes+NUM_KEYPOINT*(2+NUM_KEYPOINT_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    optimizer = tf.optimizers.SGD(learning_rate=1e-2, momentum=0.1)\n",
    "    loss_detect = NetLoss(num_classes, NUM_KEYPOINT_CLASS)\n",
    "    loss_heatmap = HeatmapLoss(NUM_KEYPOINT + 1)\n",
    "    model = createNet(num_classes, NUM_KEYPOINT, 1, anchor_k, is_backbone_train=True)\n",
    "    metric_det = [recall, precision, kp_L1, kp_cls_acc]\n",
    "\n",
    "    losses = {\"detect\": loss_detect, 'heatmap': loss_heatmap}\n",
    "    metrics = {\"detect\": metric_det, 'heatmap': heatmap_acc} \n",
    "    loss_weights = {\"detect\": 1, 'heatmap': 50} \n",
    "\n",
    "    model.compile(loss=losses, optimizer=optimizer, metrics=metrics, loss_weights=loss_weights)\n",
    "    callbacks_list = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=path_weight,\n",
    "            monitor=\"loss\",\n",
    "            save_best_only=False,\n",
    "            save_weights_only=True,\n",
    "            verbose=0,\n",
    "            save_freq=200\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()#b1:13,906,006, b2:15,431,112, b3+:25,731,444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(len(list_x_train), len(list_x_test), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    load_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "out = model.evaluate(val_dataset.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 100000\n",
    "hist = model.fit(train_dataset, epochs=epochs, callbacks=callbacks_list, verbose=1)\n",
    "'''\n",
    "2s 177ms/step - loss: 0.5565 - tf_op_layer_concat_283_loss: 0.4345 - tf_op_layer_ResizeBilinear_16_loss: 0.0122 - tf_op_layer_concat_283_recall: 1.0000 - tf_op_layer_concat_283_precision: 1.0000 - tf_op_layer_concat_283_kp_L1: 0.7677 - tf_op_layer_concat_283_kp_cls_acc: 0.5468 - tf_op_layer_ResizeBilinear_16_hitmap_acc: 0.9748\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(path_weight)\n",
    "path_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image = tf.keras.Input(shape=[None, None, 3], name=\"image\")\n",
    "predictions = model(image, training=False)\n",
    "h_detect = predictions['detect']\n",
    "h_heatmap_score = predictions['heatmap']\n",
    "h_heatmap = tf.argmax(h_heatmap_score, -1)\n",
    "detections = decodePredictions(image, h_detect, confidence_threshold=0.5, nms_iou_threshold=0.15)\n",
    "model_out = {'detect':detections, 'heatmap':h_heatmap}\n",
    "inference_model = tf.keras.Model(inputs=image, outputs=model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compare_gt_h(val_dataset, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_gt_h(val_dataset, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
