{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keypoint 는 [17, 3] 의 2차원 배열\n",
    "- 17는 키포인트의 개수이고 3은 [x, y, visibility] 이다\n",
    "- visibility {0:안보여서 알수없음, 1:가려짐, 2:보임)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_KEYPOINT = 17 # 키포인트 개수\n",
    "NUM_KEYPOINT_CH = 3 # x, y, visibility\n",
    "NUM_KEYPOINT_CLASS = 3\n",
    "thresh_keypoint_count = 10 # 데이터 선별시 최소 개수\n",
    "VISIBILITY_NOT_EXIST = 0\n",
    "VISIBILITY_OCCLUDED = 1\n",
    "VISIBILITY_VISIBLE = 2\n",
    "MASK_NEGATIVE = 0\n",
    "MASK_IGNORE = 1\n",
    "MASK_POSITIVE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_data_m = 100\n",
    "channel_label = 4 + 1 + NUM_KEYPOINT * NUM_KEYPOINT_CH #(box:4, cls:1, keypoint:17*3)\n",
    "channel_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_COCO = '/media/mvlab/469B5B3C650FBA77/data/COCO/'\n",
    "folder_COCO_annotation = folder_COCO + 'annotations_trainval2017/annotations/'\n",
    "folder_img = 'val2017'\n",
    "#folder_img = 'train2017'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_annotation_keypoints_val2017 = folder_COCO_annotation + 'person_keypoints_'+folder_img + '.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.isdir(folder_COCO), os.path.isdir(folder_COCO_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.isfile(path_annotation_keypoints_val2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_annotation_keypoints_val2017) as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "    \n",
    "len(json_data), json_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_keypoints = json_data['categories'][0]['keypoints']\n",
    "images = json_data['images']\n",
    "annotations = json_data['annotations']\n",
    "\n",
    "len(class_keypoints), len(images), len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AIHub 사람 동작 영상 AI 데이터는 키포인트 16개\n",
    "- 추가 4 head, neck, chest, hip\n",
    "- 제거 5 nose, left_eye, right_eye, left_ear, right_ear\n",
    "\n",
    "##### COCO VS AIHub\n",
    "- COCO 얼굴 중요(눈, 코, 귀)\n",
    "- AIHub 동작 중요(목, 가슴, 골반 중심)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations[0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations[0]['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_id_image_info = dict()\n",
    "image_w = []\n",
    "image_h = []\n",
    "for image_info in images:\n",
    "    #print('image_info', image_info)    \n",
    "    \n",
    "    image_w.append(image_info['width'])\n",
    "    image_h.append(image_info['height'])\n",
    "    \n",
    "    image_id = image_info['id']    \n",
    "    dict_id_image_info[image_id] = image_info\n",
    "        \n",
    "len(dict_id_image_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(image_w), np.min(image_h), np.max(image_w), np.max(image_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_w_max = 640\n",
    "image_h_max = 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(image_w) < 200).mean(), (np.array(image_h) < 200).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('w, h')\n",
    "plt.scatter(image_w, image_h, s=2)\n",
    "plt.xlabel('width')\n",
    "plt.ylabel('height')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get image_filename from image_id\n",
    "list_id = []\n",
    "list_category_id = []\n",
    "for annotation in annotations:\n",
    "    \n",
    "    image_id = annotation['image_id']\n",
    "    category_id = annotation['category_id']    \n",
    "    \n",
    "    image_info = dict_id_image_info[image_id]\n",
    "    if len(list_id) == 0:\n",
    "        print('category_id', category_id)\n",
    "        print('annotation', annotation)\n",
    "        print('image_info', image_info)\n",
    "        \n",
    "    file_name = image_info['file_name']\n",
    "    height = image_info['height']\n",
    "    width = image_info['width']\n",
    "    \n",
    "    file_full_path = folder_COCO + folder_img + os.sep + file_name\n",
    "    \n",
    "    if not os.path.isfile(file_full_path):\n",
    "        print('not exist', file_full_path)\n",
    "        continue\n",
    "        \n",
    "    annotation['file_name'] = file_full_path\n",
    "    annotation['height'] = height\n",
    "    annotation['width'] = width\n",
    "    \n",
    "    #print(annotation)\n",
    "    list_id.append(image_id)\n",
    "    list_category_id.append(category_id)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전부 사람(1) 카테고리\n",
    "np.unique(list_category_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#generate dictionary(key:filename, value:annotation list)\n",
    "#박스와 클래스, 키포인트를 통합\n",
    "dict_annotation_coco = dict()\n",
    "for annotation in annotations:\n",
    "    #print(annotation)\n",
    "    segmentation = annotation['segmentation']\n",
    "    bbox = annotation['bbox']\n",
    "    category_id = annotation['category_id']\n",
    "    keypoints = annotation['keypoints']    \n",
    "    file_name = annotation['file_name']\n",
    "    \n",
    "    #x0, y0, x0 + w, y0 + h\n",
    "    x0, y0, w, h = bbox\n",
    "    b = np.array([x0, y0, x0 + w, y0 + h]).flatten()\n",
    "    c = np.array(category_id).flatten()\n",
    "    k = np.array(keypoints).flatten()\n",
    "    #print(len(b), 'box', bbox, len(c), len(k))\n",
    "    cbox = np.concatenate((b, c, k)).reshape((1, -1))\n",
    "    \n",
    "    if len(dict_annotation_coco) == 0:        \n",
    "        print('file_name', file_name)\n",
    "        print('bbox', bbox)\n",
    "        print('segmentation', len(segmentation), len(segmentation[0]))\n",
    "        print('keypoints', np.array(keypoints).reshape((-1, 3)))    \n",
    "        print('cbox', len(cbox), cbox)\n",
    "    \n",
    "    if file_name in dict_annotation_coco.keys():\n",
    "        pre_annotation = dict_annotation_coco[file_name]\n",
    "        new_bbox = np.concatenate((pre_annotation, cbox), axis=0)        \n",
    "        dict_annotation_coco[file_name] = new_bbox\n",
    "    else:\n",
    "        dict_annotation_coco[file_name] = cbox\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_annotation_coco), list(dict_annotation_coco.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_id), len(set(list_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_keypoint_label = np.concatenate(list(dict_annotation_coco.values()), 0)\n",
    "box_keypoint_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x0 = box_keypoint_label[:, 0]\n",
    "y0 = box_keypoint_label[:, 1]\n",
    "x1 = box_keypoint_label[:, 2]\n",
    "y1 = box_keypoint_label[:, 3]\n",
    "box_w = x1-x0\n",
    "box_h = y1-y0\n",
    "plt.title('h / w = ' +  str(np.mean(box_h/box_w)))\n",
    "ax = plt.hist(box_w )\n",
    "ax = plt.hist(box_h, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 선별\n",
    "- 한사람의 키포인트가 일정 개수 이상 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_annotation_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_annotation_coco_select = dict()\n",
    "\n",
    "for key in dict_annotation_coco:\n",
    "    value = dict_annotation_coco[key]\n",
    "    #print('key', key)\n",
    "    #print('value', type(value), value.shape, value)\n",
    "    \n",
    "    box = value[:, :4]\n",
    "    cls = value[:, 4]\n",
    "    keypoint = value[:, 5:]\n",
    "    keypoint = keypoint.reshape((-1, NUM_KEYPOINT, NUM_KEYPOINT_CH))\n",
    "    visibility = keypoint[:, :, 2]\n",
    "    visibility_count_per_person = np.sum(visibility > VISIBILITY_NOT_EXIST, -1)\n",
    "    visibility_max_count_per_person = np.max(visibility_count_per_person)\n",
    "        \n",
    "    if visibility_max_count_per_person >= thresh_keypoint_count:                \n",
    "        dict_annotation_coco_select[key] = value\n",
    "\n",
    "len(dict_annotation_coco), len(dict_annotation_coco_select)\n",
    "#train2017:(64115, 45900), val2017:(2693, 2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_annotation_coco = dict_annotation_coco_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_cbox(image, cbox):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    x0, y0, x1, y1, cls = cbox\n",
    "    draw.text((x0+2, y0), str(int(cls)))\n",
    "    draw.rectangle((x0, y0, x1, y1), fill=None, outline=(0,255,0), width=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoint(image, keypoints, is_show_class=True):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    #print('draw_keypoint', keypoints.shape, keypoints[:,2])\n",
    "    #[0. 0. 0. 0. 0. 1. 2. 2. 2. 2. 2. 2. 2. 0. 2. 0. 2.]\n",
    "    for i in range(len(keypoints)):\n",
    "        point = keypoints[i]\n",
    "        cls_name = str(i)+':'+ class_keypoints[i] if is_show_class else ''                \n",
    "        cx = point[0]\n",
    "        cy = point[1]\n",
    "        state = point[2]\n",
    "        circle_radius = 2\n",
    "        if state > 1:\n",
    "            draw.ellipse((cx-circle_radius, cy-circle_radius, cx+circle_radius, cy+circle_radius),fill=None, width=1)\n",
    "            draw.text((cx, cy-10), cls_name)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_valid_line(draw, p0, p1, color):\n",
    "    #point : [x, y, state]\n",
    "    state0 = p0[2]\n",
    "    state1 = p1[2]\n",
    "    if state0 * state1 > 0:\n",
    "        draw.line((tuple(p0[:2]), tuple(p1[:2])), fill=color)\n",
    "    \n",
    "\n",
    "def draw_keypoint_line(image, keypoints):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "        \n",
    "    nose = keypoints[0]\n",
    "    left_eye = keypoints[1]\n",
    "    right_eye = keypoints[2]\n",
    "    left_ear = keypoints[3]\n",
    "    right_ear = keypoints[4]            \n",
    "    left_shoulder = keypoints[5]\n",
    "    right_shoulder = keypoints[6]\n",
    "    left_elbow = keypoints[7]\n",
    "    right_elbow = keypoints[8]\n",
    "    left_wrist = keypoints[9]\n",
    "    right_wrist = keypoints[10]\n",
    "    left_hip = keypoints[11]\n",
    "    right_hip = keypoints[12]\n",
    "    left_knee = keypoints[13]\n",
    "    right_knee = keypoints[14]\n",
    "    left_ankle = keypoints[15]\n",
    "    right_ankle = keypoints[16]        \n",
    "    \n",
    "    color_center = (255, 255, 255)\n",
    "    color_left = (255,255,0)\n",
    "    color_right = (0,255,255)\n",
    "    \n",
    "    draw_valid_line(draw, left_shoulder, right_shoulder, color_center)\n",
    "    draw_valid_line(draw, left_hip, right_hip, color_center)\n",
    "    \n",
    "    draw_valid_line(draw, nose, left_eye, color_left)\n",
    "    draw_valid_line(draw, left_eye, left_ear, color_left)\n",
    "    \n",
    "    draw_valid_line(draw, nose, right_eye, color_right)\n",
    "    draw_valid_line(draw, right_eye, right_ear, color_right)\n",
    "        \n",
    "    draw_valid_line(draw, left_shoulder, left_ear, color_left)    \n",
    "    draw_valid_line(draw, left_shoulder, left_elbow, color_left)\n",
    "    draw_valid_line(draw, left_shoulder, left_hip, color_left)\n",
    "    draw_valid_line(draw, left_elbow, left_wrist, color_left)\n",
    "    draw_valid_line(draw, left_hip, left_knee, color_left)\n",
    "    draw_valid_line(draw, left_knee, left_ankle, color_left)\n",
    "    \n",
    "    draw_valid_line(draw, right_shoulder, right_ear, color_right)\n",
    "    draw_valid_line(draw, right_shoulder, right_elbow, color_right)\n",
    "    draw_valid_line(draw, right_shoulder, right_hip, color_right)\n",
    "    draw_valid_line(draw, right_elbow, right_wrist, color_right)    \n",
    "    draw_valid_line(draw, right_hip, right_knee, color_right)\n",
    "    draw_valid_line(draw, right_knee, right_ankle, color_right)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_box_keypoint(image, keypoint_box_label, is_show_class=False):\n",
    "    #keypoint_box_label : (m, 17*3+5)\n",
    "    box_label = keypoint_box_label[:, :5]\n",
    "    keypoint_label = keypoint_box_label[:,5:].reshape((-1, NUM_KEYPOINT, NUM_KEYPOINT_CH))\n",
    "    #box_label : (m, 5)\n",
    "    #keypoint_label : (m, 17, 3)\n",
    "    \n",
    "    if np.mean(box_label[:, :4]) < 2:\n",
    "        print('unnormalize')\n",
    "        img_w, img_h = image.width, image.height        \n",
    "        box_label[:, :4] *= np.array((img_w, img_h, img_w, img_h)).reshape((1, 4))\n",
    "        keypoint_label[:, :, :2] *= np.array((img_w, img_h)).reshape((1, 1, 2))\n",
    "    \n",
    "    for keypoints in keypoint_label:\n",
    "        draw_keypoint(image, keypoints, is_show_class=is_show_class)\n",
    "        draw_keypoint_line(image, keypoints)\n",
    "\n",
    "    for cbox in box_label:\n",
    "        draw_cbox(image, cbox)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_i = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_image_path = list(dict_annotation_coco.keys())[sample_i]\n",
    "keypoint_box_label_sample = dict_annotation_coco[sample_image_path]\n",
    "\n",
    "box_label_sample = keypoint_box_label_sample[:, :5]\n",
    "keypoint_label_sample = keypoint_box_label_sample[:,5:].reshape((-1, NUM_KEYPOINT, NUM_KEYPOINT_CH))\n",
    "sample_image_path, len(keypoint_label_sample), keypoint_label_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(sample_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(keypoint_label_sample[0].astype(np.int), class_keypoints, ['x','y','visibility'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_label_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = Image.open(sample_image_path)\n",
    "print('sample_image.size', sample_image.size)\n",
    "draw_box_keypoint(sample_image, keypoint_box_label_sample, is_show_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 데이터 구성\n",
    "- X : 이미지 (h,w,3)\n",
    "- Y : 박스 + 클래스 + 키포인트 (box:4, class:1, keypoint:17*3(x,y,visibility)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keypoint 는 영상에서의 좌표가 아닌 박스에서의 상대좌표? NO, 박스가 부정확하면 keypoint 까지 망가진다\n",
    "- 박스 검출 후 그 내부에서 keypoint 의 상대좌표를 regression 하면 안됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keypoint 를 검출해서 그것으로부터 박스를 그리는 것이 안전\n",
    "- keypoint 의 visibility 까지 학습시켜서 keypoint를 그리는데 사용하자\n",
    "- 그려진 keypoint를 이용해서 box 를 그리자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- box & keypoint = positive\n",
    "- no box * no keypoint = background\n",
    "- box & no keypoint = ignore (New!)\n",
    "- no box & keypoint = not exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. keypoint는 groundtruth 로 클래스별 hitmap으로 변환해서 traget 이 되어 학습된다.\n",
    "    - hitmap gt 는 좌표맵과 키포인트 좌표의 차이를 이용해 생성한다\n",
    "      - 차이가 가장 작은 위치는 positive + (VISIBLE, OCCLUDED)\n",
    "      - 차이가 일정 범위 이하는 ignore\n",
    "      - 차이가 일정 범위 이상은 negative + (NOT_EXIST)\n",
    "    - hitmap_visibility_gt 는 hitmap_coord_gt 와 별도로 생성한다\n",
    "      - hitmap의 할당값은 VISIBLE(2), OCCLUDED(1), NOT_EXIST(0) 로 맵핑\n",
    "      - hitmap_gt는 class별로 존재하나 visibility 는 class와 독립적인 변수이다.\n",
    "      - hitmap_gt는 keypoint를 찾는 용도이며 visibility는 hitmap_gt로 찾은 keypoint의 상태이다.\n",
    "    - hitmap gt 는 multi-scale 일 필요가 있다\n",
    "    - hitmap gt 는 anchor별로 생성될 필요가 있다. 그것이 구현이 더 편리하다\n",
    "    - 모델은 클래스별 hitmap을 출력한다\n",
    "    - hitmap loss 적용\n",
    "1. 모델은 검출기와 같은 anchor를 사용한다\n",
    "   - 모델은 objectness와 박스 regression을 출력한다\n",
    "1. positive objectness를 갖는 anchor 는 모델이 예측한 box 변환 값을 이용해 오브젝트 후보의 경계 박스(ROI)를 생성한다\n",
    "   1. ROI 내부에 존재하는 최대 확률의 keypoint (ROI에 대한 상대)좌표를 구한다\n",
    "      - keypoint 간의 연결성에 대해서는 무시하는 꼴이다.\n",
    "      - 따라서 주위에서 침입한 keypoint 가 있는 경우 문제가 발생한다. 이 문제는 hierarchical keypoint 나 affinity vector를 이용하면 해결 가능하다.\n",
    "      - 일단은 연결성은 무시해서 구현해보자\n",
    "   1. ROI 정보를 이용해 상대좌표를 이미지에 대한 상대 좌표로 변환한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xy(annotation, stride=1):\n",
    "    input_list = []\n",
    "    bbox_list = []\n",
    "    path_list = []\n",
    "    i = 0\n",
    "    \n",
    "    for path_image in annotation:\n",
    "        i+=1\n",
    "        if stride!=1 and np.random.randint(1, 1+stride)%stride!=0:\n",
    "            continue\n",
    "            \n",
    "        cls_bbox = annotation[path_image]                \n",
    "        bbox = np.array(cls_bbox[:, :4])\n",
    "        cls = cls_bbox[:, 4:5]\n",
    "        keypoint = cls_bbox[:, 5:].astype(np.float)\n",
    "        keypoint_3d = np.reshape(keypoint, (-1, NUM_KEYPOINT, NUM_KEYPOINT_CH))\n",
    "        \n",
    "        img = Image.open(path_image)    \n",
    "        scale = np.array((img.width, img.height, img.width, img.height))\n",
    "        scale = np.reshape(scale, (1, 4)).astype(np.float)\n",
    "        scale_keypoint = np.array((img.width, img.height, 1)).reshape((1,1,3))\n",
    "\n",
    "        img_arr = np.array(img)\n",
    "        if img_arr.ndim < 3: \n",
    "            print('gray image skip')\n",
    "            continue #gray image skip\n",
    "        \n",
    "        try:\n",
    "            std_v = np.std(img_arr)\n",
    "            if std_v < 3:\n",
    "                print('std_v', std_v)\n",
    "                continue\n",
    "        except:\n",
    "            print('error', path_image)\n",
    "            continue\n",
    "        \n",
    "        box_width = bbox[:, 2] - bbox[:, 0]\n",
    "        box_height = bbox[:, 3] - bbox[:, 1]\n",
    "        \n",
    "        if np.min(box_width) < 1 or np.min(box_height) < 1:\n",
    "            print('box_size < 1', box_width, box_height)#check\n",
    "            continue\n",
    "            \n",
    "        bbox_norm = bbox.astype(np.float) / scale        \n",
    "        keypoint_norm = keypoint_3d / scale_keypoint        \n",
    "        keypoint_norm_2d = np.reshape(keypoint_norm, (-1, NUM_KEYPOINT * NUM_KEYPOINT_CH))\n",
    "        \n",
    "        cls_bbox_norm = np.concatenate((bbox_norm, cls, keypoint_norm_2d), axis=1)\n",
    "\n",
    "        input_list.append(img_arr)\n",
    "        bbox_list.append(cls_bbox_norm)\n",
    "        path_list.append(path_image)\n",
    "        if len(input_list)%100==0:        \n",
    "            print(len(annotation), i, len(input_list))   \n",
    "        if len(input_list) >= max_data_m:\n",
    "            break       \n",
    "\n",
    "    print(len(input_list), len(bbox_list))\n",
    "    return input_list, bbox_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_annotation_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_x, list_y = load_xy(dict_annotation_coco, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_x[0].shape, list_y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(list_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_y[0][:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_y[0][:, 5:].reshape((-1, NUM_KEYPOINT, NUM_KEYPOINT_CH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### box check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in list_y:\n",
    "    box = y[:, :4]\n",
    "    box_wh = box[:, 2:4] - box[:, 0:2]\n",
    "    print('y', y.shape)\n",
    "    print('box_wh', box_wh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinate_map_np(h, w):\n",
    "    #return (h, w, 2)    \n",
    "    x = np.arange(w)\n",
    "    y = np.arange(h)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    xy = np.stack((X, Y), -1)\n",
    "    xy = (xy).astype(np.float32) + 0.5\n",
    "    return xy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_y[0][:, 5:].reshape((-1, NUM_KEYPOINT, NUM_KEYPOINT_CH))[:,:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_y[0][:, 5:].reshape((-1, NUM_KEYPOINT, NUM_KEYPOINT_CH))[:,:,-1] = 1+np.arange(NUM_KEYPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_y[0][:, 5:].reshape((-1, NUM_KEYPOINT, NUM_KEYPOINT_CH))[:,:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_data(list_x, list_y, stride=1):\n",
    "    \n",
    "    for i in range(0, len(list_x), stride):\n",
    "        x = list_x[i]\n",
    "        y = list_y[i]\n",
    "       \n",
    "        img = Image.fromarray(x)\n",
    "        kp = y[:, -NUM_KEYPOINT * NUM_KEYPOINT_CH:]        \n",
    "        kp_cls = kp[:, 2::3]\n",
    "        print(i, 'y', 'box', y[:, :4])\n",
    "        #print('kp', kp)\n",
    "        draw_box_keypoint(img, y, is_show_class=False)\n",
    "        display(img)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_data(list_x, list_y, stride=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/building-a-resnet-in-keras-e8f1322a49ba\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import Tensor\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization,\\\n",
    "                                    Add, AveragePooling2D, Flatten, Dense, MaxPool2D\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/tensorflow/addons/blob/v0.11.2/tensorflow_addons/image/__init__.py\n",
    "from tensorflow_addons.image.color_ops import sharpness\n",
    "from tensorflow_addons.image.filters import gaussian_filter2d\n",
    "from tensorflow_addons.image.dense_image_warp import dense_image_warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__, tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4 * 2 ** 7, 3 * 2 ** 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_image_shape = (384, 384)#384, 512, 640\n",
    "anchor_k = 9\n",
    "num_classes = 3\n",
    "level_start = 4\n",
    "level_end = 8\n",
    "l1 = 1e-8\n",
    "activation = 'swish'#'selu' is not converted to tflite\n",
    "kernel_init = tf.initializers.he_normal()\n",
    "edgecolors = np.random.rand(num_classes, 3) \n",
    "edgecolors = np.minimum(edgecolors+0.1, 1.0)\n",
    "path_weight = \"weight/keypoint_efficientDet-D2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(list_x)\n",
    "m_train = m//2\n",
    "list_x_train = list_x[:m_train]\n",
    "list_x_test = list_x[m_train:]\n",
    "list_y_train = list_y[:m_train]\n",
    "list_y_test = list_y[m_train:]\n",
    "m, m_train, len(list_x), len(list_x_train), len(list_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_xy(boxes):\n",
    "    \"\"\"Swaps order the of x and y coordinates of the boxes.\n",
    "    Arguments:\n",
    "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes.\n",
    "    Returns:\n",
    "      swapped boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.stack([boxes[:, 1], boxes[:, 0], boxes[:, 3], boxes[:, 2]], axis=-1)\n",
    "\n",
    "\n",
    "def convert_to_xywh(boxes):\n",
    "    \"\"\"Changes the box format to center, width and height.\n",
    "    Arguments:\n",
    "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
    "        representing bounding boxes where each box is of the format\n",
    "        `[xmin, ymin, xmax, ymax]`.\n",
    "    Returns:\n",
    "      converted boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.concat(\n",
    "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_to_corners(boxes):\n",
    "    \"\"\"Changes the box format to corner coordinates\n",
    "    Arguments:\n",
    "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
    "        representing bounding boxes where each box is of the format\n",
    "        `[x, y, width, height]`.\n",
    "    Returns:\n",
    "      converted boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.concat(\n",
    "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Computing pairwise Intersection Over Union (IOU)\n",
    "As we will see later in the example, we would be assigning ground truth boxes\n",
    "to anchor boxes based on the extent of overlapping. This will require us to\n",
    "calculate the Intersection Over Union (IOU) between all the anchor\n",
    "boxes and ground truth boxes pairs.\n",
    "\"\"\"\n",
    "\n",
    "def compute_iou(boxes1, boxes2):#compute_iou(anchor_boxes, gt_boxes)\n",
    "    \"\"\"Computes pairwise IOU matrix for given two sets of boxes\n",
    "    Arguments:\n",
    "      boxes1: A tensor with shape `(N, 4)` representing bounding boxes\n",
    "        where each box is of the format `[x, y, width, height]`.\n",
    "        boxes2: A tensor with shape `(M, 4)` representing bounding boxes\n",
    "        where each box is of the format `[x, y, width, height]`.\n",
    "    Returns:\n",
    "      pairwise IOU matrix with shape `(N, M)`, where the value at ith row\n",
    "        jth column holds the IOU between ith box and jth box from\n",
    "        boxes1 and boxes2 respectively.\n",
    "    \"\"\"\n",
    "    boxes1_corners = convert_to_corners(boxes1)\n",
    "    boxes2_corners = convert_to_corners(boxes2)\n",
    "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
    "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n",
    "    intersection = tf.maximum(0.0, rd - lu)\n",
    "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
    "    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
    "    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
    "    union_area = tf.maximum(\n",
    "        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n",
    "    )\n",
    "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "## Implementing Anchor generator\n",
    "Anchor boxes are fixed sized boxes that the model uses to predict the bounding\n",
    "box for an object. It does this by regressing the offset between the location\n",
    "of the object's center and the center of an anchor box, and then uses the width\n",
    "and height of the anchor box to predict a relative scale of the object. In the\n",
    "case of RetinaNet, each location on a given feature map has nine anchor boxes\n",
    "(at three scales and three ratios).\n",
    "\"\"\"\n",
    "class AnchorBox:\n",
    "    \"\"\"Generates anchor boxes.\n",
    "    This class has operations to generate anchor boxes for feature maps at\n",
    "    strides `[8, 16, 32, 64, 128]`. Where each anchor each box is of the\n",
    "    format `[x, y, width, height]`.\n",
    "    Attributes:\n",
    "      aspect_ratios: A list of float values representing the aspect ratios of\n",
    "        the anchor boxes at each location on the feature map\n",
    "      scales: A list of float values representing the scale of the anchor boxes\n",
    "        at each location on the feature map.\n",
    "      num_anchors: The number of anchor boxes at each location on feature map\n",
    "      areas: A list of float values representing the areas of the anchor\n",
    "        boxes for each feature map in the feature pyramid.\n",
    "      strides: A list of float value representing the strides for each feature\n",
    "        map in the feature pyramid.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.level_start = level_start\n",
    "        self.level_end = level_end\n",
    "        \n",
    "        if anchor_k==9:\n",
    "            self.aspect_ratios = [0.5, 1.0, 2.0]        \n",
    "            self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\n",
    "        else:\n",
    "            self.aspect_ratios = [1.0]        \n",
    "            self.scales = [2 ** x for x in [0]]\n",
    "                \n",
    "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
    "        self._strides = [2 ** i for i in range(self.level_start, self.level_end)]\n",
    "        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 196.0, 256.0]]                        \n",
    "        self._areas = self._areas[:level_end - level_start]\n",
    "        \n",
    "        self._anchor_dims = self._compute_dims()\n",
    "\n",
    "    def _compute_dims(self):\n",
    "        \"\"\"Computes anchor box dimensions for all ratios and scales at all levels\n",
    "        of the feature pyramid.\n",
    "        \"\"\"\n",
    "        anchor_dims_all = []\n",
    "        for area in self._areas:\n",
    "            anchor_dims = []\n",
    "            for ratio in self.aspect_ratios:\n",
    "                anchor_height = tf.math.sqrt(area / ratio)\n",
    "                anchor_width = area / anchor_height\n",
    "                dims = tf.reshape(\n",
    "                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n",
    "                )\n",
    "                for scale in self.scales:\n",
    "                    anchor_dims.append(scale * dims)\n",
    "            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n",
    "        return anchor_dims_all\n",
    "\n",
    "    def _get_anchors(self, feature_height, feature_width, level):\n",
    "        \"\"\"Generates anchor boxes for a given feature map size and level\n",
    "        Arguments:\n",
    "          feature_height: An integer representing the height of the feature map.\n",
    "          feature_width: An integer representing the width of the feature map.\n",
    "          level: An integer representing the level of the feature map in the\n",
    "            feature pyramid.\n",
    "        Returns:\n",
    "          anchor boxes with the shape\n",
    "          `(feature_height * feature_width * num_anchors, 4)`\n",
    "        \"\"\"\n",
    "        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n",
    "        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n",
    "        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - self.level_start]\n",
    "        centers = tf.expand_dims(centers, axis=-2)\n",
    "        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
    "        dims = tf.tile(\n",
    "            self._anchor_dims[level - self.level_start], [feature_height, feature_width, 1, 1]\n",
    "        )\n",
    "        anchors = tf.concat([centers, dims], axis=-1)\n",
    "        return tf.reshape(\n",
    "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
    "        )\n",
    "\n",
    "    def get_anchors(self, image_height, image_width):\n",
    "        \"\"\"Generates anchor boxes for all the feature maps of the feature pyramid.\n",
    "        Arguments:\n",
    "          image_height: Height of the input image.\n",
    "          image_width: Width of the input image.\n",
    "        Returns:\n",
    "          anchor boxes for all the feature maps, stacked as a single tensor\n",
    "            with shape `(total_anchors, 4)`\n",
    "        \"\"\"\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2 ** i),\n",
    "                tf.math.ceil(image_width / 2 ** i),\n",
    "                i,\n",
    "            )\n",
    "            for i in range(self.level_start, self.level_end)\n",
    "        ]\n",
    "        return tf.concat(anchors, axis=0)\n",
    "    \n",
    "    def get_anchors_check(self, image_height, image_width):\n",
    "        \"\"\"Generates anchor boxes for all the feature maps of the feature pyramid.\n",
    "        Arguments:\n",
    "          image_height: Height of the input image.\n",
    "          image_width: Width of the input image.\n",
    "        Returns:\n",
    "          anchor boxes for all the feature maps, stacked as a single tensor\n",
    "            with shape `(total_anchors, 4)`\n",
    "        \"\"\"\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2 ** i),\n",
    "                tf.math.ceil(image_width / 2 ** i),\n",
    "                i,\n",
    "            )\n",
    "            for i in range(self.level_start, self.level_end)\n",
    "        ]\n",
    "        return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_image_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_pixel_lr(net, h, w, c):\n",
    "    # net # (m, h, w, c)    \n",
    "    n0, n1 = tf.split(net, 2, -1)\n",
    "    n01 = tf.stack((n0, n1), 3) # (m, h, w, 2, c)    \n",
    "    out = tf.reshape(n01, [-1, h, w*2, c//2])\n",
    "    return out\n",
    "\n",
    "def shuffle_pixel_2x(net, h, w, c):\n",
    "    # net # (m, h, w, c)\n",
    "    #n0, n1 = tf.split(net, 2, -1)\n",
    "    n0, n1, n2, n3 = tf.split(net, 4, -1)\n",
    "    n01 = tf.stack((n0, n1), 3) # (m, h, w, 2, c)\n",
    "    n23 = tf.stack((n2, n3), 3) # (m, h, w, 2, c)\n",
    "\n",
    "    n01234 = tf.stack((n01, n23), 2)# (m, h, 2, w, 2, c)\n",
    "    out = tf.reshape(n01234, [-1, h*2, w*2, c//4])\n",
    "    return out\n",
    "\n",
    "\n",
    "def shuffle_pixel_3x(net, h, w):\n",
    "    # net # (m, h, w, c)\n",
    "    #n0, n1 = tf.split(net, 2, -1)\n",
    "    n0, n1, n2, n3, n4, n5, n6, n7, n8 = tf.split(net, 9, -1)\n",
    "    r0 = tf.stack((n0, n4, n1), 3) # (m, h, w, 2, c)\n",
    "    r1 = tf.stack((n5, n6, n7), 3) # (m, h, w, 2, c)\n",
    "    r2 = tf.stack((n2, n8, n3), 3) # (m, h, w, 2, c)\n",
    "\n",
    "    r = tf.stack((r0, r1, r2), 2)# (m, h, 2, w, 2, c)\n",
    "    out = tf.reshape(r, [-1, h*3, w*3, 1])\n",
    "    return out\n",
    "\n",
    "\n",
    "def shuffle_pixel_4x(net):\n",
    "    # net # (m, h, w, c)\n",
    "    #n0, n1 = tf.split(net, 2, -1)\n",
    "    net_split = tf.split(net, 16, -1)\n",
    "    r0 = tf.stack(net_split[0:4], 3) # (m, h, w, 2, c)\n",
    "    r1 = tf.stack(net_split[4:8], 3) # (m, h, w, 2, c)\n",
    "    r2 = tf.stack(net_split[8:12], 3)\n",
    "    r3 = tf.stack(net_split[12:16], 3)\n",
    "\n",
    "    r = tf.stack((r0, r1, r2, r3), 2)# (m, h, 4, w, 2, c)\n",
    "    #out = tf.reshape(r, [-1, h*4, w*4, 1])\n",
    "    out = tf.reshape(r, [-1])\n",
    "    return out\n",
    "\n",
    "\n",
    "def shuffle_pixel_5x(net, h, w, c=1):\n",
    "    # net # (m, h, w, c)\n",
    "    k = 5\n",
    "    net_split = tf.split(net, k*k, -1)\n",
    "    r0 = tf.stack(net_split[k * 0:k * 1], 3)\n",
    "    r1 = tf.stack(net_split[k * 1:k * 2], 3)\n",
    "    r2 = tf.stack(net_split[k * 2:k * 3], 3)\n",
    "    r3 = tf.stack(net_split[k * 3:k * 4], 3)\n",
    "    r4 = tf.stack(net_split[k * 4:k * 5], 3)\n",
    "\n",
    "    r = tf.stack((r0, r1, r2, r3, r4), 2)\n",
    "    out = tf.reshape(r, [-1, h*k, w*k, c])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_color_augment(x):\n",
    "    if tf.random.uniform(()) < -0.5:\n",
    "        x_max = tf.reduce_max(x, [1, 2], True)\n",
    "        x = x_max - x\n",
    "    if tf.random.uniform(()) < -0.2:\n",
    "        r, g, b = tf.split(x, 3, axis=-1)\n",
    "        x = tf.concat((r, b, g), -1)\n",
    "    elif tf.random.uniform(()) < -0.4:\n",
    "        r, g, b = tf.split(x, 3, axis=-1)\n",
    "        x = tf.concat((b, r, g), -1)\n",
    "    if tf.random.uniform(()) < 0.2:\n",
    "        x = tf.image.random_hue(x, 0.08/2)\n",
    "        x = tf.image.random_saturation(x, 0.7, 1.3)\n",
    "    if tf.random.uniform(()) < 0.2:\n",
    "        x = tf.image.random_brightness(x, 0.05)\n",
    "        x = tf.image.random_contrast(x, 0.8, 1.2)\n",
    "    if tf.random.uniform(()) < 0.2:\n",
    "        gray = tf.image.rgb_to_grayscale(x)\n",
    "        x = tf.concat((gray, gray, gray), -1)        \n",
    "   \n",
    "    if tf.random.uniform(()) < -0.2:\n",
    "        x = gaussian_filter2d(x, filter_shape=tuple(np.random.randint(1, 10, (2))), sigma=10)\n",
    "        #x = gaussian_filter2d(x, filter_shape=np.random.randint(3, 10, (2)), sigma=10)\n",
    "    if tf.random.uniform(()) < 0.2:        \n",
    "        x = sharpness(x, factor=10)    \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_image_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_flip_horizontal(image, boxes):\n",
    "    \"\"\"Flips image and boxes horizontally with 50% chance\n",
    "    Arguments:\n",
    "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
    "        image.\n",
    "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes,\n",
    "        having normalized coordinates.\n",
    "    Returns:\n",
    "      Randomly flipped image and boxes\n",
    "    \"\"\"\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        boxes = tf.stack([1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]], axis=-1)\n",
    "   \n",
    "    return image, boxes\n",
    "\n",
    "\n",
    "def resize_and_pad_image(\n",
    "    image, min_side=512.0, jitter=[128*4, 128*4+1], stride=128.0):\n",
    "   \n",
    "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "    \n",
    "    ratio = min_side / tf.reduce_min(image_shape)\n",
    "    \n",
    "    image_shape = ratio * image_shape\n",
    "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
    "    \n",
    "    image = tf.image.pad_to_bounding_box(image, 0, 0, padded_image_shape[0], padded_image_shape[1]) \n",
    "    \n",
    "    return image, image_shape, ratio\n",
    "\n",
    "def resize_and_pad_image_input(\n",
    "    image, bbox, kp, jitter=[padded_image_shape[0]-64, padded_image_shape[0]+1]):\n",
    "    \n",
    "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)    \n",
    "    dst_h = tf.random.uniform((), jitter[0], jitter[1], dtype=tf.float32)\n",
    "    dst_w = tf.random.uniform((), jitter[0], jitter[1], dtype=tf.float32)\n",
    "    dst_shape = tf.cast((dst_h, dst_w), tf.int32)\n",
    "    ratio_h = dst_h / image_shape[0]\n",
    "    ratio_w = dst_w / image_shape[1]    \n",
    "    \n",
    "    image = tf.image.resize(image, dst_shape)        \n",
    "    image = tf.image.pad_to_bounding_box(image, 0, 0, padded_image_shape[0], padded_image_shape[1])\n",
    "            \n",
    "    bbox_padded = tf.stack(\n",
    "        [\n",
    "            bbox[:, 0] * ratio_w,\n",
    "            bbox[:, 1] * ratio_h,\n",
    "            bbox[:, 2] * ratio_w,\n",
    "            bbox[:, 3] * ratio_h,\n",
    "        ], axis=-1,\n",
    "    )\n",
    "    \n",
    "    kp_padded = tf.stack(\n",
    "        [\n",
    "            kp[:, :, 0] * ratio_w,\n",
    "            kp[:, :, 1] * ratio_h,\n",
    "            kp[:, :, 2] * 1            \n",
    "        ], axis=-1,\n",
    "    )\n",
    "    \n",
    "    return image, bbox_padded, kp_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Preprocessing data\n",
    "Preprocessing the images involves two steps:\n",
    "- Resizing the image: Images are resized such that the shortest size is equal\n",
    "to 800 px, after resizing if the longest side of the image exceeds 1333 px,\n",
    "the image is resized such that the longest size is now capped at 1333 px.\n",
    "- Applying augmentation: Random scale jittering  and random horizontal flipping\n",
    "are the only augmentations applied to the images.\n",
    "Along with the images, bounding boxes are rescaled and flipped if required.\n",
    "\"\"\"\n",
    "\n",
    "def unnormalize_box(bbox, image_shape):\n",
    "    img_h = image_shape[0]\n",
    "    img_w = image_shape[1]\n",
    "    bbox = tf.stack(\n",
    "        [\n",
    "            bbox[:, 0] * img_w,\n",
    "            bbox[:, 1] * img_h,\n",
    "            bbox[:, 2] * img_w,\n",
    "            bbox[:, 3] * img_h,\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    bbox = convert_to_xywh(bbox)    \n",
    "    return bbox    \n",
    "\n",
    "\n",
    "def unnormalize_keypoint(keypoint, image_shape):\n",
    "    img_h = image_shape[0]\n",
    "    img_w = image_shape[1]\n",
    "    keypoint_un = tf.stack(\n",
    "        [\n",
    "            keypoint[:, :, 0] * img_w,\n",
    "            keypoint[:, :, 1] * img_h,\n",
    "            keypoint[:, :, 2]\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )    \n",
    "    \n",
    "    keypoint_un = tf.cast(keypoint_un, tf.float32)\n",
    "    return keypoint_un\n",
    "\n",
    "\n",
    "def resize_image(image, method='bilinear'):\n",
    "    image_shape = tf.cast(tf.shape(image)[0:2], tf.float32)    \n",
    "    resized_ratio = tf.cast(padded_image_shape / image_shape, tf.float32)\n",
    "    \n",
    "    image = tf.image.resize(image, padded_image_shape, method)    \n",
    "    return image, resized_ratio\n",
    "\n",
    "def flip_keypoint(kp, img_w):\n",
    "    #keypoint_3d (n, 17, 3)\n",
    "    kp_nose = kp[:, 0:1]\n",
    "    kp_left = kp[:, 1::2]\n",
    "    kp_right = kp[:, 2::2]        \n",
    "    kp_nose = tf.stack((img_w - kp_nose[:, :, 0], kp_nose[:, :, 1], kp_nose[:, :, 2]), -1)\n",
    "    kp_left = tf.stack((img_w - kp_left[:, :, 0], kp_left[:, :, 1], kp_left[:, :, 2]), -1)\n",
    "    kp_right = tf.stack((img_w - kp_right[:, :, 0], kp_right[:, :, 1], kp_right[:, :, 2]), -1)\n",
    "    \n",
    "    kp_rl = tf.stack((kp_right, kp_left), 2)\n",
    "    kp_rl = tf.reshape(kp_rl, [-1, NUM_KEYPOINT-1, NUM_KEYPOINT_CH])\n",
    "    kp_flip = tf.concat((kp_nose, kp_rl), 1)\n",
    "    return kp_flip\n",
    "\n",
    "def flip_displace(displace):\n",
    "    displace_lr = displace[:, ::-1]\n",
    "    dr_x, dr_y, dg_x, dg_y = tf.split(displace_lr, 4, -1)\n",
    "    displace_out = tf.concat((-1 * dr_x, dr_y, -1 * dg_x, dg_y), -1)    \n",
    "    return displace_out\n",
    "\n",
    "def random_flip_horizontal_inputs(image, boxes, kp):\n",
    "\n",
    "    image = tf.image.flip_left_right(image)\n",
    "    img_h = tf.shape(image)[0]\n",
    "    img_w = tf.shape(image)[1]\n",
    "    img_h = tf.cast(img_h, tf.float32)\n",
    "    img_w = tf.cast(img_w, tf.float32)\n",
    "    boxes = tf.stack([img_w - boxes[:, 2], boxes[:, 1], img_w - boxes[:, 0], boxes[:, 3]], axis=-1)\n",
    "    kp = flip_keypoint(kp, img_w)    \n",
    "\n",
    "    return image, boxes, kp\n",
    "\n",
    "def random_attach(image, boxes, cls, kp, displace):\n",
    "    \n",
    "    boxes_w = boxes[:, 2] - boxes[:, 0]\n",
    "    boxes_h = boxes[:, 3] - boxes[:, 1]\n",
    "    boxes_diag = tf.sqrt(tf.square(boxes_w) + tf.square(boxes_h))\n",
    "    if tf.reduce_max(boxes_diag) > 128:\n",
    "        image_shape = tf.cast(tf.shape(image)[0:2]//2, tf.int32)\n",
    "        image_h2 = tf.cast(image_shape[0], tf.float32)\n",
    "        image_w2 = tf.cast(image_shape[1], tf.float32)\n",
    "        image_s = tf.image.resize(image, image_shape)\n",
    "        image_color_shuffle = tf.stack((image_s[:,:,0],image_s[:,:,2],image_s[:,:,1]), -1)\n",
    "        image_s2 = tf.concat((image_s, image_color_shuffle), axis=0)\n",
    "        \n",
    "        boxes_0 = boxes/2\n",
    "        boxes_1 = tf.stack((boxes_0[:, 0], image_h2+boxes_0[:, 1],boxes_0[:, 2],image_h2+boxes_0[:, 3]), -1)\n",
    "       \n",
    "        boxes_01 = tf.concat((boxes_0, boxes_1), 0)\n",
    "                \n",
    "        kp0 = tf.stack((kp[:,:,0]/2, kp[:,:,1]/2, kp[:,:,2]), -1)\n",
    "        kp1 = tf.stack((kp0[:,:,0], image_h2 + kp0[:,:,1], kp0[:,:,2]), -1)\n",
    "       \n",
    "        kp01 = tf.concat((kp0, kp1), 0)\n",
    "        \n",
    "        displace_s = tf.image.resize(displace, image_shape, 'nearest')   \n",
    "        displace_s2 = tf.concat((displace_s, displace_s), axis=0)\n",
    "              \n",
    "        image_s2_flip, boxes_01_flip, kp01_flip, displace_s2_flip = random_flip_horizontal_inputs(image_s2, boxes_01, kp01, displace_s2)\n",
    "        \n",
    "        boxes_01_flip = tf.stack((image_w2 + boxes_01_flip[:, 0],boxes_01_flip[:, 1],image_w2 +boxes_01_flip[:, 2],boxes_01_flip[:, 3]), -1)\n",
    "        kp01_flip = tf.stack((image_w2 + kp01_flip[:,:,0], kp01_flip[:,:,1], kp01_flip[:,:,2]), -1)\n",
    "        image = tf.concat((image_s2, image_s2_flip), axis=1)\n",
    "        image = tf.cast(image, tf.uint8)\n",
    "        boxes = tf.concat((boxes_01, boxes_01_flip), 0)\n",
    "        kp = tf.concat((kp01, kp01_flip), 0)\n",
    "        displace = tf.concat((displace_s2, displace_s2_flip), axis=1)\n",
    "        cls = tf.repeat(cls, 4, axis=0)\n",
    "        \n",
    "    return image, boxes, cls, kp, displace\n",
    "\n",
    "def preprocess_data(image, label):     \n",
    "     \n",
    "    #label (m,56)\n",
    "    bbox_norm = label[:, :4]    \n",
    "    cls = label[:, 4]\n",
    "    keypoints = label[:, 5:]\n",
    "    keypoint_3d = tf.reshape(keypoints, [-1, NUM_KEYPOINT, NUM_KEYPOINT_CH])    \n",
    "    keypoint_3d = tf.cast(keypoint_3d, tf.float32)\n",
    "    \n",
    "    #if tf.random.uniform(()) > 10.5: image, bbox_norm, cls, keypoint_3d, displace = random_attach(image, bbox_norm, cls, keypoint_3d, displace)\n",
    "        \n",
    "    image, bbox_norm, keypoint_3d = resize_and_pad_image_input(image, bbox_norm, keypoint_3d)\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image, bbox_norm, keypoint_3d = random_flip_horizontal_inputs(image, bbox_norm, keypoint_3d)             \n",
    "    \n",
    "    #image, image_shape, _, bbox = resize_and_pad_image_bbox(image, bbox)    \n",
    "    #image, image_shape, _ = resize_and_pad_image(image)\n",
    "        \n",
    "    image, image_shape = resize_image(image)        \n",
    "    \n",
    "    bbox_unnorm = unnormalize_box(bbox_norm, image_shape)\n",
    "    keypoints_unnorm = unnormalize_keypoint(keypoint_3d, image_shape)\n",
    "    #print('keypoints_unnorm', keypoints_unnorm)\n",
    "    \n",
    "    return image, bbox_unnorm, cls, keypoints_unnorm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(tf.random.uniform((100,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_KEYPOINT, NUM_KEYPOINT_CH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_label, NUM_KEYPOINT, NUM_KEYPOINT_CH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = list_y_train[0]\n",
    "y0.shape, y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_to_radian(angle):\n",
    "    return angle * np.pi/180\n",
    "\n",
    "def box_convert_to_xywh(boxes):\n",
    "    return np.concatenate(\n",
    "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]], axis=-1,)\n",
    "\n",
    "def box_convert_to_corners(boxes):    \n",
    "    return np.concatenate(\n",
    "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0], axis=-1,)\n",
    "\n",
    "def convert_theta(box_xy, angle, img_h, img_w):\n",
    "    img_size = np.array([[img_w, img_h]]).astype(np.float)\n",
    "    box_xy_norm = box_xy / img_size    \n",
    "    box_uv = (np.reshape(box_xy_norm, [-1, 2]) - 0.5) * 2\n",
    "    \n",
    "    scale_mat = np.array([1, 0, 0, 1.0*img_h/img_w]).reshape((2,2))\n",
    "    scale_mat_rev = np.array([1, 0, 0, 1.0*img_w/img_h]).reshape((2,2))\n",
    "    \n",
    "    radian = angle_to_radian(angle)        \n",
    "    rotate_mat = np.array([np.cos(radian), -np.sin(radian), np.sin(radian), np.cos(radian)])        \n",
    "    rotate_mat = np.reshape(rotate_mat, (2, 2))\n",
    "    \n",
    "    box_uv_trans = np.matmul(box_uv, scale_mat)\n",
    "    box_uv_trans = np.matmul(box_uv_trans, rotate_mat)\n",
    "    box_uv_trans = np.matmul(box_uv_trans, scale_mat_rev)\n",
    "    box_trans = (box_uv_trans + 1)/2\n",
    "    box_trans_xy = np.reshape(box_trans, [-1, 2]) * img_size\n",
    "    return box_trans_xy\n",
    "\n",
    "def rotate_box(box, angle, img_h, img_w):\n",
    "    box_other = box[:, 4:]\n",
    "    box_xywh = box_convert_to_xywh(box[:, :4])\n",
    "    box_xy = box_xywh[:, :2] \n",
    "    box_wh = box_xywh[:, 2:4]\n",
    "    \n",
    "    box_trans_xy = convert_theta(box_xy, angle, img_h, img_w)\n",
    "    \n",
    "    box_trans_xywh = np.concatenate((box_trans_xy, box_wh), axis=1)\n",
    "    box_trans = box_convert_to_corners(box_trans_xywh)\n",
    "    out = np.concatenate((box_trans, box_other), -1)\n",
    "    return out\n",
    "\n",
    "def rotate_keypoint(kp, angle, img_h, img_w):\n",
    "    \n",
    "    kp_xy = kp[:, :, :2] \n",
    "    kp_vis = kp[:, :, 2:]     \n",
    "\n",
    "    kp_xy = convert_theta(kp_xy, angle, img_h, img_w)\n",
    "    kp_xy = np.reshape(kp_xy, [-1, NUM_KEYPOINT, 2])\n",
    "    kp_3d = np.concatenate((kp_xy, kp_vis), axis=-1)    \n",
    "    return kp_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_y_train[0][:,:4], rotate_box(list_y_train[0], 10, 10, 10)[:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = list_y_train[0][:, 5:]\n",
    "keypoint_3d = np.reshape(keypoints, [-1, NUM_KEYPOINT, NUM_KEYPOINT_CH])    \n",
    "keypoint_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rotate_keypoint(keypoint_3d, 1, 512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_rotate_aug = True\n",
    "def generator():\n",
    "    #ind = np.arange((len(list_x_train)//2)*2)\n",
    "    #ind = np.arange(len(list_x_train))    \n",
    "    for i in range(len(list_x_train)):\n",
    "        x = list_x_train[i]\n",
    "        y = list_y_train[i]        \n",
    "        if is_rotate_aug:            \n",
    "            angle = 20 * (np.random.rand() - 0.5) #0~1 > -0.5~0.5\n",
    "            if np.abs(angle) > 0.1:                \n",
    "                x_rotated = Image.fromarray(x).rotate(angle)\n",
    "                x = np.array(x_rotated)\n",
    "                box = y[:, :5]\n",
    "                #cls = y[:, 4:5]\n",
    "                keypoints = y[:, 5:]\n",
    "                keypoint_3d = np.reshape(keypoints, [-1, NUM_KEYPOINT, NUM_KEYPOINT_CH])                                                    \n",
    "                img_h, img_w, img_c = x.shape\n",
    "                box_trans = rotate_box(box, angle, img_h, img_w)            \n",
    "                keypoint_trans = rotate_keypoint(keypoint_3d, angle, img_h, img_w)\n",
    "                keypoint_trans = np.reshape(keypoint_trans, [-1, NUM_KEYPOINT * NUM_KEYPOINT_CH])\n",
    "                y = np.concatenate((box_trans, keypoint_trans), -1)\n",
    "        yield (x, y)\n",
    "\n",
    "def generator_test():    \n",
    "    for i in range(len(list_x_test)):\n",
    "        x = list_x_test[i]\n",
    "        y = list_y_test[i]\n",
    "        yield (x, y)\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    generator, \n",
    "    output_types=(tf.uint8, tf.float32), \n",
    "    output_shapes=(tf.TensorShape([None, None, 3]), tf.TensorShape([None, channel_label])))\n",
    "dataset_test = tf.data.Dataset.from_generator(\n",
    "    generator_test, \n",
    "    output_types=(tf.uint8, tf.float32), \n",
    "    output_shapes=(tf.TensorShape([None, None, 3]), tf.TensorShape([None, channel_label])))\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "for example in tfds.as_numpy(dataset):\n",
    "    image = example[0]\n",
    "    label = example[1]\n",
    "    \n",
    "    print(image.dtype, image.shape, label.shape)\n",
    "    print(label[0])\n",
    "    #print(label[0][-NUM_KEYPOINT*3::3])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 확인\n",
    "train_dataset = dataset.map(preprocess_data)\n",
    "for x, yb, yc, yk in train_dataset:\n",
    "    print('yb', yb)\n",
    "    print('yc', yc)\n",
    "    print('yk', yk)    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinate_map_uv(h, w):\n",
    "    #return (6, 18, 256)\n",
    "    x = tf.range(0.5, w, 1) / tf.cast(w, tf.float32) * 2.0 -1\n",
    "    y = tf.range(0.5, h, 1) / tf.cast(h, tf.float32) * 2.0 -1    \n",
    "    X, Y = tf.meshgrid(x, y)\n",
    "    xy = tf.stack((X, Y), -1)\n",
    "    xy = tf.expand_dims(xy, axis=0)   \n",
    "    return xy \n",
    "\n",
    "def coordinate_map(h, w):\n",
    "    #return (h, w, 2)    \n",
    "    x = tf.range(w)\n",
    "    y = tf.range(h)\n",
    "    X, Y = tf.meshgrid(x, y)\n",
    "    xy = tf.stack((X, Y), -1)\n",
    "    xy = tf.cast(xy, tf.float32) + 0.5\n",
    "    return xy \n",
    "\n",
    "\n",
    "def coordinate_map_norm(h, w):\n",
    "    #return (h, w, 2)    \n",
    "    x = (tf.range(w, dtype=tf.float32) + 0.5) / w\n",
    "    y = (tf.range(h, dtype=tf.float32) + 0.5) / h\n",
    "    X, Y = tf.meshgrid(x, y)\n",
    "    xy = tf.stack((X, Y), -1)\n",
    "    xy = tf.cast(xy, tf.float32)\n",
    "    return xy \n",
    "\n",
    "\n",
    "def add_map(net):\n",
    "    shape = tf.shape(net)\n",
    "    map_norm = coordinate_map(shape[1], shape[2])            \n",
    "    map_norm = tf.expand_dims(map_norm, 0)\n",
    "    #net = tf.concat((map_norm + net[:, :, :, :2], net[:, :, :, 2:]), -1)    \n",
    "    net = tf.concat((net[:, :, :, :-2], net[:, :, :, -2:] + map_norm), -1)    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_grid_generator(height, width, theta):\n",
    "    num_batch = tf.shape(theta)[0]\n",
    "    \n",
    "    x = tf.linspace(-1.0, 1.0, width)\n",
    "    y = tf.linspace(-1.0, 1.0, height)\n",
    "    x_t, y_t = tf.meshgrid(x, y)\n",
    "    x_t_flat = tf.reshape(x_t, [-1])\n",
    "    y_t_flat = tf.reshape(y_t, [-1])\n",
    "\n",
    "    ones = tf.ones_like(x_t_flat)\n",
    "    sampling_grid = tf.stack([x_t_flat, y_t_flat, ones])  # (3, h*w)\n",
    "    sampling_grid = tf.expand_dims(sampling_grid, axis=0)\n",
    "    # sampling_grid = tf.tile(sampling_grid, tf.stack([num_batch, 1, 1]))#(num_batch, 3, h*w)\n",
    "    sampling_grid = tf.tile(sampling_grid, [num_batch, 1, 1])  # (num_batch, 3, h*w)\n",
    "    theta = tf.cast(theta, tf.float32)\n",
    "    sampling_grid = tf.cast(sampling_grid, tf.float32)\n",
    "\n",
    "    batch_grids = tf.matmul(theta, sampling_grid)  # (m, 2, 3)@(m, 3, h*w)=(m,2,h*w)\n",
    "    batch_grids = tf.reshape(batch_grids, [num_batch, 2, height, width])\n",
    "    return batch_grids\n",
    "\n",
    "\n",
    "def get_pixel_value(img, x, y):\n",
    "    # img (m,h,w,c)\n",
    "    # x,y (m,h,w)\n",
    "    shape = tf.shape(x)\n",
    "    m = shape[0]\n",
    "    h = shape[1]\n",
    "    w = shape[2]\n",
    "    batch_idx = tf.range(0, m)\n",
    "    batch_idx = tf.reshape(batch_idx, [m, 1, 1])\n",
    "    b = tf.tile(batch_idx, [1, h, w])\n",
    "\n",
    "    indices = tf.stack([b, y, x], axis=3)  # (m,h,w,3)\n",
    "\n",
    "    return tf.gather_nd(img, indices)\n",
    "\n",
    "\n",
    "def bilinear_sampler(img, batch_grids):\n",
    "    # batch_grids (m, 2, h, w)\n",
    "    # img (m,h,w,c)\n",
    "    uv_x = batch_grids[:, 0]\n",
    "    uv_y = batch_grids[:, 1]\n",
    "    H = tf.shape(img)[1]\n",
    "    W = tf.shape(img)[2]\n",
    "    max_y = tf.cast(H - 1, tf.float32)\n",
    "    max_x = tf.cast(W - 1, tf.float32)\n",
    "    # x [-1, 1]\n",
    "    x = 0.5 * ((uv_x + 1.0) * max_x)\n",
    "    y = 0.5 * ((uv_y + 1.0) * max_y)\n",
    "\n",
    "    # grab 4 nearest corner points for each (x_i, y_i)\n",
    "    x0 = tf.floor(x)  # precision bad?\n",
    "    x1 = x0 + 1\n",
    "    y0 = tf.floor(y)\n",
    "    y1 = y0 + 1\n",
    "\n",
    "    # clip out of boundary index\n",
    "    x0 = tf.clip_by_value(x0, 0, max_x)\n",
    "    x1 = tf.clip_by_value(x1, 0, max_x)\n",
    "    y0 = tf.clip_by_value(y0, 0, max_y)\n",
    "    y1 = tf.clip_by_value(y1, 0, max_y)\n",
    "\n",
    "    # deltas\n",
    "    wa = (x1 - x) * (y1 - y)\n",
    "    wb = (x1 - x) * (y - y0)\n",
    "    wc = (x - x0) * (y1 - y)\n",
    "    wd = (x - x0) * (y - y0)\n",
    "\n",
    "    wa = tf.expand_dims(wa, -1)\n",
    "    wb = tf.expand_dims(wb, -1)\n",
    "    wc = tf.expand_dims(wc, -1)\n",
    "    wd = tf.expand_dims(wd, -1)\n",
    "\n",
    "    x0 = tf.cast(x0, tf.int32)\n",
    "    x1 = tf.cast(x1, tf.int32)\n",
    "    y0 = tf.cast(y0, tf.int32)\n",
    "    y1 = tf.cast(y1, tf.int32)\n",
    "\n",
    "    Ia = get_pixel_value(img, x0, y0)\n",
    "    Ib = get_pixel_value(img, x0, y1)\n",
    "    Ic = get_pixel_value(img, x1, y0)\n",
    "    Id = get_pixel_value(img, x1, y1)\n",
    "\n",
    "    out = tf.add_n([wa*Ia, wb*Ib, wc*Ic, wd*Id])\n",
    "\n",
    "    return out\n",
    "\n",
    "def grid_generator(height, width, theta):\n",
    "    num_batch = tf.shape(theta)[0]\n",
    "    \n",
    "    x = tf.linspace(-1.0, 1.0, width)\n",
    "    y = tf.linspace(-1.0, 1.0, height)\n",
    "    x_t, y_t = tf.meshgrid(x, y)\n",
    "    x_t_flat = tf.reshape(x_t, [-1])\n",
    "    y_t_flat = tf.reshape(y_t, [-1])\n",
    "\n",
    "    ones = tf.ones_like(x_t_flat)\n",
    "    z = tf.ones_like(x_t_flat)\n",
    "    sampling_grid = tf.stack([x_t_flat, y_t_flat, ones])  # (3, h*w)\n",
    "    sampling_grid = tf.expand_dims(sampling_grid, axis=0)    \n",
    "    sampling_grid = tf.tile(sampling_grid, [num_batch, 1, 1])  # (num_batch, 3, h*w)\n",
    "    theta = tf.cast(theta, tf.float32)\n",
    "    sampling_grid = tf.cast(sampling_grid, tf.float32)\n",
    "\n",
    "    batch_grids = tf.matmul(theta, sampling_grid)  # (m, 3, 3)@(m, 4, h*w)=(m,3,h*w)\n",
    "    #print('batch_grids', batch_grids)\n",
    "    w = batch_grids[:, -1:]    \n",
    "    #print('w','mean', tf.reduce_mean(w), w)\n",
    "    \n",
    "    batch_grids = batch_grids[:, :2]/w\n",
    "    batch_grids = tf.reshape(batch_grids, [num_batch, 2, height, width])\n",
    "    return batch_grids\n",
    "\n",
    "\n",
    "def sampling(net, theta, dst_h, dst_w):\n",
    "    \n",
    "    #theta = tf.reshape(theta, [-1, 3, 3])\n",
    "    #theta = theta[:, :2]\n",
    "    #h = tf.shape(net)[1]\n",
    "    #w = tf.shape(net)[2]\n",
    "    #batch_grids = affine_grid_generator(dst_h, dst_w, theta)\n",
    "    batch_grids = grid_generator(dst_h, dst_w, theta)\n",
    "    out = bilinear_sampler(net, batch_grids)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_displace_map(boxes, keypoints, x_h, x_w):\n",
    "    #gt_boxes[i], keypoints[i], images_shape[1], images_shape[2]  \n",
    "    #boxes = y[:, :4]\n",
    "    #keypoints = y[:, 5:].reshape((-1, NUM_KEYPOINT, NUM_KEYPOINT_CH)).astype(np.int)# 키포인트 별로      \n",
    "    keypoints_xy = keypoints[:, :, :2]\n",
    "    keypoints_cls = keypoints[:, :, 2]\n",
    "    visible = keypoints_cls > VISIBILITY_OCCLUDED\n",
    "    #print('keypoints', keypoints)\n",
    "\n",
    "    #boxes_wh = boxes[:, 2:4] - boxes[:, 0:2]\n",
    "    #boxes_cxy = (boxes[:, 2:4] + boxes[:, 0:2]) / 2    \n",
    "    boxes_cxy = boxes[:, 0:2]\n",
    "    boxes_wh = boxes[:, 2:4]\n",
    "\n",
    "    #box_cxy:(n, 2), kp_xy:(n, k, 2), k = 17\n",
    "    #displace : (n, k, 2)\n",
    "    #displace_kp_to_root : (n, k, 2) - (n, 1, 2) = (n, k, 2)\n",
    "    #displace_kp_norm : (n, k, 2) / (n, 1, 2) = (n, k, 2)        \n",
    "    roots = tf.expand_dims(boxes_cxy, 1)\n",
    "    x_size = tf.cast(tf.reshape([x_w, x_h], [1, 1, 2]), tf.float32)\n",
    "    displace_kp_to_root_norm = (keypoints_xy - roots) / tf.expand_dims(boxes_wh, 1) #(n, k, 2)\n",
    "    displace_kp_to_root_norm_global = (keypoints_xy - roots) / x_size #(n, k, 2)\n",
    "\n",
    "    #heatmap_mask = |coord_map - kp| < disk radius : (h, w, 1, 2) - (1, 1, nk, 2)      \n",
    "    coord_map = coordinate_map(x_h, x_w)#(h, w, 2)\n",
    "    #kp_coord_distance = coord_map - kp : (h, w, 1, 2) - (1, 1, nk, 2) = (h, w, nk, 2)\n",
    "    kp_coord_distance_xy = tf.expand_dims(coord_map, 2) - tf.reshape(keypoints_xy, [1, 1, -1, 2])\n",
    "    kp_coord_distance = tf.sqrt(tf.reduce_sum(tf.square(kp_coord_distance_xy), -1))#(h, w, nk)\n",
    "    #print('kp_coord_distance', kp_coord_distance.shape)\n",
    "    disk_radius_rel = tf.reshape(tf.reduce_min(boxes_wh, -1), [1, 1, -1, 1])*0.05\n",
    "    #disk_radius = 10\n",
    "\n",
    "    kp_coord_distance_4d = tf.reshape(kp_coord_distance, [x_h, x_w, -1, NUM_KEYPOINT])#(h,w,n,k)\n",
    "    heatmap_mask_kp = kp_coord_distance_4d < disk_radius_rel\n",
    "    heatmap_mask_kp = tf.expand_dims(heatmap_mask_kp, -1)\n",
    "    heatmap_mask_kp = tf.reshape(heatmap_mask_kp, [x_h, x_w, -1, 1])\n",
    "\n",
    "    #heatmap_mask_kp = np.expand_dims((kp_coord_distance < disk_radius).astype(np.float32), -1)\n",
    "    visible_4d = tf.reshape(visible, [1, 1, -1, 1])\n",
    "\n",
    "    heatmap_mask_kp_valid = tf.cast(heatmap_mask_kp, tf.float32) * tf.cast(visible_4d, tf.float32)\n",
    "    #(h, w, nk, 1) * (1, 1, nk, 1) * (1, 1, nk, 2)\n",
    "    displace_kp = heatmap_mask_kp_valid * tf.reshape(displace_kp_to_root_norm, [1, 1, -1, 2])#(h, w, nk, 2)\n",
    "    displace_kp_global = heatmap_mask_kp_valid * tf.reshape(displace_kp_to_root_norm_global, [1, 1, -1, 2])#(h, w, nk, 2)\n",
    "    #print('displace_kp', displace_kp.shape)        \n",
    "\n",
    "    displace_kp_max = tf.reduce_max(displace_kp, 2)\n",
    "    displace_kp_min = tf.reduce_min(displace_kp, 2)\n",
    "    displace_kp = tf.where(tf.abs(displace_kp_min) > displace_kp_max, displace_kp_min, displace_kp_max)\n",
    "\n",
    "    displace_kp_global_max = tf.reduce_max(displace_kp_global, 2)\n",
    "    displace_kp_global_min = tf.reduce_min(displace_kp_global, 2)\n",
    "    displace_kp_global = tf.where(tf.abs(displace_kp_global_min) > displace_kp_global_max, displace_kp_global_min, displace_kp_global_max)\n",
    "    \n",
    "    heatmap_mask = tf.reduce_any(heatmap_mask_kp_valid > 0, 2)        \n",
    "    heatmap_mask = tf.cast(heatmap_mask, tf.float32)\n",
    "\n",
    "    displace_map = tf.concat((displace_kp, displace_kp_global), -1) * heatmap_mask\n",
    "    return displace_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Encoding labels\n",
    "The raw labels, consisting of bounding boxes and class ids need to be\n",
    "transformed into targets for training. This transformation consists of\n",
    "the following steps:\n",
    "- Generating anchor boxes for the given image dimensions\n",
    "- Assigning ground truth boxes to the anchor boxes\n",
    "- The anchor boxes that are not assigned any objects, are either assigned the\n",
    "background class or ignored depending on the IOU\n",
    "- Generating the classification and regression targets using anchor boxes\n",
    "\"\"\"\n",
    "\n",
    "class LabelEncoder:\n",
    "    \"\"\"Transforms the raw labels into targets for training.\n",
    "    This class has operations to generate targets for a batch of samples which\n",
    "    is made up of the input images, bounding boxes for the objects present and\n",
    "    their class ids.\n",
    "    Attributes:\n",
    "      anchor_box: Anchor box generator to encode the bounding boxes.\n",
    "      box_variance: The scaling factors used to scale the bounding box targets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._anchor_box = AnchorBox()\n",
    "        self._box_variance = tf.convert_to_tensor(\n",
    "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
    "        )           \n",
    "    \n",
    "    def _match_anchor_boxes(\n",
    "        self, anchor_boxes, gt_boxes, match_iou=0.3, ignore_iou=0.1\n",
    "    ):\n",
    "        # anchor : (anchor_k, 4), gt_boxes : (box_m, 4)\n",
    "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)#(anchor_k, box_m)        \n",
    "        max_iou = tf.reduce_max(iou_matrix, axis=1)#from anchor to object-box        \n",
    "        matched_gt_idx = tf.argmax(iou_matrix, axis=1)    \n",
    "        positive_mask = tf.greater_equal(max_iou, match_iou)# not only this, but also need max iou cell\n",
    "        \n",
    "        positive_proposal_mask = tf.greater_equal(iou_matrix, match_iou)\n",
    "        positive_mask = tf.reduce_any(positive_proposal_mask, axis=1)        \n",
    "        negative_mask = tf.less(max_iou, ignore_iou)\n",
    "        \n",
    "        max_iou_anchor = tf.reduce_max(iou_matrix, axis=0, keepdims=True)\n",
    "        max_iou_anchor_mask = tf.greater_equal(iou_matrix, max_iou_anchor)\n",
    "        \n",
    "        positive_max_mask = tf.reduce_any(max_iou_anchor_mask, axis=1)\n",
    "        positive_mask = tf.logical_or(positive_mask, positive_max_mask)#new      \n",
    "        \n",
    "        negative_mask = tf.logical_and(negative_mask, tf.logical_not(positive_mask))\n",
    "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))        \n",
    "        \n",
    "        return (\n",
    "            matched_gt_idx,            \n",
    "            tf.cast(positive_mask, dtype=tf.float32),\n",
    "            tf.cast(positive_max_mask, dtype=tf.float32),            \n",
    "            tf.cast(ignore_mask, dtype=tf.float32),\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
    "        \"\"\"Transforms the ground truth boxes into targets for training\"\"\"\n",
    "        target = tf.concat(\n",
    "            [\n",
    "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
    "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        target = target / self._box_variance\n",
    "        return target\n",
    "    \n",
    "    def _compute_keypoint_target(self, anchor_boxes, matched_gt_points):\n",
    "        \"\"\"Transforms the ground truth boxes into targets for training\"\"\"\n",
    "        target = (matched_gt_points[:, :, :2] - anchor_boxes[:, None, :2]) / anchor_boxes[:, None, 2:]        \n",
    "        target = target / self._box_variance[:2]\n",
    "        return target    \n",
    "      \n",
    "    \n",
    "    def _encode_sample(self, image_shape, gt_boxes, cls_ids, gt_keypoints):\n",
    "        \"\"\"Creates box and classification targets for a single sample\"\"\"\n",
    "        \n",
    "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
    "        \n",
    "        matched_gt_idx, positive_mask, positive_max_mask, ignore_mask = self._match_anchor_boxes(\n",
    "            anchor_boxes, gt_boxes)\n",
    "        \n",
    "        gt_keypoints_xy = gt_keypoints[:, :, :2]\n",
    "        gt_keypoints_cls = gt_keypoints[:, :, 2]       \n",
    "        \n",
    "        print('gt_keypoints_xy', gt_keypoints_xy)\n",
    "        \n",
    "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
    "        matched_gt_keypoint_xy = tf.gather(gt_keypoints_xy, matched_gt_idx)\n",
    "        \n",
    "        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)\n",
    "        keypoint_xy_target = self._compute_keypoint_target(anchor_boxes, matched_gt_keypoint_xy)\n",
    "                \n",
    "        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
    "        matched_gt_keypoint_cls = tf.gather(gt_keypoints_cls, matched_gt_idx)\n",
    "                \n",
    "        cls_target = tf.where(tf.not_equal(positive_mask, 1.0), 0.0, matched_gt_cls_ids)        \n",
    "        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -1.0, cls_target)\n",
    "                \n",
    "        #[?,4], [?,?], [?,17,2], []\n",
    "        keypoint_xy_target = tf.reshape(keypoint_xy_target, [-1, NUM_KEYPOINT * 2])\n",
    "        keypoint_cls_target = tf.reshape(matched_gt_keypoint_cls, [-1, NUM_KEYPOINT])\n",
    "                \n",
    "        cls_target = tf.expand_dims(cls_target, axis=-1)\n",
    "        targets = tf.concat([box_target, cls_target, keypoint_xy_target, keypoint_cls_target], axis=-1)\n",
    "        return targets\n",
    "    \n",
    "    \n",
    "    def _encode_keypoint_heatmap(self, img_h, img_w, keypoints):\n",
    "        #keypoints (n, 17, 3)\n",
    "        #return gt_map (h, w)        \n",
    "        neighbor_k = 1\n",
    "        keypoints_xy = keypoints[:, :, :2]\n",
    "        keypoints_visible = keypoints[:, :, 2]\n",
    "        keypoints_cls = tf.zeros_like(keypoints_visible) + tf.range(NUM_KEYPOINT, dtype=tf.float32)\n",
    "                \n",
    "        xy_map = coordinate_map(img_h, img_w)           \n",
    "        \n",
    "        keypoints_xy_5d = tf.reshape(keypoints_xy, [1, 1, -1, NUM_KEYPOINT, 2])#(1, 1, n, 17, 2)        \n",
    "        keypoints_visible_5d = tf.reshape(keypoints_visible, [1, 1, -1, NUM_KEYPOINT, 1])\n",
    "        keypoints_visible_4d_mask = tf.cast(keypoints_visible_5d[:,:,:,:,0] > VISIBILITY_NOT_EXIST, tf.float32)\n",
    "                \n",
    "        keypoints_xy_5d_to_find_min = keypoints_xy_5d + 5000 * tf.cast(keypoints_visible_5d < 1,tf.float32)\n",
    "        visible_max_xy = tf.reduce_max(keypoints_xy_5d, 3, True)\n",
    "        visible_min_xy = tf.reduce_min(keypoints_xy_5d_to_find_min, 3, True)\n",
    "        visible_xy_range = tf.abs(visible_max_xy - visible_min_xy)#(1, 1, n, 1, 2)\n",
    "        \n",
    "        #neighbor_k_relative = neighbor_k + 0.03 * tf.reduce_mean(visible_xy_range, -1)#(1, 1, n, 1)\n",
    "        neighbor_k_relative = neighbor_k + 0.02 * tf.reduce_mean(visible_xy_range, -1) * keypoints_visible_4d_mask\n",
    "        #neighbor_k_relative = neighbor_k_relative + (2 * keypoints_visible_4d_mask)\n",
    "        \n",
    "        xy_map_exp = tf.expand_dims(tf.expand_dims(xy_map, 2), 2)\n",
    "        distance_xy = xy_map_exp - keypoints_xy_5d#(h, w, 1, 1, 2) - (1, 1, n, 17, 2)               \n",
    "        distance = tf.sqrt(tf.reduce_sum(tf.square(distance_xy), -1))#(h, w, n, 17)\n",
    "        \n",
    "        #(h, w, n, 17)\n",
    "        heatmap_visible = tf.cast(distance < neighbor_k_relative, tf.float32) * keypoints_visible_5d[:,:,:,:,0]\n",
    "        heatmap_visible = tf.cast(tf.reduce_max(heatmap_visible, [-2, -1]), tf.int32)\n",
    "        \n",
    "        disk_mask = tf.cast(distance < neighbor_k_relative, tf.float32) * keypoints_visible_4d_mask\n",
    "        bg_zero = tf.zeros_like(distance[:,:,:,:1])\n",
    "                \n",
    "        disk_exist = tf.reduce_sum(disk_mask, [2, 3], True) > 0\n",
    "        disk_exist_mask = tf.cast(disk_exist, tf.float32)\n",
    "        bg_map = disk_exist_mask * 10000 + (1 - disk_exist_mask) * bg_zero\n",
    "        \n",
    "        distance_map_with_bg = tf.concat((bg_map, distance), -1)        \n",
    "        distance_map_with_bg = tf.reshape(distance_map_with_bg, [img_h, img_w, -1]) \n",
    "        \n",
    "        cls_map = tf.argmin(distance_map_with_bg, -1, output_type=tf.int32)#(h, w, n)                \n",
    "        cls_k = NUM_KEYPOINT + 1\n",
    "        cls_map = cls_map % cls_k        \n",
    "        \n",
    "        out = tf.stack((cls_map, heatmap_visible), -1)\n",
    "        return out\n",
    "    \n",
    "    def convert_box_to_map(self, boxes, kps, img_h, img_w):\n",
    "        #keypoints (n, 17, 3), box:convert_to_xywh\n",
    "        #box : xywh[315.802  77.022  16.874  13.665]\n",
    "        #sampling(net, theta, dst_h, dst_w) #를 이용해서 heatmap 라벨에 -1을 만들기 #todo\n",
    "        \n",
    "        visibile = kps[:, :, 2]\n",
    "        has_kp = tf.reduce_any(visibile > 0, 1, True)\n",
    "        has_kp = tf.cast(has_kp, tf.float32)\n",
    "        z = tf.zeros_like(boxes[:, 0])\n",
    "        boxes_dummy = tf.stack((z+1, z+1, z+3, z+3), -1)\n",
    "        box_margin = 5\n",
    "        boxes_wide = tf.concat((boxes[:, 0:2], boxes[:, 2:4] + box_margin), -1)\n",
    "        boxes = has_kp * boxes_dummy + (1 - has_kp) * boxes_wide\n",
    "        \n",
    "        img_wf = tf.cast(img_w, tf.float32)\n",
    "        img_hf = tf.cast(img_h, tf.float32)\n",
    "        boxes_norm_xywh = tf.stack((boxes[:, 0]/img_wf, boxes[:, 1]/img_hf, boxes[:, 2]/img_wf, boxes[:, 3]/img_hf), -1)\n",
    "        \n",
    "        sx = 1.0 / boxes_norm_xywh[:, 2]\n",
    "        sy = 1.0 / boxes_norm_xywh[:, 3]        \n",
    "        tx = (0.5 - boxes_norm_xywh[:, 0]) * 2 # +:left, -:right\n",
    "        ty = (0.5 - boxes_norm_xywh[:, 1]) * 2# +:top, -:down      \n",
    "        \n",
    "        theta_scale = tf.stack((sx, z, z, z, sy, z, z, z, z+1), -1)\n",
    "        theta_scale = tf.reshape(theta_scale, [-1, 3, 3])         \n",
    "        theta_translate = tf.stack((z+1, z, tx, z, z+1, ty, z, z, z+1), -1)\n",
    "        theta_translate = tf.reshape(theta_translate, [-1, 3, 3])         \n",
    "        m = tf.shape(boxes)[0]\n",
    "        \n",
    "        h = img_h\n",
    "        net = tf.ones((m, h, h, 1), tf.float32)        \n",
    "        net_transformed = sampling(net, theta_scale, h, h)\n",
    "        net_transformed = sampling(net_transformed, theta_translate, h, h)        \n",
    "        net_transformed = tf.squeeze(net_transformed, -1)\n",
    "        out = tf.reduce_max(net_transformed, 0)        \n",
    "        return out\n",
    "    \n",
    "    def encode_batch(self, batch_images, gt_boxes, cls_ids, keypoints):\n",
    "        \"\"\"Creates box and classification targets for a batch\"\"\"\n",
    "        #keypoints(batch_m, n, 17, 3)\n",
    "        images_shape = tf.shape(batch_images)\n",
    "        batch_size = images_shape[0]\n",
    "        \n",
    "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
    "        labels_heatmap = tf.TensorArray(dtype=tf.int32, size=batch_size, dynamic_size=True)\n",
    "        labels_heatmap_box = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
    "        label_displace = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i], keypoints[i])\n",
    "            labels = labels.write(i, label)\n",
    "            label_heat = self._encode_keypoint_heatmap(images_shape[1], images_shape[2], keypoints[i])        \n",
    "            labels_heatmap = labels_heatmap.write(i, label_heat)\n",
    "            \n",
    "            heat_box = self.convert_box_to_map(gt_boxes[i], keypoints[i], images_shape[1], images_shape[2])\n",
    "            labels_heatmap_box = labels_heatmap_box.write(i, heat_box)\n",
    "            \n",
    "            _displace = generate_displace_map(gt_boxes[i], keypoints[i], images_shape[1], images_shape[2])\n",
    "            label_displace = label_displace.write(i, _displace)\n",
    "        \n",
    "        batch_images = tf.cast(batch_images, tf.float32)\n",
    "        labels = labels.stack()\n",
    "        labels_heatmap = labels_heatmap.stack()\n",
    "        if True:\n",
    "            labels_no_kp_mask = labels_heatmap_box.stack() # test\n",
    "            labels_no_kp_mask = tf.cast(labels_no_kp_mask, tf.int32)\n",
    "            labels_no_kp_mask = tf.expand_dims(labels_no_kp_mask, -1)\n",
    "            labels_heatmap = (1 - labels_no_kp_mask) * labels_heatmap + labels_no_kp_mask * -1\n",
    "        \n",
    "        label_displace = label_displace.stack()\n",
    "        labels_heatmap_exp = tf.cast(labels_heatmap, tf.float32)\n",
    "        heatmap_dispalce = tf.concat((labels_heatmap_exp, label_displace), -1)                \n",
    "        \n",
    "        gt = {\"detect\": labels, \"heatmap\": heatmap_dispalce, 'heatmap_coord':labels}#dual\n",
    "        return batch_images, gt\n",
    "     \n",
    "    def encode_batch_train(self, batch_images, gt_boxes, cls_ids, keypoints):\n",
    "        \n",
    "        batch_images = image_color_augment(batch_images)\n",
    "        \n",
    "        return self.encode_batch(batch_images, gt_boxes, cls_ids, keypoints)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_start, 2**level_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "anchor_k = len(label_encoder._anchor_box.aspect_ratios)*len(label_encoder._anchor_box.scales)\n",
    "anchor_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "#strategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1#min(2 * strategy.num_replicas_in_sync, len(list_x_train))\n",
    "autotune = tf.data.experimental.AUTOTUNE\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_dataset = dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
    "print('check_dataset', check_dataset)\n",
    "i = 0\n",
    "for image, bbox_unnorm, cls, keypoints_unnorm in check_dataset:\n",
    "    i += 1\n",
    "    print(image.shape, bbox_unnorm.shape, cls.shape, keypoints_unnorm.shape)\n",
    "    print('bbox_unnorm', bbox_unnorm)\n",
    "    print('cls', cls)\n",
    "    print('keypoints_unnorm', keypoints_unnorm.shape)    \n",
    "    if i>0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
    "#train_dataset = train_dataset.shuffle(8 * batch_size)\n",
    "#train_dataset = train_dataset.padded_batch(batch_size=batch_size, padding_values=(0.0, 1e-8, -1), drop_remainder=False)\n",
    "train_dataset = train_dataset.padded_batch(batch_size=batch_size)\n",
    "train_dataset = train_dataset.map(label_encoder.encode_batch_train, num_parallel_calls=autotune)\n",
    "train_dataset = train_dataset.prefetch(autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_gt_heatmap_displace(gt):    \n",
    "    heatmap = gt[:, :, :, 0]\n",
    "    heatmap_visible = gt[:, :, :, 1] #{-1:unknown, 0:not visible, 1:occluded, 2:visible}\n",
    "    displace = gt[:, :, :, 2:]\n",
    "    return heatmap, heatmap_visible, displace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heatmap_displace(heatmap):\n",
    "    k = NUM_KEYPOINT + 1\n",
    "    heatmap_score = heatmap[:, :, :, :k]\n",
    "    visible_score = heatmap[:, :, :, k:k+3]\n",
    "    displace = heatmap[:, :, :, -4:]\n",
    "    \n",
    "    return heatmap_score, visible_score, displace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for x, multi_y in train_dataset:\n",
    "    i += 1\n",
    "    #print('multi_y', type(multi_y), multi_y.keys())\n",
    "    y = multi_y['detect']\n",
    "    y_heatmap_displace = multi_y['heatmap']    \n",
    "    y_heatmap, heatmap_visible, y_distance = split_gt_heatmap_displace(y_heatmap_displace)\n",
    "    print('y_distance', y_distance.shape, tf.reduce_sum(tf.abs(y_distance)))#(2, 512, 512, 2)\n",
    "    print('heatmap_visible', heatmap_visible.shape, np.unique(np.array(heatmap_visible)))\n",
    "    \n",
    "    #print('y_heatmap', tf.shape(y_heatmap), tf.reduce_max(y_heatmap), tf.reduce_max(y_distance))\n",
    "    #print('y_heatmap', tf.reduce_mean(y_heatmap))\n",
    "    \n",
    "    img_map_xy = tf.cast(y_heatmap[0], tf.uint8)\n",
    "    alpha = 255//(NUM_KEYPOINT+1)\n",
    "    hitmap_h = img_map_xy * alpha\n",
    "\n",
    "    ignore_heatmap = y_heatmap[0] < 0\n",
    "        \n",
    "    hitmap_img = tf.stack((hitmap_h, hitmap_h, hitmap_h), -1)\n",
    "    distance_img = tf.abs(y_distance[0,:,:,0])*100\n",
    "    distance_global_img = tf.abs(y_distance[0,:,:,-1])*100\n",
    "    fig, ax = plt.subplots(1, 5)    \n",
    "    ax[0].imshow(tf.cast(x[0], tf.uint8))\n",
    "    ax[1].imshow(hitmap_img, cmap='gray')\n",
    "    ax[2].imshow(tf.cast(distance_img,tf.uint8), cmap='gray')\n",
    "    ax[3].imshow(tf.cast(distance_global_img,tf.uint8), cmap='gray')\n",
    "    ax[4].imshow(heatmap_visible[0], cmap='gray')\n",
    "    if i > 2:break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img = np.stack((y_heatmap[0,:,:], y_distance[0,:,:,1]*2, x[0,:,:,0]/355.0), -1)\n",
    "plt.imshow(show_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(y_distance[0,:,:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(heatmap_visible[0])+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(heatmap_visible[0])+1, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_heatmap_displace[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = dataset_test.map(preprocess_data, num_parallel_calls=autotune)\n",
    "#val_dataset = val_dataset.padded_batch(batch_size=batch_size, padding_values=(0.0, 1e-8, -1), drop_remainder=False)\n",
    "val_dataset = val_dataset.padded_batch(batch_size=1)\n",
    "val_dataset = val_dataset.map(label_encoder.encode_batch, num_parallel_calls=autotune)\n",
    "val_dataset = val_dataset.prefetch(autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_gt(y_true):\n",
    "    #targets = tf.concat([box_target, cls_target, keypoint_xy_target, keypoint_cls_target], axis=-1)\n",
    "    y_box = y_true[:, :, :4]\n",
    "    y_cls = y_true[:, :, 4]\n",
    "    y_keypoint = y_true[:, :, 5:]\n",
    "    return y_box, y_cls, y_keypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filters = 4 + 1 + num_classes + num_keypoints * (2 + num_keypoint_classes)       \n",
    "def split_hyperthesis(y_pred):\n",
    "    h_box = y_pred[:, :, :4]\n",
    "    h_obj = y_pred[:, :, 4]\n",
    "    h_cls = y_pred[:, :, 5:5+num_classes]        \n",
    "    h_keypoint = y_pred[:, :, 5+num_classes:]       \n",
    "    \n",
    "    h_obj = tf.nn.sigmoid(h_obj)\n",
    "    return h_box, h_obj, h_cls, h_keypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_gt_keypoint(y_keypoint):\n",
    "    xy = y_keypoint[:, :, :NUM_KEYPOINT*2]\n",
    "    cls = y_keypoint[:, :, -NUM_KEYPOINT:]\n",
    "        \n",
    "    x = xy[:, :, 0::2]\n",
    "    y = xy[:, :, 1::2]    \n",
    "    return x, y, cls    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_h_keypoint(h_keypoint):    \n",
    "    h_x = h_keypoint[:, :, :NUM_KEYPOINT]\n",
    "    h_y = h_keypoint[:, :, NUM_KEYPOINT:NUM_KEYPOINT * 2]\n",
    "    h_cls_score = h_keypoint[:, :, NUM_KEYPOINT * 2:]\n",
    "    \n",
    "    #h_x = tf.tanh(h_x)\n",
    "    #h_y = tf.tanh(h_y)\n",
    "    \n",
    "    keypoints_cls = tf.round(tf.sigmoid(h_cls_score) * 2)\n",
    "    return h_x, h_y, h_cls_score, keypoints_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decode_box_predictions(anchor_boxes, box_predictions):        \n",
    "    boxes = box_predictions * label_encoder._box_variance\n",
    "    boxes = tf.concat(\n",
    "        [\n",
    "            boxes[:, :, :2] * anchor_boxes[:, :, 2:] + anchor_boxes[:, :, :2],\n",
    "            tf.math.exp(boxes[:, :, 2:]) * anchor_boxes[:, :, 2:],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    boxes_transformed = convert_to_corners(boxes)\n",
    "    return boxes_transformed\n",
    "\n",
    "def _decode_keypoint_predictions(anchor_boxes, keypoint_predictions):        \n",
    "    repeat = tf.constant([NUM_KEYPOINT], tf.int32)\n",
    "    box_variance_rep = tf.tile(label_encoder._box_variance[:2], repeat)\n",
    "    keypoints = keypoint_predictions * box_variance_rep\n",
    "    \n",
    "    batch_m = tf.shape(keypoints)[0]\n",
    "    repeat = tf.constant((1, 1, NUM_KEYPOINT))\n",
    "    \n",
    "    anchor_xy_tile = tf.tile(anchor_boxes[:, :, :2], repeat)\n",
    "    anchor_wh_tile = tf.tile(anchor_boxes[:, :, 2:], repeat)\n",
    "    \n",
    "    keypoint = keypoints * anchor_wh_tile + anchor_xy_tile\n",
    "    return keypoint\n",
    "\n",
    "\n",
    "def decode_debug(images, predictions, \n",
    "                      num_classes=num_classes,\n",
    "                      confidence_threshold=0.5,\n",
    "                      nms_iou_threshold=0.2,\n",
    "                      max_detections_per_class=1000,\n",
    "                      max_detections=1500,\n",
    "                      box_variance=[0.1, 0.1, 0.2, 0.2]):\n",
    "    \n",
    "    _anchor_box = AnchorBox()    \n",
    "        \n",
    "    image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n",
    "    #anchor_boxes = _anchor_box.get_anchors(image_shape[1], image_shape[2])#free size        \n",
    "    image_h = padded_image_shape[0]\n",
    "    image_w = padded_image_shape[1]\n",
    "    anchor_boxes = _anchor_box.get_anchors(image_h, image_w)\n",
    "    \n",
    "    box_predictions, objectness, keypoint_predictions = split_gt(predictions)\n",
    "    x, y, keypoints_cls = split_gt_keypoint(keypoint_predictions)\n",
    "    \n",
    "    m = tf.shape(images)[0]\n",
    "    keypoints_xy = tf.stack((x, y), -1)\n",
    "    keypoints_xy = tf.reshape(keypoints_xy, (m, -1, NUM_KEYPOINT * 2))\n",
    "    \n",
    "    cls = objectness * 0 + 1\n",
    "            \n",
    "    boxes = _decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n",
    "    keypoints_xy = _decode_keypoint_predictions(anchor_boxes[None, ...], keypoints_xy)\n",
    "    \n",
    "    boxes_2d = tf.reshape(boxes, [-1, 4])    \n",
    "    scores = tf.reshape(objectness, [-1, 1])\n",
    "    \n",
    "    cls = tf.reshape(cls, [-1, 1])\n",
    "    \n",
    "    keypoints_xy_3d = tf.reshape(keypoints_xy, [-1, NUM_KEYPOINT, 2])\n",
    "    keypoints_cls_3d = tf.reshape(keypoints_cls, [-1, NUM_KEYPOINT, 1])    \n",
    "    keypoints_3d = tf.concat((keypoints_xy_3d, keypoints_cls_3d), -1)\n",
    "    keypoints_2d = tf.reshape(keypoints_3d, [-1, NUM_KEYPOINT * 3])\n",
    "    \n",
    "    ccbox = tf.concat((cls, scores, boxes_2d, keypoints_2d), -1)\n",
    "    ccbox_check = tf.concat((boxes_2d, cls, keypoints_2d), -1)\n",
    "    \n",
    "    selected_indices, selected_scores = tf.image.non_max_suppression_with_scores(    \n",
    "        ccbox[:, 2:2+4],\n",
    "        ccbox[:, 1],        \n",
    "        max_detections,\n",
    "        nms_iou_threshold,\n",
    "        confidence_threshold,        \n",
    "    )\n",
    "    output = tf.gather(ccbox_check, selected_indices)        \n",
    "    return output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodePredictions(images, predictions, \n",
    "                      num_classes=num_classes,\n",
    "                      confidence_threshold=0.5,\n",
    "                      nms_iou_threshold=0.2,\n",
    "                      max_detections_per_class=100,\n",
    "                      max_detections=150,\n",
    "                      box_variance=[0.1, 0.1, 0.2, 0.2]):\n",
    "    \n",
    "    _anchor_box = AnchorBox()    \n",
    "        \n",
    "    image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n",
    "    anchor_boxes = _anchor_box.get_anchors(image_shape[1], image_shape[2])#free size        \n",
    "    #image_h = padded_image_shape[0]\n",
    "    #image_w = padded_image_shape[1]\n",
    "    #anchor_boxes = _anchor_box.get_anchors(image_h, image_w)\n",
    "   \n",
    "    h_box, objectness, h_cls, h_keypoint = split_hyperthesis(predictions)    \n",
    "    #h_x, h_y, h_cls_score, keypoints_cls = split_h_keypoint(h_keypoint)\n",
    "    \n",
    "    keypoints_xy = tf.stack((h_x, h_y), -1) #keypoint_predictions[:, :, :NUM_KEYPOINT * 2]\n",
    "    m = tf.shape(images)[0]\n",
    "    \n",
    "    keypoints_xy = tf.reshape(keypoints_xy, [m, -1, NUM_KEYPOINT * 2])\n",
    "    \n",
    "    cls_score = predictions[:, :, 5:5+num_classes]    \n",
    "    cls_prob = tf.nn.softmax(cls_score)\n",
    "    cls_prob_max = tf.reduce_max(cls_prob, -1)\n",
    "    cls = tf.argmax(cls_score, -1)\n",
    "    cls = tf.cast(cls, tf.float32)\n",
    "    cls = predictions[:, :, 4]\n",
    "            \n",
    "    boxes = _decode_box_predictions(anchor_boxes[None, ...], h_box)\n",
    "    keypoints_xy = _decode_keypoint_predictions(anchor_boxes[None, ...], keypoints_xy)\n",
    "    \n",
    "    boxes_2d = tf.reshape(boxes, [-1, 4])    \n",
    "    scores = tf.reshape(objectness, [-1, 1])\n",
    "    \n",
    "    cls = tf.reshape(cls, [-1, 1])\n",
    "    \n",
    "    keypoints_xy_3d = tf.reshape(keypoints_xy, [-1, NUM_KEYPOINT, 2])\n",
    "    keypoints_cls_3d = tf.reshape(keypoints_cls, [-1, NUM_KEYPOINT, 1])    \n",
    "    keypoints_3d = tf.concat((keypoints_xy_3d, keypoints_cls_3d), -1)\n",
    "    keypoints_2d = tf.reshape(keypoints_3d, [-1, NUM_KEYPOINT * 3])\n",
    "    \n",
    "    ccbox = tf.concat((cls, scores, boxes_2d, keypoints_2d), -1)\n",
    "    ccbox_check = tf.concat((boxes_2d, cls, keypoints_2d), -1)\n",
    "    \n",
    "    selected_indices, selected_scores = tf.image.non_max_suppression_with_scores(    \n",
    "        ccbox[:, 2:2+4],\n",
    "        ccbox[:, 1],        \n",
    "        max_detections,\n",
    "        nms_iou_threshold,\n",
    "        confidence_threshold,        \n",
    "    )\n",
    "    output = tf.gather(ccbox_check, selected_indices)        \n",
    "    return output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_check = AnchorBox()\n",
    "anchors = anchor_check.get_anchors_check(512,512)\n",
    "anchor_sum = 0\n",
    "for anchor in anchors:\n",
    "    print(anchor.shape, anchor[-1], 'sqrt', np.sqrt(anchor.shape[0]/anchor_k))\n",
    "    anchor_sum +=anchor.shape[0]\n",
    "print('anchor_sum', anchor_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, multi_y in val_dataset:\n",
    "    target = multi_y['detect']\n",
    "    heatmap = multi_y['heatmap']\n",
    "    print('image', image.shape)    \n",
    "    print('target', target.shape)# (1, 12096, 56)    \n",
    "    detections = decode_debug(image, target, confidence_threshold=0.15, nms_iou_threshold=0.2)\n",
    "    print('detections', detections.shape)\n",
    "    print(detections[0])\n",
    "    target_cp = target[:, :, 4+1:]\n",
    "    target_cp_xy = target_cp[:, :, :NUM_KEYPOINT*2]\n",
    "    print('target_cp', target_cp.shape)\n",
    "    #max tf.Tensor(187.73685, shape=(), dtype=float32) tf.Tensor(-219.20311, shape=(), dtype=float32)\n",
    "    print('max', tf.reduce_max(target_cp_xy), tf.reduce_min(target_cp_xy))#\n",
    "    break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(list_y_train[0][0].shape)\n",
    "print(list_y_train[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인코딩 라벨 어디감?\n",
    "- 65번 가운데 여자(iou thresh 높이면 나옴)\n",
    "- 73번 가운데 남자\n",
    "- 76번 할아버지(label positive 낮추면 나옴)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for image, multi_y in val_dataset:\n",
    "    i += 1\n",
    "    target = multi_y['detect']\n",
    "    heatmap_dis = multi_y['heatmap']        \n",
    "    detections = decode_debug(image, target, confidence_threshold=0.25, nms_iou_threshold=0.9)\n",
    "    print(i, 'detections', detections.shape, 'target', target.shape)    \n",
    "    kp = detections[:, -NUM_KEYPOINT * NUM_KEYPOINT_CH:]\n",
    "    kp_cls = kp[:, 2::3]\n",
    "    print(detections.shape) #양복 2명 할아버지 (7, 56)\n",
    "    print(detections[:, :5])\n",
    "    x = np.array(image[0], np.uint8)\n",
    "    h = np.array(detections)\n",
    "    \n",
    "    img = Image.fromarray(x)        \n",
    "    draw_box_keypoint(img, h, is_show_class=False)\n",
    "    display(img)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_start, level_end, 2**level_start, 2**level_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4 * 2**(level_end-1), 3 * 2**(level_end-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netbase = keras.applications.EfficientNetB4(include_top=False, input_shape=[384, 512, 3])\n",
    "netbase.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backbone():\n",
    "    backbone = keras.applications.EfficientNetB2(include_top=False, input_shape=[None, None, 3])\n",
    "    c2_output, c3_output, c4_output, c5_output = [\n",
    "        backbone.get_layer(layer_name).output\n",
    "        for layer_name in [\"block2c_add\", \"block3c_add\", \"block5d_add\", \"block6d_add\"]]#block5c_add, block6d_add    \n",
    "    return keras.Model(\n",
    "        inputs=[backbone.inputs], outputs=[c2_output, c3_output, c4_output, c5_output]\n",
    "    )\n",
    "\n",
    "#D0 for layer_name in [\"block2b_add\", \"block3b_add\", \"block5c_add\", \"block6d_add\"]]\n",
    "#B2 for layer_name in [\"block2c_add\", \"block3c_add\", \"block5d_add\", \"top_activation\"]]\n",
    "#B3 for layer_name in [\"block2c_add\", \"block3c_add\", \"block5e_add\", \"top_activation\"]]\n",
    "#B4 for layer_name in [\"block2d_add\", \"block3d_add\", \"block5f_add\", \"block6h_add\"]]\n",
    "#D7 for layer_name in [\"block2f_add\", \"block3g_add\", \"block5j_add\", \"block6d_add\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_scale(a,b,c):\n",
    "    a0 = tf.exp(a)\n",
    "    a1 = tf.exp(b)\n",
    "    a2 = tf.exp(c)\n",
    "    a_sum = a0 + a1 + a2 + 1e-5\n",
    "    a0 = a0 / a_sum\n",
    "    a1 = a1 / a_sum\n",
    "    a2 = a2 / a_sum\n",
    "    return a0, a1, a2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_scale_from_last_ch(A,B,C, index=-1):\n",
    "    a = A[:, :, :, index:index+1]\n",
    "    b = B[:, :, :, index:index+1]\n",
    "    c = C[:, :, :, index:index+1]\n",
    "    a0 = tf.exp(a)\n",
    "    a1 = tf.exp(b)\n",
    "    a2 = tf.exp(c)\n",
    "    a_sum = a0 + a1 + a2\n",
    "    a0 = a0 / a_sum\n",
    "    a1 = a1 / a_sum\n",
    "    a2 = a2 / a_sum\n",
    "    return a0, a1, a2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bifeature(c345, filters):    \n",
    "    a2 = c345[0]\n",
    "    a3 = c345[1]\n",
    "    a4 = c345[2]\n",
    "    a5 = c345[3]\n",
    "    \n",
    "    regulizer  = tf.keras.regularizers.L2(l1)\n",
    "    dil = 2\n",
    "    group = 2\n",
    "    a2_0 = Conv2D(filters, 3, 1, \"same\", groups=group, activation=activation, dilation_rate=dil, kernel_regularizer=regulizer)(a2)    \n",
    "    a33 = Conv2D(filters*2, 3, 1, \"same\", groups=group, activation=activation, dilation_rate=dil, kernel_regularizer=regulizer)(a3)\n",
    "    a44 = Conv2D(filters*2, 3, 1, \"same\", groups=group, activation=activation, dilation_rate=dil, kernel_regularizer=regulizer)(a4)\n",
    "    a55 = Conv2D(filters*2, 3, 1, \"same\", groups=group, activation=activation, dilation_rate=dil, kernel_regularizer=regulizer)(a5)\n",
    "    a66 = Conv2D(filters*2, 3, 2, \"same\", groups=group, activation=activation, kernel_regularizer=regulizer)(a5)\n",
    "    \n",
    "    a3_0, a3_1 = tf.split(a33, 2, -1)\n",
    "    a4_0, a4_1 = tf.split(a44, 2, -1)\n",
    "    a5_0, a5_1 = tf.split(a55, 2, -1)\n",
    "    a6_0, a6_1 = tf.split(a66, 2, -1)\n",
    "    \n",
    "    a7_1 = Conv2D(filters, 3, 2, \"same\", groups=group, activation=activation, kernel_regularizer=regulizer)(a6_0)     \n",
    "    b7 = Conv2D(filters, 3, 2, \"same\", groups=group, activation=activation, kernel_regularizer=regulizer)(a6_0)    \n",
    "        \n",
    "    a6_up = keras.layers.UpSampling2D(2)(a6_1)    \n",
    "    b5 = keras.layers.Add()([a5_0, a6_up])  \n",
    "        \n",
    "    a5_up = keras.layers.UpSampling2D(2)(a5_1)    \n",
    "    b4 = keras.layers.Add()([a4_0, a5_up])  \n",
    "    \n",
    "    b4_up = keras.layers.UpSampling2D(2)(b4)\n",
    "    b3 = keras.layers.Add()([a3_0, b4_up])  \n",
    "    \n",
    "    b3_up = keras.layers.UpSampling2D(2)(b3)\n",
    "    b2 = keras.layers.Add()([a2_0, b3_up])\n",
    "    \n",
    "    b2_down = Conv2D(filters, 3, 2, \"same\", groups=group, activation=activation, kernel_regularizer=regulizer)(b2)\n",
    "    b3_1 = Conv2D(filters, 3, 1, \"same\", groups=1, activation=activation, dilation_rate=dil, kernel_regularizer=regulizer)(b3)    \n",
    "    c3 = keras.layers.Add()([a3_1, b3_1, b2_down])    \n",
    "    \n",
    "    c3_down = Conv2D(filters, 3, 2, \"same\", groups=group, activation=activation, kernel_regularizer=regulizer)(c3)\n",
    "    b4_1 = Conv2D(filters, 3, 1, \"same\", groups=1, activation=activation, dilation_rate=dil, kernel_regularizer=regulizer)(b4)    \n",
    "    c4 = keras.layers.Add()([a4_1, b4_1, c3_down])    \n",
    "    \n",
    "    c4_down = Conv2D(filters, 3, 2, \"same\", groups=group, activation=activation, kernel_regularizer=regulizer)(c4)\n",
    "    b5_1 = Conv2D(filters, 3, 1, \"same\", groups=1, activation=activation, dilation_rate=dil, kernel_regularizer=regulizer)(b5)    \n",
    "    c5 = keras.layers.Add()([a5_1, b5_1, c4_down])    \n",
    "    \n",
    "    return b2, c3, c4, c5    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BifeaturePyramidNet_dilate(c345, filters):    \n",
    "    a2 = c345[0]\n",
    "    a3 = c345[1]\n",
    "    a4 = c345[2]\n",
    "    a5 = c345[3]\n",
    "    \n",
    "    regulizer  = tf.keras.regularizers.L2(l1)\n",
    "    dil = 2\n",
    "    group = 2\n",
    "    a2_0 = Conv2D(filters, 3, 1, \"same\", groups=group, activation=activation, dilation_rate=dil, kernel_regularizer=regulizer)(a2)    \n",
    "    a33 = Conv2D(filters*2, 3, 1, \"same\", groups=group, activation=activation, dilation_rate=dil, kernel_regularizer=regulizer)(a3)\n",
    "    a44 = Conv2D(filters*2, 3, 1, \"same\", groups=group, activation=activation, dilation_rate=dil, kernel_regularizer=regulizer)(a4)\n",
    "    a55 = Conv2D(filters*2, 3, 1, \"same\", groups=group, activation=activation, dilation_rate=dil, kernel_regularizer=regulizer)(a5)\n",
    "    a66 = Conv2D(filters*2, 3, 2, \"same\", groups=group, activation=activation, kernel_regularizer=regulizer)(a5)\n",
    "    \n",
    "    a3_0, a3_1 = tf.split(a33, 2, -1)\n",
    "    a4_0, a4_1 = tf.split(a44, 2, -1)\n",
    "    a5_0, a5_1 = tf.split(a55, 2, -1)\n",
    "    a6_0, a6_1 = tf.split(a66, 2, -1)\n",
    "    \n",
    "    a7_1 = Conv2D(filters, 3, 2, \"same\", groups=group, activation=activation, kernel_regularizer=regulizer)(a6_0)     \n",
    "    b7 = Conv2D(filters, 3, 2, \"same\", groups=group, activation=activation, kernel_regularizer=regulizer)(a6_0)    \n",
    "    b6 = a6_0\n",
    "    \n",
    "    a6_up = keras.layers.UpSampling2D(2)(a6_1)    \n",
    "    b5 = keras.layers.Add()([a5_0, a6_up])  \n",
    "        \n",
    "    a5_up = keras.layers.UpSampling2D(2)(a5_1)    \n",
    "    b4 = keras.layers.Add()([a4_0, a5_up])  \n",
    "    \n",
    "    b4_up = keras.layers.UpSampling2D(2)(b4)\n",
    "    b3 = keras.layers.Add()([a3_0, b4_up])  \n",
    "    \n",
    "    b3_up = keras.layers.UpSampling2D(2)(b3)\n",
    "    b2 = keras.layers.Add()([a2_0, b3_up])\n",
    "    \n",
    "    b2_down = Conv2D(filters, 3, 2, \"same\", groups=group, activation=activation, kernel_regularizer=regulizer)(b2)\n",
    "    b3_1 = Conv2D(filters, 3, 1, \"same\", groups=1, activation=activation, dilation_rate=dil, kernel_regularizer=regulizer)(b3)    \n",
    "    c3 = keras.layers.Add()([a3_1, b3_1, b2_down])    \n",
    "    \n",
    "    c3_down = Conv2D(filters, 3, 2, \"same\", groups=group, activation=activation, kernel_regularizer=regulizer)(c3)\n",
    "    b4_1 = Conv2D(filters, 3, 1, \"same\", groups=1, activation=activation, dilation_rate=dil, kernel_regularizer=regulizer)(b4)    \n",
    "    c4 = keras.layers.Add()([a4_1, b4_1, c3_down])    \n",
    "    \n",
    "    c4_down = Conv2D(filters, 3, 2, \"same\", groups=group, activation=activation, kernel_regularizer=regulizer)(c4)\n",
    "    b5_1 = Conv2D(filters, 3, 1, \"same\", groups=1, activation=activation, dilation_rate=dil, kernel_regularizer=regulizer)(b5)    \n",
    "    c5 = keras.layers.Add()([a5_1, b5_1, c4_down])    \n",
    "    \n",
    "    c5_down = Conv2D(filters, 3, 2, \"same\", groups=group, activation=activation, kernel_regularizer=regulizer)(c5)\n",
    "    b6_1 = Conv2D(filters, 3, 1, \"same\", groups=1, activation=activation, dilation_rate=dil, kernel_regularizer=regulizer)(b6)    \n",
    "    c6 = keras.layers.Add()([a6_1, b6_1, c5_down])\n",
    "    \n",
    "    c6_down = Conv2D(filters, 3, 2, \"same\", groups=group, activation=activation, kernel_regularizer=regulizer)(c6)\n",
    "    b7_1 = Conv2D(filters, 3, 1, \"same\", groups=1, activation=activation, dilation_rate=dil, kernel_regularizer=regulizer)(b7)\n",
    "    c7 = keras.layers.Add()([a7_1, b7_1, c6_down])\n",
    "    return a2 + b2, a3 + c3, a4 + c4, a5 + c5, c6, c7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**level_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heatmap(nets, img_h, img_w, ch, alphas):\n",
    "    #p234567\n",
    "    is_shuffle_pixel = True\n",
    "    ch = ch + 1 + 2 + 2 #(alpha, displace_rel, displace_global)\n",
    "    if is_shuffle_pixel:\n",
    "        ch *= 2\n",
    "    heatmap_0 = nets[0][:, :, :, :ch]\n",
    "    heatmap_1 = nets[1][:, :, :, :ch]\n",
    "    heatmap_2 = nets[2][:, :, :, :ch]\n",
    "    heatmap_3 = nets[3][:, :, :, :ch]\n",
    "    \n",
    "    if True:\n",
    "        visible_0 = nets[0][:, :, :, ch+1:ch+3]\n",
    "        visible_1 = nets[1][:, :, :, ch+1:ch+3]\n",
    "        visible_2 = nets[2][:, :, :, ch+1:ch+3]\n",
    "        visible_3 = nets[3][:, :, :, ch+1:ch+3]\n",
    "        visible_0 = tf.image.resize(visible_0, [img_h, img_w])\n",
    "        visible_1 = tf.image.resize(visible_1, [img_h, img_w])\n",
    "        visible_2 = tf.image.resize(visible_2, [img_h, img_w])\n",
    "        visible_3 = tf.image.resize(visible_3, [img_h, img_w])\n",
    "        visible = (visible_0 + visible_1 + visible_2 + visible_3)/4\n",
    "    \n",
    "    if is_shuffle_pixel:\n",
    "        heatmap_0 = shuffle_pixel_lr(heatmap_0, img_h//4, img_w//4, ch)        \n",
    "        heatmap_1 = shuffle_pixel_lr(heatmap_1, img_h//8, img_w//8, ch)\n",
    "        heatmap_2 = shuffle_pixel_lr(heatmap_2, img_h//16, img_w//16, ch)\n",
    "        heatmap_3 = shuffle_pixel_lr(heatmap_3, img_h//32, img_w//32, ch)\n",
    "    \n",
    "    heatmap_fullsize_0 = tf.image.resize(heatmap_0, [img_h, img_w])\n",
    "    heatmap_fullsize_1 = tf.image.resize(heatmap_1, [img_h, img_w])\n",
    "    heatmap_fullsize_2 = tf.image.resize(heatmap_2, [img_h, img_w])\n",
    "    heatmap_fullsize_3 = tf.image.resize(heatmap_3, [img_h, img_w])\n",
    "  \n",
    "    a0, a1, a2 = softmax_scale_from_last_ch(heatmap_fullsize_0, heatmap_fullsize_1, heatmap_fullsize_2, -5)                \n",
    "    heatmap = a0 * heatmap_fullsize_0 + a1 * heatmap_fullsize_1 + a2 * heatmap_fullsize_2 + .2 * heatmap_fullsize_3\n",
    "    #heatmap = heatmap_fullsize_0 + heatmap_fullsize_1 + heatmap_fullsize_2 + heatmap_fullsize_3\n",
    "        \n",
    "    heatmap_kp = heatmap[:, :, :, :-5]\n",
    "    displace_kp = tf.nn.sigmoid(heatmap[:, :, :, -4:], name='displace') - 0.5            \n",
    "    visible = tf.concat((heatmap_kp[:, :, :, :1], visible), -1)\n",
    "            \n",
    "    heatmap_displace = tf.concat((heatmap_kp, visible, displace_kp), -1)        \n",
    "    return heatmap_displace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_position_encoding(nets):\n",
    "    list_net = []\n",
    "    for net in nets:\n",
    "        #net_h = tf.shape(net)[1]\n",
    "        #net_w = tf.shape(net)[2]\n",
    "        #coord_xy = coordinate_map_norm(net_h, net_w)#(h, w, 2)\n",
    "        net_emb = add_map(net)\n",
    "        #net_emb = tf.concat((net[:, :, :, :2] + tf.expand_dims(coord_xy), net[:, :, :, 2:]), -1)\n",
    "        list_net.append(net_emb)\n",
    "    return list_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNet(num_classes, num_keypoints, num_keypoint_classes, anchor_k, is_train=False, is_backbone_train=True):    \n",
    "    inputs = Input(shape=(None, None, 3))            \n",
    "    prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
    "    kernel_init = tf.initializers.he_normal()\n",
    "    \n",
    "    backbone = get_backbone()\n",
    "    backbone.trainable = is_backbone_train\n",
    "    nets_3 = backbone(inputs, training=is_train)\n",
    "    #nets_3 = add_position_encoding(nets_3)\n",
    "    ch = 256\n",
    "    p34567 = Bifeature(nets_3, ch)\n",
    "    p234567 = BifeaturePyramidNet_dilate(p34567, ch)\n",
    "    #p34567 = BifeaturePyramidNet(nets_3, ch)\n",
    "    img_h = tf.shape(inputs)[1]\n",
    "    img_w = tf.shape(inputs)[2]\n",
    "    \n",
    "    cls_outputs = []\n",
    "    box_outputs = []\n",
    "    \n",
    "    kernel_init = tf.initializers.he_normal()\n",
    "    regulizer = tf.keras.regularizers.L2(l1)\n",
    "    \n",
    "    filters = 4 + 1 + num_classes# + num_keypoints * (2 + num_keypoint_classes)\n",
    "    print('filters', filters)\n",
    "    \n",
    "    #conv_inter = keras.layers.Conv2D(ch, 3, activation=activation, padding=\"same\", kernel_regularizer=regulizer, name='head_inter_0')\n",
    "    conv_h0 = keras.layers.Conv2D(anchor_k * filters, 3, padding=\"same\", name='head_0', kernel_initializer=kernel_init)   \n",
    "    conv_h1 = keras.layers.Conv2D(anchor_k * filters, 3, padding=\"same\", name='head_1', kernel_initializer=kernel_init)   \n",
    "    conv_h2 = keras.layers.Conv2D(anchor_k * filters, 3, padding=\"same\", name='head_2', kernel_initializer=kernel_init)\n",
    "    conv_h3 = keras.layers.Conv2D(anchor_k * filters, 3, padding=\"same\", name='head_3', kernel_initializer=kernel_init)\n",
    "    \n",
    "    conv_kernels = [conv_h0, conv_h0, conv_h0, conv_h1, conv_h2, conv_h3]\n",
    "    \n",
    "    drop = keras.layers.Dropout(0.1)\n",
    "    N = tf.shape(nets_3[0])[0]\n",
    "    \n",
    "    cbox_outputs = []    \n",
    "    alphas = []\n",
    "    for i in range(2, len(p234567)):            \n",
    "        feature = p234567[i]\n",
    "        feature = tf.concat((feature[:,:,:,:60], drop(feature[:,:,:,60:])), -1)\n",
    "        conv_kernel = conv_kernels[i]\n",
    "        net_out = conv_kernel(feature)\n",
    "        cbox_out = tf.reshape(net_out, [N, -1, filters])                \n",
    "        cbox_outputs.append(cbox_out)\n",
    "    \n",
    "    cbox_all = tf.concat(cbox_outputs, axis=1)\n",
    "    heatmap_displace = get_heatmap(p234567, img_h, img_w, num_keypoints + 1, alphas)    \n",
    "    \n",
    "    outputs = {'detect':cbox_all, 'heatmap':heatmap_displace, 'heatmap_coord':heatmap_displace}        \n",
    "    return keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoxLoss(tf.losses.Loss):\n",
    "\n",
    "    def __init__(self, delta):\n",
    "        super(BoxLoss, self).__init__(reduction=\"none\", name=\"BoxLoss\")\n",
    "        self._delta = delta\n",
    "\n",
    "    def call(self, y_true, y_pred):    \n",
    "        \n",
    "        difference = y_true - y_pred\n",
    "        absolute_difference = tf.abs(difference)        \n",
    "        loss = tf.where(\n",
    "            tf.less(absolute_difference, self._delta),\n",
    "            0.5 * (difference ** 2),\n",
    "            absolute_difference - 0.5,\n",
    "        )\n",
    "        loss = tf.where(loss < 0.05, 0.0, loss)#new marginal loss        \n",
    "        return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "class ClassificationLoss(tf.losses.Loss):\n",
    "\n",
    "    def __init__(self, alpha, gamma, num_classes):\n",
    "        super(ClassificationLoss, self).__init__(reduction=\"none\", name=\"ClassificationLoss\")\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "        self._num_classes = num_classes\n",
    "        \n",
    "        \n",
    "    def call(self, y_cls, y_pred):\n",
    "        y_cls_int = tf.cast(y_cls, dtype=tf.int32)\n",
    "        y_hot = tf.one_hot(y_cls_int, depth=self._num_classes, dtype=tf.float32,)\n",
    "        \n",
    "        y_positive = tf.cast(y_cls > 0, tf.float32)#finetune, 1:unknown\n",
    "        y_positive_identity = tf.cast(y_cls > 1, tf.float32)# 1:unknown\n",
    "        \n",
    "        obj_score = tf.identity(y_pred[:, :], name='obj_score')\n",
    "        #cls_score = y_pred[:, :, 1:1+self._num_classes]\n",
    "        \n",
    "        pt = tf.clip_by_value(obj_score, 1e-7, 1.0 - 1e-7)\n",
    "                \n",
    "        loss_p = - (1.0 - self._alpha) * tf.pow(1.0 - pt, self._gamma) * y_positive * tf.math.log(pt)        \n",
    "        loss_f = - self._alpha * tf.pow(pt, self._gamma) * (1 - y_positive) * tf.math.log(1 - pt)\n",
    "        loss_obj = loss_p + loss_f\n",
    "          \n",
    "        if False:\n",
    "            cls_pt = tf.nn.softmax(cls_score)        \n",
    "            cls_pt = tf.clip_by_value(cls_pt, 1e-7, 1.0 - 1e-7)\n",
    "            loss_cls_p = - tf.pow(1.0 - cls_pt, self._gamma) * y_hot * tf.math.log(cls_pt)\n",
    "            loss_cls_f = - tf.pow(cls_pt, self._gamma) * (1 - y_hot) * tf.math.log(1 - cls_pt)\n",
    "            loss_cls = tf.reduce_sum(loss_cls_p + loss_cls_f, axis=-1)        \n",
    "            is_various_cls_exist = tf.cast(tf.math.reduce_std(y_cls) > 0, tf.float32)                \n",
    "        #loss_cls = is_various_cls_exist * y_positive * loss_cls\n",
    "                        \n",
    "        #normalizer = tf.reduce_sum(y_positive_identity, axis=-1)\n",
    "        #loss_cls = tf.math.divide_no_nan(tf.reduce_sum(loss_cls, axis=-1), normalizer)                        \n",
    "        \n",
    "        loss = loss_obj# + .1*loss_cls#when not stable                \n",
    "        return loss   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointLoss(tf.losses.Loss):\n",
    "    \"\"\"Implements Smooth L1 loss\"\"\"\n",
    "\n",
    "    def __init__(self, delta, gamma, num_classes):\n",
    "        super(KeypointLoss, self).__init__(reduction=\"none\", name=\"KeypointLoss\")\n",
    "        self._delta = delta\n",
    "        self._gamma = gamma\n",
    "        self._num_classes = num_classes\n",
    "        \n",
    "    def get_xy_loss(self, y_true, y_pred):        \n",
    "        difference = y_true - y_pred\n",
    "        absolute_difference = tf.abs(difference)        \n",
    "        loss = tf.where(\n",
    "            tf.less(absolute_difference, self._delta),\n",
    "            0.5 * (difference ** 2),\n",
    "            absolute_difference - 0.5,\n",
    "        )\n",
    "        #loss_xy = tf.where(loss < 0.05, 0.0, loss)#new marginal loss        \n",
    "        loss = tf.reduce_mean(loss, axis=-1)\n",
    "        return loss\n",
    "    \n",
    "    def get_cls_loss(self, y_cls, y_pred):        \n",
    "        loss = tf.square(y_cls - y_pred)\n",
    "        return loss    \n",
    "\n",
    "    def call(self, y_true, y_pred):    \n",
    "        #y: [m, anchors, NUM_KEYPOINT * (2 + classes)]        \n",
    "        gt_x, gt_y, gt_cls = split_gt_keypoint(y_true)        \n",
    "        h_x, h_y, h_cls, h_kp_cls = split_h_keypoint(y_pred)\n",
    "        \n",
    "        gt_xy = tf.stack((gt_x, gt_y), -1)\n",
    "        h_xy = tf.stack((h_x, h_y), -1) \n",
    "        m = tf.shape(y_true)[0]\n",
    "        #h_cls_score = tf.reshape(h_cls, [m, -1, NUM_KEYPOINT, NUM_KEYPOINT_CLASS])\n",
    "        \n",
    "        loss_cls = self.get_cls_loss(gt_cls, h_cls)#(m, anchor)\n",
    "        loss_xy = self.get_xy_loss(gt_xy, h_xy)#(m, anchor, NUM_KEYPOINT)        \n",
    "           \n",
    "        is_exist_kp_label = tf.cast(tf.reduce_any(gt_cls > 0, -1, True), tf.float32)\n",
    "        \n",
    "        loss_xy = gt_cls * loss_xy \n",
    "        loss_cls = loss_cls * is_exist_kp_label\n",
    "        loss = tf.reduce_mean(loss_xy + 0.1 * loss_cls, axis=-1) #(m, anchor)         \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_KEYPOINT, NUM_KEYPOINT_CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetLoss(tf.losses.Loss):\n",
    "    \"\"\"Wrapper to combine both the losses\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=80, num_keypoint_classes=3, alpha=0.3, gamma=3.0, delta=1.0):#alpha=0.25\n",
    "        super(NetLoss, self).__init__(reduction=\"auto\", name=\"NetLoss\")\n",
    "        self._clf_loss = ClassificationLoss(alpha, gamma, num_classes)\n",
    "        self._box_loss = BoxLoss(delta)        \n",
    "        self._keypoint_loss = KeypointLoss(delta, gamma, num_keypoint_classes)                \n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        \n",
    "        y_box, y_cls, y_keypoint = split_gt(y_true)\n",
    "        h_box, h_obj, h_cls, h_keypoint = split_hyperthesis(y_pred)\n",
    "        gt_kp_x, gt_kp_y, gt_kp_cls = split_gt_keypoint(y_true)\n",
    "        \n",
    "        positive_mask = tf.greater(y_cls, 0.0)\n",
    "        ignore_mask = tf.less(y_cls, 0.0)\n",
    "        \n",
    "        clf_loss = self._clf_loss(y_cls, h_obj)\n",
    "        box_loss = self._box_loss(y_box, h_box) \n",
    "        #keypoint_loss = self._keypoint_loss(y_keypoint, h_keypoint) \n",
    "        \n",
    "        kp_cls_sum_per_anchor = 1 + tf.reduce_sum(gt_kp_cls, -1)\n",
    "        weight_kp = kp_cls_sum_per_anchor / tf.reduce_max(kp_cls_sum_per_anchor, -1, True)\n",
    "        \n",
    "        clf_loss = tf.where(ignore_mask, 0.0, clf_loss)                \n",
    "        box_loss = tf.where(positive_mask, box_loss, 0.0)\n",
    "        #keypoint_loss = tf.where(positive_mask, keypoint_loss, 0.0)\n",
    "        loss = clf_loss + 1 * box_loss# + 0.0000000 * keypoint_loss)\n",
    "        \n",
    "        positive_mask = tf.cast(positive_mask, tf.float32)        \n",
    "        normalizer = tf.reduce_sum(positive_mask, axis=-1)\n",
    "        normalizer = tf.sqrt(normalizer)\n",
    "        loss = tf.math.divide_no_nan(tf.reduce_sum(loss, axis=-1), normalizer)        \n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(y_cls, score, class_k, gamma):\n",
    "    y_cls_int = tf.cast(y_cls, dtype=tf.int32)\n",
    "    y_hot = tf.one_hot(y_cls_int, depth=class_k, dtype=tf.float32)\n",
    "        \n",
    "    cls_pt = tf.nn.softmax(score)        \n",
    "    cls_pt = tf.clip_by_value(cls_pt, 1e-7, 1.0 - 1e-7)\n",
    "    loss_cls_p = - tf.pow(1.0 - cls_pt, gamma) * y_hot * tf.math.log(cls_pt)\n",
    "    loss_cls_f = - tf.pow(cls_pt, gamma) * (1 - y_hot) * tf.math.log(1 - cls_pt)\n",
    "    loss_cls = tf.reduce_sum(loss_cls_p + loss_cls_f, axis=-1) \n",
    "    return loss_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_w = 7\n",
    "a = tf.reshape(tf.range(a_w*a_w, dtype=tf.float32), [1, a_w, a_w, 1])\n",
    "a = tf.where(tf.logical_and(a>10, a<14), a*0+1, 0)\n",
    "filters = tf.ones((5, 5, 1), tf.float32)\n",
    "a_dil = tf.nn.dilation2d(a, filters, [1,1,1,1], 'SAME', 'NHWC', dilations=(1,1,1,1))\n",
    "a_ero = tf.nn.erosion2d(a, filters, [1,1,1,1], 'SAME', 'NHWC', dilations=(1,1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dil[0,:,:,0]# - a[0,:,:,0] > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ero[0,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_displace_loss(gt, h):            \n",
    "    # gt : (m, h, w, 2) float32\n",
    "    # h :  (m, h, w, 2) sigmoid - 0.5             \n",
    "    diff = gt - h\n",
    "    loss = tf.square(diff) + tf.abs(diff)# * tf.cast(tf.abs(gt)>0.01, tf.float32)\n",
    "    loss = tf.boolean_mask(loss, tf.abs(gt) > 0.001)\n",
    "    #loss = tf.reduce_sum(loss, -1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeatmapLoss(tf.losses.Loss):\n",
    "    \"\"\"Wrapper to combine both the losses\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, alpha=0.25, gamma=3.0, delta=1.0):#alpha=0.25\n",
    "        super(HeatmapLoss, self).__init__(reduction=\"auto\", name=\"HeatmapLoss\")\n",
    "        self._delta = delta\n",
    "        self._gamma = gamma\n",
    "        self._num_classes = num_classes        \n",
    "        \n",
    "    def focal_loss(self, y_cls, score, class_k, gamma, alpha=0.5):\n",
    "        #눈코입 라벨이 없으면 눈코입 히트맵은 살려놓자        \n",
    "        y_cls_int = tf.cast(y_cls, dtype=tf.int32)\n",
    "        y_hot = tf.one_hot(y_cls_int, depth=class_k, dtype=tf.float32)\n",
    "        \n",
    "        k_cls = tf.reduce_sum(y_hot[:,:,:,1:], [1, 2], True)#(m,1,1,k)\n",
    "        k_cls_norm = k_cls / (0.01 + tf.reduce_max(k_cls, -1, True))\n",
    "        k_cls_norm = tf.square(k_cls_norm)\n",
    "        \n",
    "        #cls 가 없는 채널은 손실을 0.x 적게 적용 > 손실 없음\n",
    "        weight_cls = tf.cast(k_cls_norm, tf.float32)\n",
    "        weight_cls = tf.concat((tf.reduce_min(weight_cls, -1, True), weight_cls), -1)\n",
    "        \n",
    "        cls_pt = tf.nn.softmax(score)        \n",
    "        cls_pt = tf.clip_by_value(cls_pt, 1e-7, 1.0 - 1e-7)\n",
    "        loss_cls_p = - alpha * tf.pow(1.0 - cls_pt, gamma) * y_hot * tf.math.log(cls_pt)\n",
    "        loss_cls_f = - (1 - alpha) * tf.pow(cls_pt, gamma) * (1 - y_hot) * tf.math.log(1 - cls_pt)\n",
    "        loss_cls = tf.reduce_sum(loss_cls_p + loss_cls_f, axis=-1) \n",
    "                \n",
    "        loss_cls_exist_cls = tf.reduce_sum(weight_cls * (loss_cls_p + loss_cls_f), axis=-1)\n",
    "        return loss_cls, loss_cls_exist_cls\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        #(m, h, w, m_classes)\n",
    "        \n",
    "        y_heatmap, y_visible, y_displace = split_gt_heatmap_displace(y_true)\n",
    "        h_heatmap, h_visible, h_displace = split_heatmap_displace(y_pred)        \n",
    "        y_positive = tf.cast(y_heatmap > 0, tf.float32)\n",
    "        y_ignore = tf.cast(y_heatmap < 0, tf.float32)\n",
    "                \n",
    "        #[1,32,32,18] vs. [1,32,32,7,18] mul        \n",
    "        loss, loss_weighted = self.focal_loss(y_heatmap, h_heatmap, self._num_classes, self._gamma)\n",
    "        loss_visible, loss_visible_w = self.focal_loss(y_visible, h_visible, 3, 3.0)        \n",
    "        loss_visible = tf.where(y_visible < 0, 0.0, loss_visible)\n",
    "        loss_visible = tf.where(y_visible < 1, 0.001 * loss_visible, loss_visible)\n",
    "                \n",
    "        filters = tf.ones((70, 70, 1), tf.float32)\n",
    "        y_positive_dil = tf.nn.dilation2d(tf.expand_dims(y_positive, -1), filters, [1,1,1,1], 'SAME', 'NHWC', dilations=(1,1,1,1))\n",
    "        y_positive_dil = tf.cast(y_positive_dil > 0, tf.float32)\n",
    "        y_positive_dil = tf.squeeze(y_positive_dil, -1)\n",
    "        area_near = y_positive_dil\n",
    "        area_far = 1 - y_positive_dil        \n",
    "        w_near = 2/3\n",
    "                \n",
    "        loss_kp = loss * tf.cast(y_heatmap > 0, tf.float32)        \n",
    "        loss_bg = (1 - y_ignore)  * tf.cast(y_heatmap < 1, tf.float32) * loss_weighted\n",
    "        loss_bg = w_near * area_near * loss_bg + (1 - w_near) * area_far * loss_bg\n",
    "        loss_displace = get_displace_loss(y_displace, h_displace)        \n",
    "        \n",
    "        #오토바이 남녀 발이 보이는데 kp 표시가 없음        \n",
    "        loss_heatmap = tf.reduce_mean(loss_kp, [1,2]) + 1e-2 * tf.reduce_mean(loss_bg, [1,2]) \n",
    "        loss = tf.reduce_mean(loss_heatmap) + 10 * tf.reduce_mean(loss_displace) + tf.reduce_mean(loss_visible)\n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_by_box(images, img_h, img_w, boxes, crop_size_hw):# single batch only\n",
    "    \n",
    "    x1 = boxes[:, 0] / img_w\n",
    "    y1 = boxes[:, 1] / img_h\n",
    "    x2 = boxes[:, 2] / img_w\n",
    "    y2 = boxes[:, 3] / img_h\n",
    "    boxes_norm = tf.stack((y1, x1, y2, x2), -1)    #[num_boxes, 4], normalized coordinates ` [y1, x1, y2, x2]\n",
    "    box_indices = tf.zeros_like(boxes_norm[:, 0], tf.int32)\n",
    "    \n",
    "    crop_image = tf.image.crop_and_resize(images, boxes_norm, box_indices, crop_size_hw)\n",
    "    return crop_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kp_from_heatmap(crop_heatmap_score, crop_size):    \n",
    "    #(m==1, h, w, ch+1)\n",
    "    cls_idx = tf.range(NUM_KEYPOINT+1)    \n",
    "    heamap_logit = tf.nn.softmax(crop_heatmap_score)\n",
    "    heamap_logit_kp = heamap_logit[:,:,:,1:]\n",
    "    \n",
    "    #visible_1 = tf.reduce_any(heamap_logit_kp > 0.5, [1, 2])\n",
    "    #visible_2 = tf.reduce_any(heamap_logit_kp > 0.9, [1, 2])\n",
    "    \n",
    "    thresh_k = crop_size[0] * 0.05\n",
    "    k_1 = tf.reduce_sum(tf.cast(heamap_logit_kp > 0.5, tf.float32), [1, 2])\n",
    "    k_2 = tf.reduce_sum(tf.cast(heamap_logit_kp > 0.9, tf.float32), [1, 2])\n",
    "    visible_1 = k_1 > thresh_k\n",
    "    visible_2 = k_2 > thresh_k\n",
    "    \n",
    "    visible = tf.cast(visible_1, tf.float32) + tf.cast(visible_2, tf.float32)\n",
    "    \n",
    "    crop_heatmap_score = crop_heatmap_score[:,:,:,1:]\n",
    "    crop_heatmap_exp = tf.exp(crop_heatmap_score)\n",
    "        \n",
    "    crop_heatmap_norm = crop_heatmap_exp / (0 + tf.reduce_sum(crop_heatmap_exp, [1, 2], True))\n",
    "    coord_xy = coordinate_map_norm(crop_size[0], crop_size[1])#(h, w, 2)\n",
    "    coord_xy_5d = tf.reshape(coord_xy, [1, crop_size[0], crop_size[1], 1, 2])\n",
    "    crop_heatmap_norm_5d = tf.expand_dims(crop_heatmap_norm, -1)\n",
    "    #(1, h, w, 1, 2) * (None, 100, 200, 18, 1) = (None, h, w, 18, 2), sum() > (None, 18, 2)\n",
    "    kp_xy = tf.reduce_sum(coord_xy_5d * crop_heatmap_norm_5d, [1, 2])\n",
    "    return kp_xy, visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kp_from_heatmap_displace(crop_heatmap_score, displace, crop_size):\n",
    "    \n",
    "    #(m==1, h, w, ch+1)\n",
    "    cls_idx = tf.range(NUM_KEYPOINT+1)    \n",
    "    heamap_logit = tf.nn.softmax(crop_heatmap_score)\n",
    "    heamap_logit_kp = heamap_logit[:,:,:,1:]\n",
    "    \n",
    "    #visible_1 = tf.reduce_any(heamap_logit_kp > 0.5, [1, 2])\n",
    "    #visible_2 = tf.reduce_any(heamap_logit_kp > 0.9, [1, 2])\n",
    "    \n",
    "    thresh_k = crop_size[0] * 0.1\n",
    "    k_1 = tf.reduce_sum(tf.cast(heamap_logit_kp > 0.5, tf.float32), [1, 2])\n",
    "    k_2 = tf.reduce_sum(tf.cast(heamap_logit_kp > 0.9, tf.float32), [1, 2])\n",
    "    visible_1 = k_1 > thresh_k\n",
    "    visible_2 = k_2 > thresh_k\n",
    "    \n",
    "    visible = tf.cast(visible_1, tf.float32) + tf.cast(visible_2, tf.float32)    \n",
    "    \n",
    "    positive_kp_mask = tf.cast(tf.argmax(crop_heatmap_score, -1) > 0, tf.float32)\n",
    "    crop_heatmap_score = crop_heatmap_score[:,:,:,1:]    \n",
    "        \n",
    "    coord_xy = coordinate_map_norm(crop_size[0], crop_size[1])#(h, w, 2), [0, 1]\n",
    "    coord_xy_4d = tf.reshape(coord_xy, [1, crop_size[0], crop_size[1], 2])\n",
    "    coord_xy_5d = tf.reshape(coord_xy, [1, crop_size[0], crop_size[1], 1, 2])\n",
    "    coord_xy_uv = coord_xy - 0.5\n",
    "    \n",
    "    #gt : keypoints_xy - roots\n",
    "    #distance와 kp의 거리\n",
    "    #print('coord_xy_uv', coord_xy_uv)#(100, 200, 2)\n",
    "    #print('displace', displace)#(None, 100, 200, 2)\n",
    "    displace_rel = displace[:, :, :, :2]\n",
    "    displace_global = displace[:, :, :, 2:4]\n",
    "    distance_error = tf.sqrt(tf.reduce_sum(tf.square(tf.expand_dims(coord_xy_uv, 0) - displace_rel), -1, True))    \n",
    "    distance_show = tf.cast(distance_error*255, tf.uint8)\n",
    "    distance_show = tf.concat((distance_show,distance_show, distance_show), -1)\n",
    "    \n",
    "    #선별 (m,h,w,k) * (m,h,w,1)\n",
    "    \n",
    "    #positive kp내부에서 k개의 최소 에러와 최대 에러, 평균 에러\n",
    "    if False:\n",
    "        # p-kp 내부의 distance_error 의 평균 이하인 스코어를 선택\n",
    "        distance_error_on_positive_kp = tf.expand_dims(positive_kp_mask, -1) * distance_error\n",
    "        crop_heatmap_score_on_positive_kp = tf.expand_dims(positive_kp_mask, -1) * crop_heatmap_score\n",
    "        distance_error_mean = tf.reduce_mean(distance_error, [1,2], True)\n",
    "        crop_heatmap_score_mean = tf.reduce_mean(crop_heatmap_score, [1,2], True)        \n",
    "        cond = tf.logical_and(crop_heatmap_score > crop_heatmap_score_mean, distance_error<distance_error_mean)\n",
    "        crop_heatmap_score = tf.where(cond, crop_heatmap_score, crop_heatmap_score*0.1)#todo test\n",
    "        #crop_heatmap_exp_group = tf.where(distance_error < distance_error_mean, tf.exp(crop_heatmap_score), 0)#todo test\n",
    "        \n",
    "    #crop_heatmap_exp_group = crop_heatmap_exp * tf.cast(valid_group, tf.float32)\n",
    "    crop_heatmap_exp_group = tf.exp(crop_heatmap_score) * tf.pow(1 - distance_error, 5)        \n",
    "    \n",
    "    crop_heatmap_norm = crop_heatmap_exp_group / (0 + tf.reduce_sum(crop_heatmap_exp_group, [1, 2], True))\n",
    "    crop_heatmap_norm_5d = tf.expand_dims(crop_heatmap_norm, -1)\n",
    "    \n",
    "    #(1, h, w, 1, 2) * (None, 100, 200, 18, 1) = (None, h, w, 18, 2), sum() > (None, 18, 2)\n",
    "    kp_xy = tf.reduce_sum(coord_xy_5d * crop_heatmap_norm_5d, [1, 2])\n",
    "    \n",
    "    #실제 좌표맵과 kp의 거리 , (1, h, w, 2 ) - (m, h, w, 2) = (m, h, w, 2)\n",
    "    \n",
    "    return kp_xy, visible, distance_show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kp_from_heatmap_displacex2(crop_heatmap_score, displace, crop_size, crop_coord_map):\n",
    "    \n",
    "    #(m==1, h, w, ch+1)\n",
    "    cls_idx = tf.range(NUM_KEYPOINT+1)    \n",
    "    heamap_logit = tf.nn.softmax(crop_heatmap_score)\n",
    "    heamap_logit_kp = heamap_logit[:,:,:,1:]\n",
    "    \n",
    "    thresh_k = crop_size[0] * 0.03\n",
    "    k_1 = tf.reduce_sum(tf.cast(heamap_logit_kp > 0.4, tf.float32), [1, 2])\n",
    "    k_2 = tf.reduce_sum(tf.cast(heamap_logit_kp > 0.9, tf.float32), [1, 2])\n",
    "    visible_1 = k_1 > thresh_k\n",
    "    visible_2 = k_2 > thresh_k\n",
    "    \n",
    "    visible = tf.cast(visible_1, tf.float32) + tf.cast(visible_2, tf.float32)    \n",
    "    \n",
    "    positive_kp_mask = tf.cast(tf.argmax(crop_heatmap_score, -1) > 0, tf.float32)\n",
    "    crop_heatmap_score = crop_heatmap_score[:,:,:,1:] \n",
    "        \n",
    "    coord_xy = coordinate_map_norm(crop_size[0], crop_size[1])#(h, w, 2), [0, 1]\n",
    "    coord_xy_4d = tf.reshape(coord_xy, [1, crop_size[0], crop_size[1], 2])\n",
    "    coord_xy_5d = tf.reshape(coord_xy, [1, crop_size[0], crop_size[1], 1, 2])\n",
    "    coord_xy_uv = coord_xy - 0.5\n",
    "    \n",
    "    #gt : keypoints_xy - roots\n",
    "    #distance와 kp의 거리\n",
    "    #print('coord_xy_uv', coord_xy_uv)#(100, 200, 2)\n",
    "    #print('displace', displace)#(None, 100, 200, 2)\n",
    "    displace_rel = displace[:, :, :, :2]\n",
    "    displace_global = displace[:, :, :, 2:4]\n",
    "    \n",
    "    #print('crop_coord_map', crop_coord_map)#(m, 100, 100, 2)\n",
    "    # h_displace_global = kp_center - box_center \n",
    "    #displace_kp_to_root_norm_global = (keypoints_xy - roots) / np.reshape([x_w, x_h], [1, 1, 2]) #(n, k, 2)\n",
    "    \n",
    "    distance_rel_error = tf.sqrt(tf.reduce_sum(tf.square(tf.expand_dims(coord_xy_uv, 0) - displace_rel), -1, True))    \n",
    "    displace_global_gt = crop_coord_map - tf.reduce_mean(crop_coord_map, [1, 2], True)\n",
    "    displace_global_error = tf.sqrt(tf.reduce_sum(tf.square(displace_global_gt - displace_global), -1, True))\n",
    "  \n",
    "    displace_rel_kp = tf.expand_dims(positive_kp_mask, -1) * displace_rel\n",
    "    displace_rel_error_self = tf.sqrt(tf.reduce_sum(tf.square(displace_rel_kp - tf.reduce_mean(displace_rel_kp, [1,2], True)), -1, True))\n",
    "    \n",
    "    #선별 (m,h,w,k) * (m,h,w,1)    \n",
    "    #crop_heatmap_exp_group = crop_heatmap_exp * tf.cast(valid_group, tf.float32)    \n",
    "    displace_error = distance_rel_error + displace_global_error\n",
    "    w = tf.pow(tf.maximum(0.0, 1 - displace_error), 2)    \n",
    "    \n",
    "    crop_heatmap_exp_group = heamap_logit_kp * w * tf.expand_dims(positive_kp_mask, -1)\n",
    "    crop_heatmap_exp_group = 1e-7 + tf.where(crop_heatmap_exp_group >= 0.9 * tf.reduce_max(crop_heatmap_exp_group, [1, 2], True), crop_heatmap_exp_group, crop_heatmap_exp_group*0.00)\n",
    "    \n",
    "    crop_heatmap_norm = crop_heatmap_exp_group / tf.reduce_sum(crop_heatmap_exp_group, [1, 2], True)\n",
    "    crop_heatmap_norm_5d = tf.expand_dims(crop_heatmap_norm, -1)\n",
    "    \n",
    "    #(1, h, w, 1, 2) * (None, 100, 200, 18, 1) = (None, h, w, 18, 2), sum() > (None, 18, 2)\n",
    "    kp_xy = tf.reduce_sum(coord_xy_5d * crop_heatmap_norm_5d, [1, 2])\n",
    "    \n",
    "    #실제 좌표맵과 kp의 거리 , (1, h, w, 2 ) - (m, h, w, 2) = (m, h, w, 2)\n",
    "    \n",
    "    return kp_xy, visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate_map_norm(4,4) - tf.reduce_mean(coordinate_map_norm(4,4), [0, 1], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.power(1 - 0.1, 2), np.power(1 - 0.1 - 0.1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate_map_np(9, 19).mean(axis=0).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.power(4, 2), np.power(0.5, 2), np.power(0.5, 3), np.power(0.5, 4), np.power(0.5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms_kp_dist(gt, h_heatmap):\n",
    "    \n",
    "    _num_classes = NUM_KEYPOINT + 1\n",
    "    _anchor_ch = anchor_ch\n",
    "    _anchor_box = AnchorBox()\n",
    "    \n",
    "    m = tf.shape(gt)[0]\n",
    "    image_h = padded_image_shape[0]\n",
    "    image_w = padded_image_shape[1]\n",
    "    \n",
    "    anchor_boxes = _anchor_box.get_anchors(image_h, image_w)\n",
    "\n",
    "    box_predictions, objectness, keypoint_gt = split_gt(gt)\n",
    "    x, y, keypoints_cls = split_gt_keypoint(keypoint_gt)\n",
    "\n",
    "    keypoints_xy = tf.stack((x, y), -1)\n",
    "    keypoints_xy = tf.reshape(keypoints_xy, (m, -1, NUM_KEYPOINT * 2))\n",
    "\n",
    "    boxes = _decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n",
    "    keypoints_xy = _decode_keypoint_predictions(anchor_boxes[None, ...], keypoints_xy)\n",
    "    boxes_2d = tf.reshape(boxes, [-1, 4])    \n",
    "    scores = tf.reshape(objectness, [-1, 1])\n",
    "\n",
    "    cls = objectness * 0 + 1\n",
    "    cls = tf.reshape(cls, [-1, 1])\n",
    "\n",
    "    keypoints_xy_3d = tf.reshape(keypoints_xy, [-1, NUM_KEYPOINT, 2])\n",
    "    keypoints_cls_3d = tf.reshape(keypoints_cls, [-1, NUM_KEYPOINT, 1])    \n",
    "    keypoints_3d = tf.concat((keypoints_xy_3d, keypoints_cls_3d), -1)\n",
    "    keypoints_2d = tf.reshape(keypoints_3d, [-1, NUM_KEYPOINT * 3])\n",
    "\n",
    "    ccbox = tf.concat((cls, scores, boxes_2d, keypoints_2d), -1)\n",
    "    ccbox_check = tf.concat((boxes_2d, cls, keypoints_2d), -1)\n",
    "\n",
    "    confidence_threshold=0.5\n",
    "    nms_iou_threshold=0.2\n",
    "    max_detections_per_class=100\n",
    "    max_detections=150\n",
    "\n",
    "    selected_indices, selected_scores = tf.image.non_max_suppression_with_scores(    \n",
    "        ccbox[:, 2:2+4],\n",
    "        ccbox[:, 1],        \n",
    "        max_detections,\n",
    "        nms_iou_threshold,\n",
    "        confidence_threshold,        \n",
    "    )\n",
    "        \n",
    "    positive_box = tf.gather(ccbox_check, selected_indices)\n",
    "    \n",
    "    positive_keypoints = positive_box[:, 5:]\n",
    "    positive_keypoints_x = positive_keypoints[:, 0::3]\n",
    "    positive_keypoints_y = positive_keypoints[:, 1::3]\n",
    "    positive_keypoints_cls = positive_keypoints[:, 2::3]\n",
    "    positive_keypoints_xy = tf.stack((positive_keypoints_x, positive_keypoints_y), -1)\n",
    "    \n",
    "    cls_sum_max = tf.reduce_max(tf.reduce_sum(positive_keypoints_cls, -1))\n",
    "    cond_select = tf.reduce_sum(positive_keypoints_cls, -1) > cls_sum_max / 2\n",
    "    positive_box = tf.gather_nd(ccbox_check, tf.where(cond_select))#todo fix. without, Nan  \n",
    "    \n",
    "    positive_kp = positive_box[:, 5:]\n",
    "    positive_kp_x = positive_kp[:, 0::3]\n",
    "    positive_kp_y = positive_kp[:, 1::3]\n",
    "    positive_kp_cls = positive_kp[:, 2::3]\n",
    "    positive_kp_xy = tf.stack((positive_kp_x, positive_kp_y), -1)\n",
    "        \n",
    "    x1 = positive_box[:, 0] / image_w\n",
    "    y1 = positive_box[:, 1] / image_h\n",
    "    x2 = positive_box[:, 2] / image_w\n",
    "    y2 = positive_box[:, 3] / image_h\n",
    "    boxes = tf.stack((y1, x1, y2, x2), -1)    #[num_boxes, 4], normalized coordinates ` [y1, x1, y2, x2]\n",
    "    box_indices = tf.zeros_like(boxes[:, 0], tf.int32)\n",
    "    crop_size = [100, 100]#h, w        \n",
    "    crop_heatmap_score = tf.image.crop_and_resize(h_heatmap, boxes, box_indices, crop_size)#not None\n",
    "\n",
    "    kp_xy_norm, visible = get_kp_from_heatmap(crop_heatmap_score, crop_size)#None\n",
    "\n",
    "    kp_offset = positive_box[:, None, :2]    \n",
    "    kp_scale = positive_box[:, None, 2:4] - positive_box[:, None, 0:2]\n",
    "    kp_xy = kp_scale * kp_xy_norm + kp_offset\n",
    "    positive_kp_xy_norm = (positive_kp_xy - kp_offset) / kp_scale\n",
    "\n",
    "    #loss_kp_xy = tf.reduce_mean(tf.abs(positive_kp_xy - kp_xy), -1)\n",
    "    loss_kp_xy_norm = tf.reduce_sum(tf.abs(positive_kp_xy_norm - kp_xy_norm), -1)\n",
    "    loss_kp_xy = tf.reduce_sum(tf.sqrt(tf.abs(positive_kp_xy - kp_xy)), -1)\n",
    "    dist = (loss_kp_xy + loss_kp_xy_norm) * tf.cast(positive_kp_cls > 1, tf.float32)\n",
    "\n",
    "    return tf.reduce_mean(dist)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeatmapCoordLoss(tf.losses.Loss):   \n",
    "    \n",
    "    def __init__(self, num_classes, anchor_ch):\n",
    "        super(HeatmapCoordLoss, self).__init__(reduction=\"auto\", name=\"HeatmapCoordLoss\")\n",
    "        self._num_classes = num_classes\n",
    "        self._anchor_ch = anchor_ch\n",
    "        self._anchor_box = AnchorBox()   \n",
    "\n",
    "    def call(self, gt, h_heatmap_displace):\n",
    "       \n",
    "        m = tf.shape(gt)[0]\n",
    "        image_h = padded_image_shape[0]\n",
    "        image_w = padded_image_shape[1]                \n",
    "        anchor_boxes = self._anchor_box.get_anchors(image_h, image_w)\n",
    "            \n",
    "        h_heatmap, h_visible, h_displace = split_heatmap_displace(h_heatmap_displace)\n",
    "        box_predictions, objectness, keypoint_gt = split_gt(gt)\n",
    "        x, y, keypoints_cls = split_gt_keypoint(keypoint_gt)\n",
    "        \n",
    "        keypoints_xy = tf.stack((x, y), -1)\n",
    "        keypoints_xy = tf.reshape(keypoints_xy, (m, -1, NUM_KEYPOINT * 2))\n",
    "\n",
    "        boxes = _decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n",
    "        keypoints_xy = _decode_keypoint_predictions(anchor_boxes[None, ...], keypoints_xy)\n",
    "        boxes_2d = tf.reshape(boxes, [-1, 4])    \n",
    "        scores = tf.reshape(objectness, [-1, 1])\n",
    "        cls = objectness * 0 + 1\n",
    "        cls = tf.reshape(cls, [-1, 1])\n",
    "\n",
    "        keypoints_xy_3d = tf.reshape(keypoints_xy, [-1, NUM_KEYPOINT, 2])\n",
    "        keypoints_cls_3d = tf.reshape(keypoints_cls, [-1, NUM_KEYPOINT, 1])    \n",
    "        keypoints_3d = tf.concat((keypoints_xy_3d, keypoints_cls_3d), -1)\n",
    "        keypoints_2d = tf.reshape(keypoints_3d, [-1, NUM_KEYPOINT * 3])\n",
    "\n",
    "        ccbox = tf.concat((cls, scores, boxes_2d, keypoints_2d), -1)\n",
    "        ccbox_check = tf.concat((boxes_2d, cls, keypoints_2d), -1)\n",
    "        \n",
    "        confidence_threshold=0.5\n",
    "        nms_iou_threshold=0.2\n",
    "        max_detections_per_class=100\n",
    "        max_detections=150\n",
    "\n",
    "        selected_indices, selected_scores = tf.image.non_max_suppression_with_scores(    \n",
    "            ccbox[:, 2:2+4], ccbox[:, 1], max_detections, nms_iou_threshold, confidence_threshold)\n",
    "        \n",
    "        positive_box = tf.gather(ccbox_check, selected_indices)\n",
    "        positive_keypoints = positive_box[:, 5:]        \n",
    "        positive_keypoints_cls = positive_keypoints[:, 2::3]\n",
    "                        \n",
    "        cls_sum_max = tf.reduce_max(tf.reduce_sum(positive_keypoints_cls, -1))\n",
    "        cond_select = tf.reduce_sum(positive_keypoints_cls, -1) > cls_sum_max / 2\n",
    "        positive_box = tf.gather_nd(ccbox_check, tf.where(cond_select))#todo fix. without, Nan  \n",
    "       \n",
    "        positive_kp = positive_box[:, 5:]\n",
    "        positive_kp_x = positive_kp[:, 0::3]\n",
    "        positive_kp_y = positive_kp[:, 1::3]\n",
    "        positive_kp_cls = positive_kp[:, 2::3]\n",
    "        positive_kp_xy = tf.stack((positive_kp_x, positive_kp_y), -1)        \n",
    "        \n",
    "        coord_map = coordinate_map_norm(image_h, image_w)\n",
    "        coord_map = tf.expand_dims(coord_map, 0) + tf.zeros_like(h_heatmap[:,:,:,:2])        \n",
    "        crop_size = [100, 100]#h, w\n",
    "        \n",
    "        #h_heatmap_displace_coord = tf.concat((h_heatmap_displace, coord_map), -1)\n",
    "        crop_heatmap_score = crop_image_by_box(h_heatmap, image_h, image_w, positive_box, crop_size)\n",
    "        crop_displace = crop_image_by_box(h_displace, image_h, image_w, positive_box, crop_size)\n",
    "        crop_coord_map = crop_image_by_box(coord_map, image_h, image_w, positive_box, crop_size)\n",
    "                \n",
    "        kp_xy_norm, visible = get_kp_from_heatmap_displacex2(crop_heatmap_score, crop_displace, crop_size, crop_coord_map)\n",
    "                \n",
    "        kp_offset = positive_box[:, None, :2]    \n",
    "        kp_scale = positive_box[:, None, 2:4] - positive_box[:, None, 0:2]\n",
    "        kp_xy = kp_scale * kp_xy_norm + kp_offset\n",
    "        positive_kp_xy_norm = (positive_kp_xy - kp_offset) / kp_scale\n",
    "        \n",
    "        weight_scale =  (1 + tf.reduce_max(kp_scale, -1, True)/image_h)\n",
    "        loss_weight = tf.reduce_sum(positive_kp_cls, -1, True) + weight_scale\n",
    "        loss_kp_xy_abs = tf.reduce_mean(tf.abs(positive_kp_xy - kp_xy)/crop_size[0], -1)\n",
    "        loss_kp_xy_abs = loss_weight * loss_kp_xy_abs * tf.cast(positive_kp_cls > VISIBILITY_OCCLUDED, tf.float32)#박스 내부에 나타난 kp만 학습        \n",
    "        \n",
    "        loss_kp_xy_norm = positive_kp_xy_norm - kp_xy_norm\n",
    "        loss_kp_xy_norm = tf.where(tf.abs(loss_kp_xy_norm) < 0.05, 0.0, loss_kp_xy_norm)#marginal loss        \n",
    "        loss_kp = tf.reduce_mean(tf.abs(loss_kp_xy_norm) + tf.square(loss_kp_xy_norm), -1)        \n",
    "        loss_kp = loss_weight * loss_kp * tf.cast(positive_kp_cls > VISIBILITY_OCCLUDED, tf.float32)#박스 내부에 나타난 kp만 학습\n",
    "        loss_kp = tf.reduce_sum(loss_kp, -1)\n",
    "        \n",
    "        # visibility 0 이 나오면 안됨\n",
    "        positive_kp_cls_4d = tf.reshape(positive_kp_cls, [-1, 1, 1, NUM_KEYPOINT])\n",
    "        crop_heatmap_logit = tf.nn.softmax(crop_heatmap_score)\n",
    "        crop_heatmap_logit_no_bg = crop_heatmap_logit[:, :, :, 1:]\n",
    "        \n",
    "        loss = tf.reduce_mean(loss_kp) + tf.reduce_mean(loss_kp_xy_abs)\n",
    "        return loss         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisplaceLoss(tf.losses.Loss):   \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DisplaceLoss, self).__init__(reduction=\"auto\", name=\"DisplaceLoss\")\n",
    "        pass\n",
    "    \n",
    "    def call(self, gt, h):            \n",
    "        # gt : (m, h, w, 2) float32\n",
    "        # h :  (m, h, w, 2) sigmoid - 0.5             \n",
    "        diff = gt - h\n",
    "        loss = tf.square(diff) + tf.abs(diff)# * tf.cast(tf.abs(gt)>0.01, tf.float32)\n",
    "        loss = tf.boolean_mask(loss, tf.abs(gt) > 0.01)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_ch = 4+1+num_classes+NUM_KEYPOINT*(2+1)\n",
    "anchor_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_positive = tf.cast(y_cls > 0, tf.int32)\n",
    "    y_bg = tf.cast(tf.abs(y_cls)==0, tf.int32)\n",
    "    h_score = y_pred[:, :, 4]\n",
    "    h_prob = tf.nn.sigmoid(h_score)    \n",
    "    h_postive = tf.cast(tf.round(h_prob), tf.int32)\n",
    "    \n",
    "    true_positives = tf.cast(tf.logical_and(y_cls > 0, h_postive>0), tf.float32)\n",
    "    false_negative = y_positive * (1 - h_postive)\n",
    "                \n",
    "    tp = tf.reduce_sum(true_positives, axis=1)# + 0.01\n",
    "    fn = tf.reduce_sum(false_negative, axis=1)\n",
    "    tp = tf.cast(tp, tf.float32)\n",
    "    fn = tf.cast(fn, tf.float32)\n",
    "    \n",
    "    rec = tp / (tp + fn + 1e-8)\n",
    "    return rec\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \n",
    "    y_cls_symbol = tf.cast(y_true[:, :, 4], dtype=tf.int32)    \n",
    "    y_cls_symbol = tf.cast(y_cls_symbol != 0, tf.int32)\n",
    "    h_obj_prob = tf.nn.sigmoid(y_pred[:, :, 4])\n",
    "    h_cls_symbol = tf.round(h_obj_prob)    \n",
    "    h_cls_symbol = tf.cast(h_cls_symbol, tf.int32)\n",
    "    \n",
    "    true_positives = y_cls_symbol * h_cls_symbol\n",
    "    false_positive = (1 - y_cls_symbol) * h_cls_symbol\n",
    "    \n",
    "    ones = tf.ones_like(true_positives)\n",
    "    zeeros = tf.zeros_like(true_positives)\n",
    "    true_positives = tf.cast(tf.equal(true_positives, ones), tf.float32)\n",
    "    false_positive = tf.cast(tf.equal(false_positive, ones), tf.float32)\n",
    "    \n",
    "    tp = tf.reduce_sum(true_positives, axis=1)# + 0.01\n",
    "    fp = tf.reduce_sum(false_positive, axis=1)\n",
    "    tp = tf.cast(tp, tf.float32)\n",
    "    fp = tf.cast(fp, tf.float32)\n",
    "    prec = tp / (tp + fp + 1e-8)\n",
    "    return prec\n",
    "\n",
    "def acc(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_positive = y_cls > 0    \n",
    "    h_cls = tf.math.argmax(y_pred[:, :, 5:5+num_classes], -1, output_type=tf.int32)        \n",
    "    acc = tf.boolean_mask(tf.equal(y_cls, h_cls), y_positive)    \n",
    "    #acc = tf.equal(y_cls, h_cls)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kp_cls_acc(y_true, y_pred):#keypoint\n",
    "    y_box, y_cls, y_keypoint = split_gt(y_true)    \n",
    "    h_box, h_obj, h_cls, h_keypoint = split_hyperthesis(y_pred)\n",
    "    \n",
    "    gt_x, gt_y, gt_cls = split_gt_keypoint(y_keypoint)\n",
    "    h_x, h_y, h_kp_cls_score, h_kp_cls = split_h_keypoint(h_keypoint)\n",
    "\n",
    "    gt_xy = tf.concat((gt_x, gt_y), -1)\n",
    "    h_xy = tf.concat((h_x, h_y), -1) \n",
    "    m = tf.shape(y_true)[0]\n",
    "        \n",
    "    #[1,14160,17] vs. [1,12240,17]    \n",
    "    gt_cls = tf.cast(gt_cls, tf.int32)\n",
    "    h_kp_cls = tf.cast(h_kp_cls, tf.int32)\n",
    "    acc = tf.equal(gt_cls, h_kp_cls)\n",
    "    #is_exist_kp_label = tf.cast(tf.reduce_any(gt_cls > 0, -1, True), tf.float32)\n",
    "    cond = tf.logical_and(y_cls > 0, tf.reduce_any(gt_cls > 0, -1))\n",
    "    acc = tf.boolean_mask(acc, cond)\n",
    "    return acc   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_acc(y_true, y_pred):#keypoint\n",
    "    \n",
    "    y_heatmap, heatmap_visible, y_displace = split_gt_heatmap_displace(y_true)\n",
    "    h_heatmap, h_visible, h_displace = split_heatmap_displace(y_pred)\n",
    "    \n",
    "    h_cls = tf.argmax(h_heatmap, -1)    \n",
    "    h_cls = tf.cast(h_cls, tf.int32)\n",
    "    y_cls = tf.cast(y_heatmap, tf.int32)\n",
    "    \n",
    "    acc = tf.equal(y_cls, h_cls) \n",
    "    acc = tf.boolean_mask(acc, y_heatmap > 0)\n",
    "    return acc\n",
    "\n",
    "def visible_acc(y_true, y_pred):\n",
    "    y_heatmap, y_visible, y_displace = split_gt_heatmap_displace(y_true)\n",
    "    h_heatmap, h_visible, h_displace = split_heatmap_displace(y_pred)\n",
    "    \n",
    "    h_cls = tf.argmax(h_visible, -1)    \n",
    "    h_cls = tf.cast(h_cls, tf.int32)\n",
    "    y_cls = tf.cast(y_visible, tf.int32)\n",
    "    \n",
    "    acc = tf.equal(y_cls, h_cls) \n",
    "    acc = tf.boolean_mask(acc, y_visible > 0)\n",
    "    return acc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, linewidth=200)\n",
    "image_height, image_width = padded_image_shape\n",
    "\n",
    "img_check = 0\n",
    "for image, multi_y in val_dataset:\n",
    "    output_map = multi_y['detect']\n",
    "    heatmap = multi_y['heatmap']\n",
    "    print('output_map', output_map.shape)\n",
    "    cbbox = output_map    \n",
    "    bbox = cbbox[:, :, :4]\n",
    "    cls_gt = cbbox[:,:,4]\n",
    "    img_m, image_height, image_width, image_ch = image.shape\n",
    "    anchor_feature_size = [(np.ceil(image_height / 2 ** i), np.ceil(image_width / 2 ** i)) \n",
    "                           for i in range(level_start, level_end)]\n",
    "    print('anchor_feature_size', anchor_feature_size)    \n",
    "    m = len(cbbox)    \n",
    "    positive_count = np.sum(cls_gt>0)\n",
    "    print('cbbox', cbbox.shape)\n",
    "    print('cls_sum',np.sum(cls_gt < 0.0), np.sum(cls_gt == 0.0), \n",
    "          np.sum(cls_gt == 1.0), np.sum(cls_gt > 1.0))\n",
    "    print('cls_mean',np.mean(cls_gt < 0.0), np.mean(cls_gt == 0.0), \n",
    "          np.mean(cls_gt == 1.0), np.mean(cls_gt > 0.0))\n",
    "    print('shape',image.shape, cbbox.shape,'unique', np.unique(cls_gt))\n",
    "    print('anchor_feature_size', anchor_feature_size)\n",
    "    offset = 0\n",
    "    positive_maps = []\n",
    "    for anchor_feature_size_1 in anchor_feature_size:        \n",
    "        fm_h, fm_w = anchor_feature_size_1\n",
    "        fm_h = int(fm_h)\n",
    "        fm_w = int(fm_w)        \n",
    "        fm_wh = int(fm_h * fm_w * anchor_k)\n",
    "        cbbox_anchor = cbbox[:, offset:offset+fm_wh, 4]\n",
    "        cbbox_anchor = np.reshape(cbbox_anchor, [m, fm_h, fm_w, anchor_k])\n",
    "        coount_m1 = np.count_nonzero(cbbox_anchor==-1)\n",
    "        coount_0 = np.count_nonzero(cbbox_anchor==0)\n",
    "        coount_1 = np.count_nonzero(cbbox_anchor==1)\n",
    "        coount_1_over = np.count_nonzero(cbbox_anchor>1)\n",
    "        positive_ratio = np.mean(cbbox_anchor>0)\n",
    "        positive_maps.append(cbbox_anchor>0)\n",
    "        print('cbbox_anchor', cbbox_anchor.shape, coount_m1, coount_0, coount_1, coount_1_over, 'ratio', positive_ratio)\n",
    "        sample_0_cbbox = cbbox_anchor[0]\n",
    "        sample_0_cbbox_sum = np.max(sample_0_cbbox, -1).astype(np.int)       \n",
    "      \n",
    "        offset += fm_wh\n",
    "        if False:            \n",
    "            file_name = str(fm_h)+ '_' + str(fm_w)+ '.txt'\n",
    "            np.savetxt(file_name,sample_0_cbbox_sum, fmt='%d',delimiter='')\n",
    "    img_check = image\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmap0 = np.array(Image.fromarray(np.max(positive_maps[0][0],-1)).resize((image_width, image_height)))\n",
    "pmap1 = np.array(Image.fromarray(np.max(positive_maps[1][0],-1)).resize((image_width, image_height)))\n",
    "pmap2 = np.array(Image.fromarray(np.max(positive_maps[2][0],-1)).resize((image_width, image_height)))\n",
    "#pmap3 = np.array(Image.fromarray(np.max(positive_maps[3][0],-1)).resize((image_width, image_height)))\n",
    "#pmap4 = np.array(Image.fromarray(np.max(positive_maps[4][0],-1)).resize((image_width, image_height)))\n",
    "pmap0 = pmap0.astype(np.uint8)\n",
    "pmap1 = pmap1.astype(np.uint8)\n",
    "pmap2 = pmap2.astype(np.uint8)\n",
    "pmap3 = 0#pmap3.astype(np.uint8)\n",
    "pmap4 = 0#pmap4.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pmap_with_img = np.array(img_check)[0]#*255\n",
    "pmap_with_img = pmap_with_img.astype(np.uint8)\n",
    "pmap_add = np.expand_dims(pmap0+pmap1+pmap2+pmap3+pmap4, -1)\n",
    "pmap = pmap_add*(255//np.max(pmap_add))\n",
    "mix_rgb = np.concatenate((pmap, pmap_with_img[:,:,1:]),-1)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(mix_rgb)\n",
    "plt.title(str(np.mean(pmap_add)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_image(img0, img1):\n",
    "    a = np.array(img0)\n",
    "    b = np.array(img1)\n",
    "    return Image.fromarray(np.concatenate((a, b), axis=1))\n",
    "\n",
    "def stack_image_3(img0, img1, img2):\n",
    "    a = np.array(img0)\n",
    "    b = np.array(img1)\n",
    "    c = np.array(img2)\n",
    "    return Image.fromarray(np.concatenate((a, b, c), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_gt_h(check_dataset, stride=1, is_show_class=False):\n",
    "    i = 0\n",
    "    for image, target_dict in check_dataset:        \n",
    "        i += 1\n",
    "        if stride > 1 and i%stride!=0:\n",
    "            continue\n",
    "\n",
    "        x = np.array(image[0], np.uint8)\n",
    "        y_detect = target_dict['detect']\n",
    "        y_heatmap_displace = target_dict['heatmap']\n",
    "        y_heatmap, heatmap_visible, y_displace = split_gt_heatmap_displace(y_heatmap_displace)\n",
    "        \n",
    "        detections_gt = decode_debug(image, y_detect, confidence_threshold=0.15, nms_iou_threshold=0.2)    \n",
    "        model_out = inference_model.predict(image)  \n",
    "        detections = model_out['detect']\n",
    "        heatmap = model_out['heatmap']\n",
    "        #heatmap, displace = split_heatmap_displace(heatmap_displace)\n",
    "        #displace = model_out['person_mask']\n",
    "        y_heatmap = y_heatmap[0]\n",
    "        heatmap = heatmap[0]        \n",
    "        #displace = displace[0]\n",
    "        \n",
    "        #print('x', x.shape, 'detections', detections.shape, 'heatmap', heatmap.shape, 'displace.max()', np.max(displace))\n",
    "        \n",
    "        h = np.array(detections)  \n",
    "        gt = np.array(detections_gt)\n",
    "        print(i, 'gt', gt.shape, 'h', h.shape)\n",
    "        print('y_heatmap', y_heatmap.shape, 'heatmap', heatmap.shape, 'max', np.max(heatmap), 'min', np.min(heatmap))\n",
    "        \n",
    "        heatmap_scale = (255//(1+NUM_KEYPOINT))\n",
    "        if len(np.array(heatmap).shape) < 3:            \n",
    "            heatmap_rev = (256 - heatmap.astype(np.float)*heatmap_scale).astype(np.uint8)\n",
    "            heatmap_arr = np.array(heatmap).astype(np.uint8) \n",
    "            heatmap_arr = np.stack((heatmap_arr* heatmap_scale, heatmap_rev, np.power(heatmap_arr*5, 2)), -1)\n",
    "        else:\n",
    "            heatmap_arr = np.array(heatmap).astype(np.uint8) \n",
    "                \n",
    "        img_heatmap = Image.fromarray(heatmap_arr)\n",
    "        \n",
    "        img = Image.fromarray(x)\n",
    "        img_copy = x.copy()\n",
    "        img_copy[:, :, 0] = tf.cast(np.array(y_heatmap*heatmap_scale), tf.uint8)\n",
    "        \n",
    "        img_gt = Image.fromarray(img_copy)\n",
    "        draw_box_keypoint(img, h, is_show_class=is_show_class)\n",
    "        draw_box_keypoint(img_gt, gt, is_show_class=is_show_class)\n",
    "        draw_box_keypoint(img_heatmap, gt, is_show_class=is_show_class)        \n",
    "        \n",
    "        #print('heatmap_arr', heatmap_arr.shape)\n",
    "        display(stack_image_3(img, img_gt, img_heatmap))    \n",
    "        #display(stack_image(img, img_gt))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.array([1,2]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weight():           \n",
    "    print('latest_checkpoint', path_weight)\n",
    "    model.load_weights(path_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes, NUM_KEYPOINT_CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filters\n",
    "anchor_ch = 4+1+num_classes+NUM_KEYPOINT*(2+1)\n",
    "anchor_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with strategy.scope():\n",
    "optimizer = tf.optimizers.SGD(learning_rate=1e-2, momentum=0.25)\n",
    "loss_detect = NetLoss(num_classes, NUM_KEYPOINT_CLASS)\n",
    "loss_heatmap = HeatmapLoss(NUM_KEYPOINT + 1)\n",
    "loss_heatmap_coord = HeatmapCoordLoss(NUM_KEYPOINT + 1, anchor_ch)\n",
    "model = createNet(num_classes, NUM_KEYPOINT, 1, anchor_k, is_backbone_train=True)\n",
    "metric_det = [recall, precision]\n",
    "\n",
    "losses = {\"detect\": loss_detect, 'heatmap': loss_heatmap, 'heatmap_coord':loss_heatmap_coord}\n",
    "metrics = {\"detect\": metric_det, 'heatmap': [heatmap_acc, visible_acc]}#, 'heatmap_coord':nms_kp_dist} \n",
    "loss_weights = {\"detect\": 0.1, 'heatmap': 1.0, 'heatmap_coord':0.2}\n",
    "\n",
    "model.compile(loss=losses, optimizer=optimizer, metrics=metrics, loss_weights=loss_weights)\n",
    "callbacks_list = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=path_weight,\n",
    "        monitor=\"loss\",\n",
    "        save_best_only=False,\n",
    "        save_weights_only=True,\n",
    "        verbose=0,\n",
    "        save_freq=200\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()#b1:13,906,006, b2:15,431,112, b3t:25,731,444, b4:25M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(list_x_train), len(list_x_test), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#with strategy.scope():\n",
    "load_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.evaluate(train_dataset.take(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#3s 116ms/step - loss: 23.5998 - tf_op_layer_concat_loss: 44.9855 - tf_op_layer_concat_3_loss: 0.9294 - tf_op_layer_concat_3_1_loss: 72.6875 - tf_op_layer_concat_recall: 0.6485 - tf_op_layer_concat_precision: 0.8724 - tf_op_layer_concat_3_heatmap_acc: 0.1274 - tf_op_layer_concat_3_visible_acc: 0.1581\n",
    "out = model.evaluate(val_dataset.take(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "2s 177ms/step - loss: 0.5565 - tf_op_layer_concat_283_loss: 0.4345 - tf_op_layer_ResizeBilinear_16_loss: 0.0122 - tf_op_layer_concat_283_recall: 1.0000 - tf_op_layer_concat_283_precision: 1.0000 - tf_op_layer_concat_283_kp_L1: 0.7677 - tf_op_layer_concat_283_kp_cls_acc: 0.5468 - tf_op_layer_ResizeBilinear_16_hitmap_acc: 0.9748\n",
    "4s 182ms/step - loss: 0.1565 - tf_op_layer_concat_51_loss: 0.0050 - tf_op_layer_concat_52_loss: 0.1560 - tf_op_layer_concat_52_1_loss: 20.4920 - tf_op_layer_concat_51_recall: 1.0000 - tf_op_layer_concat_51_precision: 1.0000 - tf_op_layer_concat_51_kp_L1: 4.4291 - tf_op_layer_concat_51_kp_cls_acc: 0.3286 - tf_op_layer_concat_52_heatmap_acc: 0.9500\n",
    "4s 185ms/step - loss: 0.1053 - tf_op_layer_concat_449_loss: 0.0402 - tf_op_layer_concat_450_loss: 0.1013 - tf_op_layer_concat_450_1_loss: 20.1714 - tf_op_layer_concat_449_recall: 1.0000 - tf_op_layer_concat_449_precision: 1.0000 - tf_op_layer_concat_449_kp_L1: 4.4300 - tf_op_layer_concat_449_kp_cls_acc: 0.3288 - tf_op_layer_concat_450_heatmap_acc: 0.9571\n",
    "0s 45ms/step - loss: 0.0947 - tf_op_layer_concat_loss: 1.7922e-04 - tf_op_layer_concat_1_loss: 0.5477 - tf_op_layer_concat_1_1_loss: 0.3989 - tf_op_layer_concat_recall: 1.0000 - tf_op_layer_concat_precision: 1.0000 - tf_op_layer_concat_1_heatmap_acc: 0.8923\n",
    "2s 93ms/step - loss: 0.0963 - tf_op_layer_concat_80_loss: 0.0200 - tf_op_layer_concat_81_loss: 0.4499 - tf_op_layer_concat_81_1_loss: 0.4922 - tf_op_layer_concat_80_recall: 1.0000 - tf_op_layer_concat_80_precision: 1.0000 - tf_op_layer_concat_81_heatmap_acc: 0.7975\n",
    "3s 95ms/step - loss: 0.2461 - tf_op_layer_concat_343_loss: 0.3998 - tf_op_layer_concat_344_loss: 0.0961 - tf_op_layer_concat_344_1_loss: 0.3665 - tf_op_layer_concat_343_recall: 0.9974 - tf_op_layer_concat_343_precision: 0.9998 - tf_op_layer_concat_344_heatmap_acc: 0.9175\n",
    "3s 98ms/step - loss: 0.1898 - tf_op_layer_concat_1016_loss: 0.3246 - tf_op_layer_concat_1017_loss: 0.0708 - tf_op_layer_concat_1017_1_loss: 0.1729 - tf_op_layer_concat_1016_recall: 0.9996 - tf_op_layer_concat_1016_precision: 0.9986 - tf_op_layer_concat_1017_heatmap_acc: 0.8858\n",
    "4s 101ms/step - loss: 0.1934 - tf_op_layer_concat_63_loss: 0.2979 - tf_op_layer_concat_64_loss: 0.0594 - tf_op_layer_concat_64_1_loss: 0.2083 - tf_op_layer_concat_63_recall: 0.9989 - tf_op_layer_concat_63_precision: 0.9991 - tf_op_layer_concat_64_heatmap_acc: 0.9230\n",
    "7s 145ms/step - loss: 0.1202 - tf_op_layer_concat_272_loss: 0.3086 - tf_op_layer_concat_273_loss: 0.0521 - tf_op_layer_concat_273_1_loss: 0.0745 - tf_op_layer_concat_272_recall: 0.9996 - tf_op_layer_concat_272_precision: 0.9986 - tf_op_layer_concat_273_heatmap_acc: 0.9329 - tf_op_layer_concat_273_visible_acc: 0.8672\n",
    "8s 160ms/step - loss: 0.1351 - tf_op_layer_concat_52_loss: 0.4764 - tf_op_layer_concat_55_loss: 0.0610 - tf_op_layer_concat_55_1_loss: 0.1054 - tf_op_layer_concat_52_recall: 0.9983 - tf_op_layer_concat_52_precision: 0.9995 - tf_op_layer_concat_55_heatmap_acc: 0.9215 - tf_op_layer_concat_55_visible_acc: 0.8757\n",
    "'''\n",
    "epochs = 100000\n",
    "hist = model.fit(train_dataset, epochs=epochs, callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.save_weights(path_weight)\n",
    "path_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_heatmap(images, predictions, h_heatmap_score, displace,\n",
    "                      num_classes=num_classes,\n",
    "                      confidence_threshold=0.5,\n",
    "                      nms_iou_threshold=0.2,\n",
    "                      max_detections_per_class=100,\n",
    "                      max_detections=150,\n",
    "                      box_variance=[0.1, 0.1, 0.2, 0.2]):\n",
    "    \n",
    "    _anchor_box = AnchorBox()   \n",
    "    m = tf.shape(images)[0]    \n",
    "    image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n",
    "    img_h = image_shape[1]\n",
    "    img_w = image_shape[2]\n",
    "    anchor_boxes = _anchor_box.get_anchors(image_shape[1], image_shape[2])   \n",
    "    h_box, objectness, h_cls, h_keypoint = split_hyperthesis(predictions) \n",
    "    \n",
    "    boxes = _decode_box_predictions(anchor_boxes[None, ...], h_box)     \n",
    "    boxes_2d = tf.reshape(boxes, [-1, 4])    \n",
    "    scores = tf.reshape(objectness, [-1, 1])\n",
    "    cls = objectness * 0 + 1\n",
    "    cls = tf.reshape(cls, [-1, 1])\n",
    "    \n",
    "    ccbox = tf.concat((cls, scores, boxes_2d), -1)\n",
    "    ccbox_check = tf.concat((boxes_2d, cls), -1)\n",
    "    \n",
    "    selected_indices, selected_scores = tf.image.non_max_suppression_with_scores(    \n",
    "        ccbox[:, 2:2+4],\n",
    "        ccbox[:, 1],        \n",
    "        max_detections,\n",
    "        nms_iou_threshold,\n",
    "        confidence_threshold,        \n",
    "    )\n",
    "    output = tf.gather(ccbox_check, selected_indices)    \n",
    "    \n",
    "    x1 = output[:, 0] / img_w\n",
    "    y1 = output[:, 1] / img_h\n",
    "    x2 = output[:, 2] / img_w\n",
    "    y2 = output[:, 3] / img_h\n",
    "    boxes = tf.stack((y1, x1, y2, x2), -1)    #[num_boxes, 4], normalized coordinates ` [y1, x1, y2, x2]\n",
    "    box_indices = tf.zeros_like(boxes[:, 0], tf.int32)\n",
    "    crop_size = [100, 100]#h, w\n",
    "    crop_image = tf.image.crop_and_resize(images, boxes, box_indices, crop_size)\n",
    "    crop_heatmap_score = tf.image.crop_and_resize(h_heatmap_score, boxes, box_indices, crop_size)\n",
    "    crop_displace = tf.image.crop_and_resize(displace, boxes, box_indices, crop_size)    \n",
    "    \n",
    "    h_heatmap_cls = tf.argmax(crop_heatmap_score, -1) * 12\n",
    "    h_heatmap_cls = tf.cast(h_heatmap_cls, tf.float32)    \n",
    "    \n",
    "    coord_map = coordinate_map_norm(img_h, img_w)\n",
    "    coord_map = tf.expand_dims(coord_map, 0) + tf.zeros_like(h_heatmap_score[:,:,:,:2])    \n",
    "    crop_coord_map = tf.image.crop_and_resize(coord_map, boxes, box_indices, crop_size)\n",
    "\n",
    "    kp_xy, visible = get_kp_from_heatmap_displacex2(crop_heatmap_score, crop_displace, crop_size, crop_coord_map)\n",
    "    #kp_xy, visible, valid_group_show = get_kp_from_heatmap_displace(crop_heatmap_score, crop_displace, crop_size)\n",
    "    \n",
    "    kp_offset = output[:, None, :2]    \n",
    "    kp_scale = output[:, None, 2:4] - output[:, None, 0:2]\n",
    "    kp_xy = kp_scale * kp_xy + kp_offset\n",
    "    kp_cls = tf.expand_dims(visible, -1)\n",
    "    kp = tf.concat((kp_xy, kp_cls), -1)\n",
    "    kp = tf.reshape(kp, [-1, NUM_KEYPOINT * NUM_KEYPOINT_CH])\n",
    "    output = tf.concat((output, kp), -1)\n",
    "    \n",
    "    return output, crop_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image = tf.keras.Input(shape=[None, None, 3], name=\"image\")\n",
    "predictions = model(image, training=False)\n",
    "h_detect = predictions['detect']\n",
    "h_heatmap_score_displace = predictions['heatmap']\n",
    "h_heatmap_score, h_visible_score, displace = split_heatmap_displace(h_heatmap_score_displace)\n",
    "h_heatmap = tf.argmax(h_heatmap_score, -1)\n",
    "detections, crop_image = decode_heatmap(image, h_detect, h_heatmap_score, displace, confidence_threshold=0.85, nms_iou_threshold=0.15)\n",
    "model_out = {'detect':detections, 'heatmap':h_heatmap, 'crop_image':crop_image, 'crop_heatmap':crop_image}\n",
    "inference_model = tf.keras.Model(inputs=image, outputs=model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_gt_h(train_dataset, stride=1, is_show_class=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compare_gt_h(val_dataset, stride=1, is_show_class=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_gt_h(val_dataset, stride=1, is_show_class=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_gt_h(val_dataset, stride=1, is_show_class=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_gt_h(val_dataset, stride=1, is_show_class=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_gt_h(val_dataset, stride=1, is_show_class=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세그멘테이션 마스크 레이블이 있다면 사람이 겹치는 상황에서 세그멘테이션 내부에서 키포인트 위치 추정를 추정해서 다른 사람의 신체에 혼동없이 위치를 학습시킬 수있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i = 0\n",
    "for image, target_dict in val_dataset:        \n",
    "    i += 1   \n",
    "    x = np.array(image[0], np.uint8)\n",
    "    y_detect = target_dict['detect']\n",
    "    y_heatmap = target_dict['heatmap']    \n",
    "    model_out = inference_model.predict(image)  \n",
    "    crop_img = model_out['crop_image']\n",
    "    crop_heatmap = model_out['crop_heatmap']\n",
    "    heatmap = model_out['heatmap']\n",
    "    y_heatmap = y_heatmap[0]\n",
    "    heatmap = heatmap[0]\n",
    "    \n",
    "    print('crop_img', crop_img.shape, np.max(crop_img))\n",
    "    print('crop_heatmap', crop_heatmap.shape, np.max(crop_heatmap))\n",
    "    crop_img_concat = np.concatenate(crop_img, 1)\n",
    "    \n",
    "    crop_heatmap_rgb = np.stack((crop_heatmap, crop_heatmap, crop_heatmap), -1)\n",
    "    crop_heatmap_concat = np.concatenate(crop_heatmap_rgb, 1)\n",
    "    print('crop_heatmap_concat', crop_heatmap_concat.shape)\n",
    "    \n",
    "    crop_img_concat = crop_img_concat.astype(np.uint8)\n",
    "    crop_heatmap_concat = crop_heatmap_concat.astype(np.uint8)\n",
    "    crop_img_show = np.concatenate((crop_img_concat, crop_heatmap_concat), 0)\n",
    "    \n",
    "    #plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(crop_img_show)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
